{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pettingzoo in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (1.24.3)\n",
      "Requirement already satisfied: SuperSuit in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (3.9.3)\n",
      "Requirement already satisfied: tensorboard in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (2.18.0)\n",
      "Requirement already satisfied: torch in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from pettingzoo) (1.26.4)\n",
      "Requirement already satisfied: gymnasium>=0.28.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from pettingzoo) (0.29.1)\n",
      "Requirement already satisfied: tinyscaler>=1.2.6 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from SuperSuit) (1.2.8)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (1.69.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: packaging in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (5.29.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (72.2.0)\n",
      "Requirement already satisfied: six>1.9 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: filelock in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from gymnasium>=0.28.0->pettingzoo) (3.0.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from gymnasium>=0.28.0->pettingzoo) (0.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pettingzoo SuperSuit tensorboard torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode 0\n",
      "Episodic Return: -21.83510971069336\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 176.5322265625\n",
      "Policy Loss: 17.053508758544922\n",
      "Old Approx KL: -0.004810879472643137\n",
      "Approx KL: 0.000524863600730896\n",
      "Clip Fraction: 0.16741415899660853\n",
      "Explained Variance: -0.011383771896362305\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1\n",
      "Episodic Return: 1.2011070251464844\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 291.7607421875\n",
      "Policy Loss: -21.812397003173828\n",
      "Old Approx KL: 0.005162262357771397\n",
      "Approx KL: 0.00046031674719415605\n",
      "Clip Fraction: 0.05658034318023258\n",
      "Explained Variance: -0.023245811462402344\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 2\n",
      "Episodic Return: -12.560975074768066\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 56.53511047363281\n",
      "Policy Loss: 10.423421859741211\n",
      "Old Approx KL: -0.014624112285673618\n",
      "Approx KL: 0.003404325805604458\n",
      "Clip Fraction: 0.19193672867388362\n",
      "Explained Variance: -0.016336560249328613\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 3\n",
      "Episodic Return: -15.559455871582031\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 42.554039001464844\n",
      "Policy Loss: -7.297128677368164\n",
      "Old Approx KL: -0.00019281440472695976\n",
      "Approx KL: 0.0019357303390279412\n",
      "Clip Fraction: 0.007872781638676921\n",
      "Explained Variance: -0.010054588317871094\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 4\n",
      "Episodic Return: -17.635448455810547\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 21.392993927001953\n",
      "Policy Loss: -5.808231353759766\n",
      "Old Approx KL: -0.0023887588176876307\n",
      "Approx KL: 0.00033001104020513594\n",
      "Clip Fraction: 0.0032069830275658104\n",
      "Explained Variance: -0.0036257505416870117\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 5\n",
      "Episodic Return: -3.041853427886963\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 17.801517486572266\n",
      "Policy Loss: 2.729302406311035\n",
      "Old Approx KL: -0.005175840575248003\n",
      "Approx KL: 0.0002321302890777588\n",
      "Clip Fraction: 0.004991319444444444\n",
      "Explained Variance: 0.0003845095634460449\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 6\n",
      "Episodic Return: 5.559443473815918\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 16.738035202026367\n",
      "Policy Loss: -4.52750301361084\n",
      "Old Approx KL: -0.021594995632767677\n",
      "Approx KL: 0.002668787958100438\n",
      "Clip Fraction: 0.0068118249376614886\n",
      "Explained Variance: -0.010066032409667969\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 7\n",
      "Episodic Return: -17.98956298828125\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 23.60047721862793\n",
      "Policy Loss: 5.946814060211182\n",
      "Old Approx KL: 0.022136490792036057\n",
      "Approx KL: 0.0015473994426429272\n",
      "Clip Fraction: 0.07096354166666667\n",
      "Explained Variance: -0.01952493190765381\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 8\n",
      "Episodic Return: -8.982809066772461\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 89.76577758789062\n",
      "Policy Loss: -12.422672271728516\n",
      "Old Approx KL: 0.0020192679949104786\n",
      "Approx KL: 0.0003307113947812468\n",
      "Clip Fraction: 0.05995611479091975\n",
      "Explained Variance: 0.0004938840866088867\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 9\n",
      "Episodic Return: -15.721516609191895\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 24.317853927612305\n",
      "Policy Loss: 3.8380563259124756\n",
      "Old Approx KL: -0.012573672458529472\n",
      "Approx KL: 0.0019601897802203894\n",
      "Clip Fraction: 0.023763020833333332\n",
      "Explained Variance: -0.005449056625366211\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 10\n",
      "Episodic Return: -21.32533836364746\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 2.825920343399048\n",
      "Policy Loss: -1.7234089374542236\n",
      "Old Approx KL: 0.0022469526156783104\n",
      "Approx KL: 0.000682507932651788\n",
      "Clip Fraction: 0.01590229552756581\n",
      "Explained Variance: -0.02501058578491211\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 11\n",
      "Episodic Return: 40.037662506103516\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 24.21398162841797\n",
      "Policy Loss: 1.350628137588501\n",
      "Old Approx KL: -0.009523164480924606\n",
      "Approx KL: 0.001410404802300036\n",
      "Clip Fraction: 0.015251253860899143\n",
      "Explained Variance: -0.0011047124862670898\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 12\n",
      "Episodic Return: 8.265887260437012\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 9.84604549407959\n",
      "Policy Loss: 1.6599503755569458\n",
      "Old Approx KL: 0.0006963328924030066\n",
      "Approx KL: 0.0004895412130281329\n",
      "Clip Fraction: 0.006184895833333333\n",
      "Explained Variance: 0.014826357364654541\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 13\n",
      "Episodic Return: 3.4767308235168457\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 56.21223831176758\n",
      "Policy Loss: -9.504365921020508\n",
      "Old Approx KL: 0.004393935203552246\n",
      "Approx KL: 0.0009002586011774838\n",
      "Clip Fraction: 0.022472993832909398\n",
      "Explained Variance: -0.0017344951629638672\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 14\n",
      "Episodic Return: 0.19537091255187988\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 6.351223945617676\n",
      "Policy Loss: 0.4059606194496155\n",
      "Old Approx KL: 0.0046030813828110695\n",
      "Approx KL: 0.0004324863257352263\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: 0.005737662315368652\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 15\n",
      "Episodic Return: -3.472219944000244\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 9.89162540435791\n",
      "Policy Loss: -3.7137598991394043\n",
      "Old Approx KL: 0.010196175426244736\n",
      "Approx KL: 0.003482323605567217\n",
      "Clip Fraction: 0.09186921310093668\n",
      "Explained Variance: -0.09633290767669678\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 16\n",
      "Episodic Return: -0.9011653661727905\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 66.3592529296875\n",
      "Policy Loss: 8.850048065185547\n",
      "Old Approx KL: 0.003351877210661769\n",
      "Approx KL: 0.000916341959964484\n",
      "Clip Fraction: 0.0327208719164547\n",
      "Explained Variance: -0.009673357009887695\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 17\n",
      "Episodic Return: -23.44647216796875\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 104.8312759399414\n",
      "Policy Loss: 12.573697090148926\n",
      "Old Approx KL: 0.0029720664024353027\n",
      "Approx KL: 0.0013679183321073651\n",
      "Clip Fraction: 0.1336443866085675\n",
      "Explained Variance: 0.004189431667327881\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 18\n",
      "Episodic Return: -18.901763916015625\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 3.9121716022491455\n",
      "Policy Loss: 2.433518648147583\n",
      "Old Approx KL: 0.005273712798953056\n",
      "Approx KL: 0.0025258080568164587\n",
      "Clip Fraction: 0.006956500777353843\n",
      "Explained Variance: -0.019908666610717773\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 19\n",
      "Episodic Return: 34.39946365356445\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 17.46111297607422\n",
      "Policy Loss: -5.245004653930664\n",
      "Old Approx KL: -0.0035589998587965965\n",
      "Approx KL: 0.0016801274614408612\n",
      "Clip Fraction: 0.024848090277777776\n",
      "Explained Variance: 0.014221370220184326\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 20\n",
      "Episodic Return: 9.641824722290039\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 14.82923412322998\n",
      "Policy Loss: -2.801046848297119\n",
      "Old Approx KL: 0.015471733175218105\n",
      "Approx KL: 0.0008627987699583173\n",
      "Clip Fraction: 0.004834587194232477\n",
      "Explained Variance: 0.011300325393676758\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 21\n",
      "Episodic Return: -16.008657455444336\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 9.980875968933105\n",
      "Policy Loss: 3.1384787559509277\n",
      "Old Approx KL: 0.004756218753755093\n",
      "Approx KL: 0.00032873949385248125\n",
      "Clip Fraction: 0.0030381944444444445\n",
      "Explained Variance: 0.0017677545547485352\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 22\n",
      "Episodic Return: -17.285715103149414\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 11.14263916015625\n",
      "Policy Loss: -2.8873298168182373\n",
      "Old Approx KL: 0.0005701035261154175\n",
      "Approx KL: 0.00263111456297338\n",
      "Clip Fraction: 0.010910976264211867\n",
      "Explained Variance: -0.0009057521820068359\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 23\n",
      "Episodic Return: 1.240601897239685\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 12.25635814666748\n",
      "Policy Loss: -3.961059093475342\n",
      "Old Approx KL: -0.0014007157878950238\n",
      "Approx KL: 0.0003351867198944092\n",
      "Clip Fraction: 0.05257764274978803\n",
      "Explained Variance: -0.023616671562194824\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 24\n",
      "Episodic Return: -2.0786564350128174\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 21.993711471557617\n",
      "Policy Loss: 6.011223316192627\n",
      "Old Approx KL: 0.009777937084436417\n",
      "Approx KL: 0.0022827584762126207\n",
      "Clip Fraction: 0.010199652777777778\n",
      "Explained Variance: 0.019473612308502197\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 25\n",
      "Episodic Return: -19.505821228027344\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 19.915740966796875\n",
      "Policy Loss: -5.354097843170166\n",
      "Old Approx KL: 0.0028076304588466883\n",
      "Approx KL: 0.0007627838058397174\n",
      "Clip Fraction: 0.005425347222222222\n",
      "Explained Variance: 0.0018078088760375977\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 26\n",
      "Episodic Return: 10.591707229614258\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 3.99672794342041\n",
      "Policy Loss: 0.7962470054626465\n",
      "Old Approx KL: -0.018630865961313248\n",
      "Approx KL: 0.003148401854559779\n",
      "Clip Fraction: 0.017590181281169254\n",
      "Explained Variance: 0.07854992151260376\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 27\n",
      "Episodic Return: -11.388898849487305\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 9.475627899169922\n",
      "Policy Loss: 2.1738452911376953\n",
      "Old Approx KL: -0.0025066700764000416\n",
      "Approx KL: 0.0007341106538660824\n",
      "Clip Fraction: 0.01850646219423248\n",
      "Explained Variance: 0.020097196102142334\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 28\n",
      "Episodic Return: -11.186442375183105\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 16.361068725585938\n",
      "Policy Loss: -5.174252033233643\n",
      "Old Approx KL: -7.812512922100723e-05\n",
      "Approx KL: 0.0013593832263723016\n",
      "Clip Fraction: 0.010862750777353844\n",
      "Explained Variance: -0.006561279296875\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 29\n",
      "Episodic Return: -21.77713394165039\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 26.44475746154785\n",
      "Policy Loss: 6.379330158233643\n",
      "Old Approx KL: 0.009013280272483826\n",
      "Approx KL: 0.0025608623400330544\n",
      "Clip Fraction: 0.019977334110687178\n",
      "Explained Variance: -0.07903027534484863\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 30\n",
      "Episodic Return: -13.41498851776123\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 39.230873107910156\n",
      "Policy Loss: -5.294443130493164\n",
      "Old Approx KL: 0.0016974923200905323\n",
      "Approx KL: 0.001696574967354536\n",
      "Clip Fraction: 0.013961226860475209\n",
      "Explained Variance: 0.0034911632537841797\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 31\n",
      "Episodic Return: 57.79221725463867\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 53.635833740234375\n",
      "Policy Loss: -1.4947428703308105\n",
      "Old Approx KL: -0.00283294590190053\n",
      "Approx KL: 0.0017754981527104974\n",
      "Clip Fraction: 0.04465663580534359\n",
      "Explained Variance: 0.07922255992889404\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 32\n",
      "Episodic Return: -5.5233354568481445\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 4.731424331665039\n",
      "Policy Loss: -0.6265580654144287\n",
      "Old Approx KL: -0.003980504348874092\n",
      "Approx KL: 0.00029461251688189805\n",
      "Clip Fraction: 0.008957851083121367\n",
      "Explained Variance: -0.7004871368408203\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 33\n",
      "Episodic Return: 0.6295241117477417\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 17.76280403137207\n",
      "Policy Loss: -0.7957759499549866\n",
      "Old Approx KL: 0.0010015153093263507\n",
      "Approx KL: 0.0011566695757210255\n",
      "Clip Fraction: 0.015094521610687176\n",
      "Explained Variance: -0.024088740348815918\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 34\n",
      "Episodic Return: -12.457635879516602\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 21.363685607910156\n",
      "Policy Loss: 5.749073028564453\n",
      "Old Approx KL: 0.009326264262199402\n",
      "Approx KL: 0.0040019783191382885\n",
      "Clip Fraction: 0.1830753278401163\n",
      "Explained Variance: -0.14854395389556885\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 35\n",
      "Episodic Return: -15.419008255004883\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 2.8921854496002197\n",
      "Policy Loss: -1.672950267791748\n",
      "Old Approx KL: -0.01068304292857647\n",
      "Approx KL: 0.0015547374496236444\n",
      "Clip Fraction: 0.00043402777777777775\n",
      "Explained Variance: -0.004415750503540039\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 36\n",
      "Episodic Return: 1.8128557205200195\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 15.354686737060547\n",
      "Policy Loss: -3.060108184814453\n",
      "Old Approx KL: 7.011162233538926e-05\n",
      "Approx KL: 0.001389708835631609\n",
      "Clip Fraction: 0.0478515625\n",
      "Explained Variance: -0.019207239151000977\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 37\n",
      "Episodic Return: 10.072892189025879\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 76.45428466796875\n",
      "Policy Loss: 4.000765323638916\n",
      "Old Approx KL: -0.008717586286365986\n",
      "Approx KL: 0.0013840661849826574\n",
      "Clip Fraction: 0.05380738818914526\n",
      "Explained Variance: 0.0013807415962219238\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 38\n",
      "Episodic Return: -3.098710536956787\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 42.0731086730957\n",
      "Policy Loss: 8.341447830200195\n",
      "Old Approx KL: -0.007143805269151926\n",
      "Approx KL: 0.001521892030723393\n",
      "Clip Fraction: 0.03175636577523417\n",
      "Explained Variance: -0.007705211639404297\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 39\n",
      "Episodic Return: 22.906644821166992\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 60.71266174316406\n",
      "Policy Loss: -10.438993453979492\n",
      "Old Approx KL: 0.002674245275557041\n",
      "Approx KL: 0.0015074975090101361\n",
      "Clip Fraction: 0.07197627321713501\n",
      "Explained Variance: 0.020962774753570557\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 40\n",
      "Episodic Return: 20.07040786743164\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 11.451752662658691\n",
      "Policy Loss: 0.9132619500160217\n",
      "Old Approx KL: 0.003803471801802516\n",
      "Approx KL: 0.0002135750255547464\n",
      "Clip Fraction: 0.0005425347222222222\n",
      "Explained Variance: 0.03479909896850586\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 41\n",
      "Episodic Return: -12.312585830688477\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 4.692899703979492\n",
      "Policy Loss: 2.74668288230896\n",
      "Old Approx KL: 0.0004150999884586781\n",
      "Approx KL: 3.5232966183684766e-05\n",
      "Clip Fraction: 0.00010850694444444444\n",
      "Explained Variance: -0.064566969871521\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 42\n",
      "Episodic Return: 2.6730527877807617\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 34.21098327636719\n",
      "Policy Loss: -6.4368767738342285\n",
      "Old Approx KL: 0.0007620371179655194\n",
      "Approx KL: 0.0007275326643139124\n",
      "Clip Fraction: 0.020290798611111112\n",
      "Explained Variance: 0.016974210739135742\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 43\n",
      "Episodic Return: -5.66852331161499\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 14.80497932434082\n",
      "Policy Loss: 5.04688835144043\n",
      "Old Approx KL: -0.008150219917297363\n",
      "Approx KL: 0.0038220733404159546\n",
      "Clip Fraction: 0.010271990464793311\n",
      "Explained Variance: -0.0678558349609375\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 44\n",
      "Episodic Return: 2.126251220703125\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 10.747447967529297\n",
      "Policy Loss: -0.5158665180206299\n",
      "Old Approx KL: 0.0014720360049977899\n",
      "Approx KL: 0.0020540538243949413\n",
      "Clip Fraction: 0.08824025849915212\n",
      "Explained Variance: 0.04131847620010376\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 45\n",
      "Episodic Return: -19.96353530883789\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 4.126125812530518\n",
      "Policy Loss: -2.1570985317230225\n",
      "Old Approx KL: 0.0008800923824310303\n",
      "Approx KL: 0.000566007336601615\n",
      "Clip Fraction: 0.003363715277777778\n",
      "Explained Variance: -0.19336950778961182\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 46\n",
      "Episodic Return: -12.312589645385742\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 1.1004546880722046\n",
      "Policy Loss: 0.9561680555343628\n",
      "Old Approx KL: 0.0001147654329543002\n",
      "Approx KL: 4.049804374517407e-06\n",
      "Clip Fraction: 0.0029296875\n",
      "Explained Variance: -0.24450576305389404\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 47\n",
      "Episodic Return: -8.237407684326172\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 13.444273948669434\n",
      "Policy Loss: -1.4622445106506348\n",
      "Old Approx KL: 0.00036930706119164824\n",
      "Approx KL: 0.00015108619118109345\n",
      "Clip Fraction: 0.003146701388888889\n",
      "Explained Variance: 0.044239580631256104\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 48\n",
      "Episodic Return: -0.4790382981300354\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 10.837076187133789\n",
      "Policy Loss: -2.032966375350952\n",
      "Old Approx KL: 0.0015818873653188348\n",
      "Approx KL: 0.0006737709045410156\n",
      "Clip Fraction: 0.006196952166242732\n",
      "Explained Variance: 0.004187464714050293\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 49\n",
      "Episodic Return: -7.757660865783691\n",
      "Episode Length: 149\n",
      "\n",
      "Value Loss: 13.188335418701172\n",
      "Policy Loss: 3.30051326751709\n",
      "Old Approx KL: -0.01025208830833435\n",
      "Approx KL: 0.005522039253264666\n",
      "Clip Fraction: 0.16049382711450258\n",
      "Explained Variance: -0.04681110382080078\n",
      "\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Basic code which shows what it's like to run PPO on the Pistonball env using the parallel API, this code is inspired by CleanRL.\n",
    "\n",
    "This code is exceedingly basic, with no logging or weights saving.\n",
    "The intention was for users to have a (relatively clean) ~200 line file to refer to when they want to design their own learning algorithm.\n",
    "\n",
    "Author: Jet (https://github.com/jjshoots)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from supersuit import color_reduction_v0, frame_stack_v1, resize_v1\n",
    "from torch.distributions.categorical import Categorical\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            self._layer_init(nn.Conv2d(4, 32, 3, padding=1)),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            self._layer_init(nn.Conv2d(32, 64, 3, padding=1)),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            self._layer_init(nn.Conv2d(64, 128, 3, padding=1)),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            self._layer_init(nn.Linear(128 * 8 * 8, 512)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.actor = self._layer_init(nn.Linear(512, num_actions), std=0.01)\n",
    "        self.critic = self._layer_init(nn.Linear(512, 1))\n",
    "\n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(self.network(x / 255.0))\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        hidden = self.network(x / 255.0)\n",
    "        logits = self.actor(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)\n",
    "\n",
    "\n",
    "def batchify_obs(obs, device):\n",
    "    \"\"\"Converts PZ style observations to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "    obs = np.stack([obs[a] for a in obs], axis=0)\n",
    "    # transpose to be (batch, channel, height, width)\n",
    "    obs = obs.transpose(0, -1, 1, 2)\n",
    "    # convert to torch\n",
    "    obs = torch.tensor(obs).to(device)\n",
    "\n",
    "    return obs\n",
    "\n",
    "\n",
    "def batchify(x, device):\n",
    "    \"\"\"Converts PZ style returns to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "    x = np.stack([x[a] for a in x], axis=0)\n",
    "    # convert to torch\n",
    "    x = torch.tensor(x).to(device)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def unbatchify(x, env):\n",
    "    \"\"\"Converts np array to PZ style arguments.\"\"\"\n",
    "    x = x.cpu().numpy()\n",
    "    x = {a: x[i] for i, a in enumerate(env.possible_agents)}\n",
    "\n",
    "    return x\n",
    "\n",
    "# Inicjalizujemy Writer (TensorBoard):\n",
    "writer = SummaryWriter(f\"runs/PPOv1_{int(time.time())}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"ALGO PARAMS\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ent_coef = 0.1\n",
    "    vf_coef = 0.1\n",
    "    clip_coef = 0.1\n",
    "    gamma = 0.99\n",
    "    batch_size = 128\n",
    "    stack_size = 4\n",
    "    frame_size = (64, 64)\n",
    "    max_cycles = 150\n",
    "    total_episodes = 50\n",
    "\n",
    "    \"\"\" ENV SETUP \"\"\"\n",
    "    env = pistonball_v6.parallel_env(\n",
    "        render_mode=\"rgb_array\", continuous=False, max_cycles=max_cycles\n",
    "    )\n",
    "    env = color_reduction_v0(env)\n",
    "    env = resize_v1(env, frame_size[0], frame_size[1])\n",
    "    env = frame_stack_v1(env, stack_size=stack_size)\n",
    "    num_agents = len(env.possible_agents)\n",
    "    num_actions = env.action_space(env.possible_agents[0]).n\n",
    "    observation_size = env.observation_space(env.possible_agents[0]).shape\n",
    "\n",
    "    \"\"\" LEARNER SETUP \"\"\"\n",
    "    agent = Agent(num_actions=num_actions).to(device)\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=0.001, eps=1e-5)\n",
    "\n",
    "    \"\"\" ALGO LOGIC: EPISODE STORAGE\"\"\"\n",
    "    end_step = 0\n",
    "    total_episodic_return = 0\n",
    "    rb_obs = torch.zeros((max_cycles, num_agents, stack_size, *frame_size)).to(device)\n",
    "    rb_actions = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "    rb_logprobs = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "    rb_rewards = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "    rb_terms = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "    rb_values = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "\n",
    "    \"\"\" TRAINING LOGIC \"\"\"\n",
    "    # train for n number of episodes\n",
    "    for episode in range(total_episodes):\n",
    "        # collect an episode\n",
    "        with torch.no_grad():\n",
    "            # collect observations and convert to batch of torch tensors\n",
    "            next_obs, info = env.reset(seed=None)\n",
    "            # reset the episodic return\n",
    "            total_episodic_return = 0\n",
    "\n",
    "            # each episode has num_steps\n",
    "            for step in range(0, max_cycles):\n",
    "                # rollover the observation\n",
    "                obs = batchify_obs(next_obs, device)\n",
    "\n",
    "                # get action from the agent\n",
    "                actions, logprobs, _, values = agent.get_action_and_value(obs)\n",
    "\n",
    "                # execute the environment and log data\n",
    "                next_obs, rewards, terms, truncs, infos = env.step(\n",
    "                    unbatchify(actions, env)\n",
    "                )\n",
    "\n",
    "                # add to episode storage\n",
    "                rb_obs[step] = obs\n",
    "                rb_rewards[step] = batchify(rewards, device)\n",
    "                rb_terms[step] = batchify(terms, device)\n",
    "                rb_actions[step] = actions\n",
    "                rb_logprobs[step] = logprobs\n",
    "                rb_values[step] = values.flatten()\n",
    "\n",
    "                # compute episodic return\n",
    "                total_episodic_return += rb_rewards[step].cpu().numpy()\n",
    "\n",
    "                # if we reach termination or truncation, end\n",
    "                if any([terms[a] for a in terms]) or any([truncs[a] for a in truncs]):\n",
    "                    end_step = step\n",
    "                    break\n",
    "\n",
    "        # bootstrap value if not done\n",
    "        with torch.no_grad():\n",
    "            rb_advantages = torch.zeros_like(rb_rewards).to(device)\n",
    "            for t in reversed(range(end_step)):\n",
    "                delta = (\n",
    "                    rb_rewards[t]\n",
    "                    + gamma * rb_values[t + 1] * rb_terms[t + 1]\n",
    "                    - rb_values[t]\n",
    "                )\n",
    "                rb_advantages[t] = delta + gamma * gamma * rb_advantages[t + 1]\n",
    "            rb_returns = rb_advantages + rb_values\n",
    "\n",
    "        # convert our episodes to batch of individual transitions\n",
    "        b_obs = torch.flatten(rb_obs[:end_step], start_dim=0, end_dim=1)\n",
    "        b_logprobs = torch.flatten(rb_logprobs[:end_step], start_dim=0, end_dim=1)\n",
    "        b_actions = torch.flatten(rb_actions[:end_step], start_dim=0, end_dim=1)\n",
    "        b_returns = torch.flatten(rb_returns[:end_step], start_dim=0, end_dim=1)\n",
    "        b_values = torch.flatten(rb_values[:end_step], start_dim=0, end_dim=1)\n",
    "        b_advantages = torch.flatten(rb_advantages[:end_step], start_dim=0, end_dim=1)\n",
    "\n",
    "        # Optimizing the policy and value network\n",
    "        b_index = np.arange(len(b_obs))\n",
    "        clip_fracs = []\n",
    "        for repeat in range(3):\n",
    "            # shuffle the indices we use to access the data\n",
    "            np.random.shuffle(b_index)\n",
    "            for start in range(0, len(b_obs), batch_size):\n",
    "                # select the indices we want to train on\n",
    "                end = start + batch_size\n",
    "                batch_index = b_index[start:end]\n",
    "\n",
    "                _, newlogprob, entropy, value = agent.get_action_and_value(\n",
    "                    b_obs[batch_index], b_actions.long()[batch_index]\n",
    "                )\n",
    "                logratio = newlogprob - b_logprobs[batch_index]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clip_fracs += [\n",
    "                        ((ratio - 1.0).abs() > clip_coef).float().mean().item()\n",
    "                    ]\n",
    "\n",
    "                # normalize advantaegs\n",
    "                advantages = b_advantages[batch_index]\n",
    "                advantages = (advantages - advantages.mean()) / (\n",
    "                    advantages.std() + 1e-8\n",
    "                )\n",
    "\n",
    "                # Policy loss\n",
    "                pg_loss1 = -b_advantages[batch_index] * ratio\n",
    "                pg_loss2 = -b_advantages[batch_index] * torch.clamp(\n",
    "                    ratio, 1 - clip_coef, 1 + clip_coef\n",
    "                )\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # Value loss\n",
    "                value = value.flatten()\n",
    "                v_loss_unclipped = (value - b_returns[batch_index]) ** 2\n",
    "                v_clipped = b_values[batch_index] + torch.clamp(\n",
    "                    value - b_values[batch_index],\n",
    "                    -clip_coef,\n",
    "                    clip_coef,\n",
    "                )\n",
    "                v_loss_clipped = (v_clipped - b_returns[batch_index]) ** 2\n",
    "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "        print(f\"Training episode {episode}\")\n",
    "        print(f\"Episodic Return: {np.mean(total_episodic_return)}\")\n",
    "        print(f\"Episode Length: {end_step}\")\n",
    "        print(\"\")\n",
    "        print(f\"Value Loss: {v_loss.item()}\")\n",
    "        print(f\"Policy Loss: {pg_loss.item()}\")\n",
    "        print(f\"Old Approx KL: {old_approx_kl.item()}\")\n",
    "        print(f\"Approx KL: {approx_kl.item()}\")\n",
    "        print(f\"Clip Fraction: {np.mean(clip_fracs)}\")\n",
    "        print(f\"Explained Variance: {explained_var.item()}\")\n",
    "        print(\"\\n-------------------------------------------\\n\")\n",
    "        writer.add_scalar(\"train/episodic_return\", np.mean(total_episodic_return), episode)\n",
    "        writer.add_scalar(\"train/value_loss\", v_loss.item(), episode)\n",
    "        writer.add_scalar(\"train/policy_loss\", pg_loss.item(), episode)\n",
    "        writer.add_scalar(\"train/approx_kl\", approx_kl.item(), episode)\n",
    "        writer.add_scalar(\"train/explained_variance\", explained_var.item(), episode)\n",
    "\n",
    "    \"\"\" RENDER THE POLICY \"\"\"\n",
    "    env = pistonball_v6.parallel_env(render_mode=\"human\", continuous=False)\n",
    "    env = color_reduction_v0(env)\n",
    "    env = resize_v1(env, 64, 64)\n",
    "    env = frame_stack_v1(env, stack_size=4)\n",
    "\n",
    "    agent.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # render 5 episodes out\n",
    "        for episode in range(5):\n",
    "            obs, infos = env.reset(seed=None)\n",
    "            obs = batchify_obs(obs, device)\n",
    "            terms = [False]\n",
    "            truncs = [False]\n",
    "            while not any(terms) and not any(truncs):\n",
    "                actions, logprobs, _, values = agent.get_action_and_value(obs)\n",
    "                obs, rewards, terms, truncs, infos = env.step(unbatchify(actions, env))\n",
    "                obs = batchify_obs(obs, device)\n",
    "                terms = [terms[a] for a in terms]\n",
    "                truncs = [truncs[a] for a in truncs]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlzh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
