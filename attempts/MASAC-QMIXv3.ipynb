{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (2.4.1)\n",
      "Requirement already satisfied: numpy in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: supersuit in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (3.9.3)\n",
      "Requirement already satisfied: pettingzoo in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (1.24.3)\n",
      "Requirement already satisfied: pymunk in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (6.10.0)\n",
      "Requirement already satisfied: scipy in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (1.15.0)\n",
      "Requirement already satisfied: gymnasium in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (0.29.1)\n",
      "Requirement already satisfied: matplotlib in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: einops in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (0.8.0)\n",
      "Requirement already satisfied: tensorboard in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (2.18.0)\n",
      "Requirement already satisfied: wandb in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (0.19.4)\n",
      "Requirement already satisfied: imageio in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (2.35.0)\n",
      "Requirement already satisfied: cloudpickle in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from stable-baselines3) (3.0.0)\n",
      "Requirement already satisfied: pandas in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from stable-baselines3) (2.2.3)\n",
      "Requirement already satisfied: filelock in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: tinyscaler>=1.2.6 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from supersuit) (1.2.8)\n",
      "Requirement already satisfied: cffi>=1.17.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from pymunk) (1.17.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (1.69.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (5.29.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (72.2.0)\n",
      "Requirement already satisfied: six>1.9 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (4.2.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (6.0.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (2.10.5)\n",
      "Requirement already satisfied: pyyaml in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (2.20.0)\n",
      "Requirement already satisfied: setproctitle in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: pycparser in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from cffi>=1.17.1->pymunk) (2.22)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from pandas->stable-baselines3) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from pandas->stable-baselines3) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install stable-baselines3 numpy torch supersuit pettingzoo pymunk scipy gymnasium matplotlib einops tensorboard wandb imageio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(seed=1, total_episodes=1000000, max_cycles=75, n_agents=2, buffer_size=20000, batch_size=1, gamma=0.98, tau=0.005, alpha=0.2, target_entropy=-3.0, lr_actor=0.003, lr_critic=0.003, lr_mixer=0.003, learning_starts=2, train_freq=1, eval_freq=50, num_eval_episodes=4, exp_name='SAC_QMIX_spread')\n",
      "Box(0.0, 1.0, (5,), float32)\n",
      "State dim: 24, Obs dim each: 12, Act dim each: 5, Total act dim: 10\n",
      "Target entropy: -5.0\n",
      "[EVAL] Episode 50, avg_return=-164.058\n",
      "[EVAL] Episode 100, avg_return=-240.143\n",
      "[EVAL] Episode 150, avg_return=-173.297\n",
      "[EVAL] Episode 200, avg_return=-182.484\n",
      "[EVAL] Episode 250, avg_return=-220.289\n",
      "[EVAL] Episode 300, avg_return=-239.561\n",
      "[EVAL] Episode 350, avg_return=-199.805\n",
      "[EVAL] Episode 400, avg_return=-255.228\n",
      "[EVAL] Episode 450, avg_return=-120.286\n",
      "[EVAL] Episode 500, avg_return=-178.034\n",
      "[EVAL] Episode 550, avg_return=-128.065\n",
      "[EVAL] Episode 600, avg_return=-206.392\n",
      "[EVAL] Episode 650, avg_return=-242.885\n",
      "[EVAL] Episode 700, avg_return=-176.047\n",
      "[EVAL] Episode 750, avg_return=-206.613\n",
      "[EVAL] Episode 800, avg_return=-261.220\n",
      "[EVAL] Episode 850, avg_return=-249.787\n",
      "[EVAL] Episode 900, avg_return=-212.953\n",
      "[EVAL] Episode 950, avg_return=-152.661\n",
      "[EVAL] Episode 1000, avg_return=-200.568\n",
      "[EVAL] Episode 1050, avg_return=-211.036\n",
      "[EVAL] Episode 1100, avg_return=-144.975\n",
      "[EVAL] Episode 1150, avg_return=-235.591\n",
      "[EVAL] Episode 1200, avg_return=-167.892\n",
      "[EVAL] Episode 1250, avg_return=-178.983\n",
      "[EVAL] Episode 1300, avg_return=-196.512\n",
      "[EVAL] Episode 1350, avg_return=-192.482\n",
      "[EVAL] Episode 1400, avg_return=-189.938\n",
      "[EVAL] Episode 1450, avg_return=-150.515\n",
      "[EVAL] Episode 1500, avg_return=-178.354\n",
      "[EVAL] Episode 1550, avg_return=-206.117\n",
      "[EVAL] Episode 1600, avg_return=-181.532\n",
      "[EVAL] Episode 1650, avg_return=-268.118\n",
      "[EVAL] Episode 1700, avg_return=-155.892\n",
      "[EVAL] Episode 1750, avg_return=-169.899\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 609\u001b[0m\n\u001b[1;32m    607\u001b[0m args \u001b[38;5;241m=\u001b[39m get_args()\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28mprint\u001b[39m(args)\n\u001b[0;32m--> 609\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43mrun_sac_qmix\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone. Last 10 episodes avg return:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(rewards[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m:]))\n",
      "Cell \u001b[0;32mIn[2], line 548\u001b[0m, in \u001b[0;36mrun_sac_qmix\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    545\u001b[0m     q_tot_actor \u001b[38;5;241m=\u001b[39m mixer(q_cat_actor, batch_states)\n\u001b[1;32m    547\u001b[0m \u001b[38;5;66;03m# Przed obliczeniem actor_loss\u001b[39;00m\n\u001b[0;32m--> 548\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdebug/agent_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_q_contribution\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_tot_actor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdebug/agent_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_entropy_contribution\u001b[39m\u001b[38;5;124m\"\u001b[39m, (alpha \u001b[38;5;241m*\u001b[39m new_logp_i)\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem(), global_step)\n\u001b[1;32m    551\u001b[0m actor_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(q_tot_actor \u001b[38;5;241m-\u001b[39m alpha \u001b[38;5;241m*\u001b[39m new_logp_i)\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/miniconda/envs/drlzh/lib/python3.11/site-packages/torch/utils/tensorboard/writer.py:396\u001b[0m, in \u001b[0;36mSummaryWriter.add_scalar\u001b[0;34m(self, tag, scalar_value, global_step, walltime, new_style, double_precision)\u001b[0m\n\u001b[1;32m    391\u001b[0m     scalar_value \u001b[38;5;241m=\u001b[39m workspace\u001b[38;5;241m.\u001b[39mFetchBlob(scalar_value)\n\u001b[1;32m    393\u001b[0m summary \u001b[38;5;241m=\u001b[39m scalar(\n\u001b[1;32m    394\u001b[0m     tag, scalar_value, new_style\u001b[38;5;241m=\u001b[39mnew_style, double_precision\u001b[38;5;241m=\u001b[39mdouble_precision\n\u001b[1;32m    395\u001b[0m )\n\u001b[0;32m--> 396\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_file_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwalltime\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/drlzh/lib/python3.11/site-packages/torch/utils/tensorboard/writer.py:114\u001b[0m, in \u001b[0;36mFileWriter.add_summary\u001b[0;34m(self, summary, global_step, walltime)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add a `Summary` protocol buffer to the event file.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03mThis method wraps the provided summary in an `Event` protocol buffer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    walltime (from time.time()) seconds after epoch\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m event \u001b[38;5;241m=\u001b[39m event_pb2\u001b[38;5;241m.\u001b[39mEvent(summary\u001b[38;5;241m=\u001b[39msummary)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwalltime\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/drlzh/lib/python3.11/site-packages/torch/utils/tensorboard/writer.py:98\u001b[0m, in \u001b[0;36mFileWriter.add_event\u001b[0;34m(self, event, step, walltime)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# Make sure step is converted from numpy or other formats\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# since protobuf might not convert depending on version\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     event\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(step)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent_writer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/drlzh/lib/python3.11/site-packages/tensorboard/summary/writer/event_file_writer.py:117\u001b[0m, in \u001b[0;36mEventFileWriter.add_event\u001b[0;34m(self, event)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, event_pb2\u001b[38;5;241m.\u001b[39mEvent):\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected an event_pb2.Event proto, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(event)\n\u001b[1;32m    116\u001b[0m     )\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_async_writer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSerializeToString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/drlzh/lib/python3.11/site-packages/tensorboard/summary/writer/event_file_writer.py:174\u001b[0m, in \u001b[0;36m_AsyncWriter.write\u001b[0;34m(self, bytestring)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWriter is closed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_byte_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytestring\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Check the status again in case the background worker thread has\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# failed in the meantime to avoid waiting until the next call to\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# surface the error.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_worker_status()\n",
      "File \u001b[0;32m~/miniconda/envs/drlzh/lib/python3.11/queue.py:133\u001b[0m, in \u001b[0;36mQueue.put\u001b[0;34m(self, item, block, timeout)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mput\u001b[39m(\u001b[38;5;28mself\u001b[39m, item, block\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    123\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Put an item into the queue.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03m    If optional args 'block' is true and 'timeout' is None (the default),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    is ignored in that case).\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_full\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxsize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/drlzh/lib/python3.11/threading.py:271\u001b[0m, in \u001b[0;36mCondition.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39m_at_fork_reinit()\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiters\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import deque\n",
    "from typing import List, Dict, NamedTuple\n",
    "import gymnasium as gym\n",
    "from pettingzoo.mpe import simple_spread_v3\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from argparse import Namespace\n",
    "import time\n",
    "\n",
    "# =====================================================================\n",
    "# 1. Parsing argumentów (Namespace) - przykładowa konfiguracja\n",
    "# =====================================================================\n",
    "def get_args():\n",
    "    args = Namespace()\n",
    "    args.seed = 1\n",
    "    args.total_episodes = 1000000\n",
    "    args.max_cycles = 75       # maksymalna liczba kroków w epizodzie (w MPE: max_cycles)\n",
    "    args.n_agents = 2          # N=3 w simple_spread\n",
    "    args.buffer_size = 20000\n",
    "    args.batch_size = 1 \n",
    "    args.gamma = 0.98\n",
    "    args.tau = 0.005 # 0.05\n",
    "    args.alpha = 0.2 #0.005           # stała entropii w SAC\n",
    "    args.target_entropy = -3.0\n",
    "    args.lr_actor = 3e-3\n",
    "    args.lr_critic = 3e-3\n",
    "    args.lr_mixer = 3e-3\n",
    "    # args.learning_starts = args.batch_size * 5# musi byc wiekszy niz batch_size\n",
    "    args.learning_starts = 2\n",
    "    # args.learning_starts = args.total_episodes // 10\n",
    "    args.train_freq = 1        # co ile kroków/epizodów robić update\n",
    "    args.eval_freq = 50 # co ile epizodów robić ewaluację\n",
    "    # args.eval_freq = args.total_episodes // 100       # co ile epizodów robić ewaluację\n",
    "    args.num_eval_episodes = 4 # ile epizodów w ewaluacji\n",
    "    args.exp_name = \"SAC_QMIX_spread\"\n",
    "    return args\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 2. Sieć aktora - do akcji ciągłych (SAC)\n",
    "# =====================================================================\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.mean_head = nn.Linear(hidden_dim, act_dim)\n",
    "        self.logstd_head = nn.Linear(hidden_dim, act_dim)\n",
    "        self.LOG_STD_MIN = -5\n",
    "        self.LOG_STD_MAX = 2\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = self.net(obs)\n",
    "        mean = self.mean_head(x)\n",
    "        log_std = self.logstd_head(x)\n",
    "        # ograniczamy zakres log_std\n",
    "        log_std = torch.clamp(log_std, min=self.LOG_STD_MIN, max=self.LOG_STD_MAX)\n",
    "        return mean, log_std\n",
    "\n",
    "    def sample(self, obs):\n",
    "        \"\"\"\n",
    "        Zwraca (action, log_prob), z:\n",
    "        - action w (-1,1)\n",
    "        - log_prob łączny (po wymiarach akcji)\n",
    "        \"\"\"\n",
    "        mean, log_std = self.forward(obs)\n",
    "        std = log_std.exp()\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        z = dist.rsample()  # reparametrization\n",
    "        action = torch.tanh(z) # od -1 do 1\n",
    "        action = .5 * (action + 1)  # od 0 do 1\n",
    "\n",
    "        # log_prob: korekta za tanh\n",
    "        # log_prob = dist.log_prob(z) - torch.log(1 - action.pow(2) + 1e-7)\n",
    "        # log_prob = dist.log_prob(z) - torch.log(torch.clamp(1 - action.pow(2), min=1e-7))\n",
    "        log_prob = dist.log_prob(z)\n",
    "        log_prob = log_prob - torch.log(torch.tensor(0.5, device=z.device)) \\\n",
    "                            - torch.log(1 - torch.tanh(z).pow(2) + 1e-7)\n",
    "        log_prob = log_prob.sum(dim=-1, keepdim=True)\n",
    "        return action, log_prob\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 3. Krytyk Q dla agenta i (Twin Q -> critic1[i], critic2[i])\n",
    "# =====================================================================\n",
    "class QCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: cat(global_state, joint_action)\n",
    "    Output: scalar Q^i\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 4. Mixing Network (QMIX)\n",
    "# =====================================================================\n",
    "class MixingNetwork(nn.Module):\n",
    "    def __init__(self, n_agents, state_dim, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.n_agents = n_agents\n",
    "        self.state_dim = state_dim\n",
    "\n",
    "        self.hyper_w1 = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_agents * hidden_dim)\n",
    "        )\n",
    "        self.hyper_b1 = nn.Linear(state_dim, hidden_dim)\n",
    "\n",
    "        self.hyper_w2 = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.hyper_b2 = nn.Linear(state_dim, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, q_values, state):\n",
    "        \"\"\"\n",
    "        q_values: (batch_size, n_agents)\n",
    "        state: (batch_size, state_dim)\n",
    "        Zwraca (batch_size, 1)\n",
    "        \"\"\"\n",
    "        bs = q_values.size(0)\n",
    "        # 1 warstwa\n",
    "        w1 = self.hyper_w1(state).view(bs, self.n_agents, -1)\n",
    "        w1 = torch.relu(w1)\n",
    "        b1 = self.hyper_b1(state).view(bs, 1, -1)\n",
    "\n",
    "        q_values = q_values.unsqueeze(1)  # (bs, 1, n_agents)\n",
    "        hidden = torch.bmm(q_values, w1) + b1  # => (bs, 1, hidden_dim)\n",
    "        hidden = self.relu(hidden)\n",
    "\n",
    "        # 2 warstwa\n",
    "        w2 = self.hyper_w2(state).view(bs, -1, 1)\n",
    "        w2 = torch.relu(w2)\n",
    "        b2 = self.hyper_b2(state).view(bs, 1, 1)\n",
    "\n",
    "        q_tot = torch.bmm(hidden, w2) + b2  # => (bs,1,1)\n",
    "        return q_tot.view(bs, 1)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 5. Replay buffer\n",
    "# =====================================================================\n",
    "class Transition(NamedTuple):\n",
    "    state: np.ndarray\n",
    "    obs: List[np.ndarray]  # local obs for each agent\n",
    "    action: List[np.ndarray]\n",
    "    reward: float\n",
    "    next_state: np.ndarray\n",
    "    next_obs: List[np.ndarray]\n",
    "    done: bool\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=100000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # print(\"Sampling from buffer\")\n",
    "        # print(\"Buffer size:\", len(self.buffer))\n",
    "        # print(\"Batch size:\", batch_size)\n",
    "\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        return list(zip(*batch))\n",
    "        # Zwracamy listy/tuple T= (state, obs, action, reward, next_state, next_obs, done)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 6. Główna klasa/wydzielona pętla treningowa\n",
    "# =====================================================================\n",
    "def run_sac_qmix(args):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # 6.1 Przygotowanie środowiska\n",
    "    env = simple_spread_v3.parallel_env(\n",
    "        N=args.n_agents,\n",
    "        local_ratio=0.2,\n",
    "        max_cycles=args.max_cycles,\n",
    "        continuous_actions=True\n",
    "    )\n",
    "    env.reset(seed=args.seed)\n",
    "    \n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    # 6.2 Ustalenie wymiarów\n",
    "    agents = env.possible_agents  # np. [agent_0, agent_1, agent_2]\n",
    "    \n",
    "    n_agents = len(agents)\n",
    "    # Sprawdzamy, czy n_agents == args.n_agents\n",
    "    assert n_agents == args.n_agents, \"Nie zgadza się liczba agentów!\"\n",
    "    act_dim_each = env.action_space(agents[0]).shape[0]  # np. 2\n",
    "    state_dim = np.prod(env.state().shape)               # global state\n",
    "    # local obs\n",
    "    obs_dim_each = np.prod(env.observation_space(agents[0]).shape)\n",
    "\n",
    "    print(env.action_space(agents[0])) \n",
    "    # Joint action dimension\n",
    "    act_dim_total = act_dim_each * n_agents\n",
    "    print(f\"State dim: {state_dim}, Obs dim each: {obs_dim_each}, Act dim each: {act_dim_each}, Total act dim: {act_dim_total}\")\n",
    "\n",
    "    # 6.3 Inicjalizacja aktorów i krytyków (plus targety)\n",
    "    #  - actor[i], critic1[i], critic2[i], target_critic1[i], target_critic2[i]\n",
    "    #  - mixing network\n",
    "    #  - target mixing network\n",
    "    actors = []\n",
    "    critics1 = []\n",
    "    critics2 = []\n",
    "    target_critics1 = []\n",
    "    target_critics2 = []\n",
    "\n",
    "    for i in range(n_agents):\n",
    "        actor = Actor(obs_dim_each, act_dim_each).to(device)\n",
    "        actors.append(actor)\n",
    "\n",
    "        c1 = QCritic(state_dim + act_dim_total).to(device)\n",
    "        c2 = QCritic(state_dim + act_dim_total).to(device)\n",
    "        tc1 = QCritic(state_dim + act_dim_total).to(device)\n",
    "        tc1.load_state_dict(c1.state_dict())\n",
    "        tc2 = QCritic(state_dim + act_dim_total).to(device)\n",
    "        tc2.load_state_dict(c2.state_dict())\n",
    "\n",
    "        critics1.append(c1)\n",
    "        critics2.append(c2)\n",
    "        target_critics1.append(tc1)\n",
    "        target_critics2.append(tc2)\n",
    "\n",
    "    mixer = MixingNetwork(n_agents, state_dim).to(device)\n",
    "    target_mixer = MixingNetwork(n_agents, state_dim).to(device)\n",
    "    target_mixer.load_state_dict(mixer.state_dict())\n",
    "\n",
    "    # 6.4 Optimizers\n",
    "    actor_opts = [optim.Adam(actors[i].parameters(), lr=args.lr_actor) for i in range(n_agents)]\n",
    "    critic_opts = []\n",
    "    for i in range(n_agents):\n",
    "        params = list(critics1[i].parameters()) + list(critics2[i].parameters())\n",
    "        critic_opts.append(optim.Adam(params, lr=args.lr_critic))\n",
    "    mixer_opt = optim.Adam(mixer.parameters(), lr=args.lr_mixer)\n",
    "    \n",
    "    # DYNAMICZNE ALPHA\n",
    "    args.target_entropy = -float(act_dim_each)  # np. minus liczba wymiarów akcji\n",
    "    print(\"Target entropy:\", args.target_entropy)\n",
    "    # log_alpha = nn.Parameter(torch.zeros(1, requires_grad=True, device=device))\n",
    "    # log_alpha = nn.Parameter(torch.log(torch.tensor(args.alpha, dtype=torch.float32, device=device)))\n",
    "    # alpha_optim = optim.Adam([log_alpha], lr=5e-6)\n",
    "    log_alpha = nn.Parameter(torch.log(torch.tensor(0.2, device=device)))\n",
    "    alpha_optim = optim.Adam([log_alpha], lr=1e-4)\n",
    "    with torch.no_grad():\n",
    "        alpha = torch.clamp(log_alpha.exp(), min=5e-4)  # minimalna wartość\n",
    "\n",
    "    # 6.5 Bufor replay\n",
    "    replay = ReplayBuffer(max_size=args.buffer_size)\n",
    "\n",
    "    # 6.6 TensorBoard\n",
    "    writer = SummaryWriter(comment=f\"_{args.exp_name}\")\n",
    "    global_step = 0\n",
    "    episode_rewards = []\n",
    "\n",
    "    def soft_update(source_net, target_net, tau):\n",
    "        for p, tp in zip(source_net.parameters(), target_net.parameters()):\n",
    "            tp.data.copy_(tau*p.data + (1.0 - tau)*tp.data)\n",
    "\n",
    "    def evaluate_policy(episode_idx):\n",
    "        \"\"\"\n",
    "        Uruchamia kilka epizodów w trybie testowym (bez noise), liczy średni zwrot.\n",
    "        Loguje do TB.\n",
    "        \"\"\"\n",
    "        eval_episodes = args.num_eval_episodes\n",
    "        returns = []\n",
    "        for _ in range(eval_episodes):\n",
    "            obs_dict, _ = env.reset()\n",
    "            done_dict = {ag: False for ag in agents}\n",
    "            ep_ret = 0.0\n",
    "            while not all(done_dict.values()):\n",
    "                actions_dict = {}\n",
    "                for i, ag in enumerate(agents):\n",
    "                    # Deterministycznie: bierzemy mean sieci\n",
    "                    obs_i = torch.FloatTensor(obs_dict[ag]).unsqueeze(0)\n",
    "                    with torch.no_grad():\n",
    "                        mean, _ = actors[i].forward(obs_i)\n",
    "                        action = torch.tanh(mean) # od -1 do 1\n",
    "                        action = .5 * (action + 1)  # od 0 do 1\n",
    "                    actions_dict[ag] = action.cpu().numpy().flatten()\n",
    "                    writer.add_scalar(f\"debug/eval_action_mean_{ag}\", actions_dict[ag].mean(), episode_idx)\n",
    "                    writer.add_scalar(f\"debug/eval_action_std_{ag}\", actions_dict[ag].std(), episode_idx)\n",
    "\n",
    "                next_obs, rews, done_dict, _, _ = env.step(actions_dict)\n",
    "                ep_ret += sum(rews.values())\n",
    "                obs_dict = next_obs\n",
    "            returns.append(ep_ret)\n",
    "        avg_ret = np.mean(returns)\n",
    "        writer.add_scalar(\"evaluate/avg_return\", avg_ret, episode_idx)\n",
    "        print(f\"[EVAL] Episode {episode_idx}, avg_return={avg_ret:.3f}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 6.7 Główna pętla treningowa\n",
    "    for ep in range(args.total_episodes):\n",
    "        obs_dict, _ = env.reset()\n",
    "        done_dict = {ag: False for ag in agents}\n",
    "        state_np = env.state()\n",
    "        ep_ret = 0.0\n",
    "        step = 0\n",
    "\n",
    "        while not all(done_dict.values()) and step < args.max_cycles:\n",
    "            actions_dict = {}\n",
    "            local_obs_list = []\n",
    "            for i, ag in enumerate(agents):\n",
    "\n",
    "                # obs_i = obs_dict[ag]\n",
    "                obs_i = torch.FloatTensor(obs_dict[ag]).unsqueeze(0)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    act_i, _ = actors[i].sample(obs_i)\n",
    "                actions_dict[ag] = act_i.cpu().numpy().flatten()\n",
    "                local_obs_list.append(obs_dict[ag])\n",
    "\n",
    "            next_obs_dict, rew_dict, done_dict, _, _ = env.step(actions_dict)\n",
    "            next_state_np = env.state()\n",
    "            reward = sum(rew_dict.values())\n",
    "            done = any(done_dict.values())  # ep. done\n",
    "\n",
    "            # Po otrzymaniu nagrody\n",
    "            writer.add_scalar(\"debug/step_reward\", reward, global_step)\n",
    "            for ag, rew in rew_dict.items():\n",
    "                writer.add_scalar(f\"debug/agent_{ag}_reward\", rew, global_step)\n",
    "            # Zapis do bufora\n",
    "            act_list = [actions_dict[ag] for ag in agents]\n",
    "            next_local_obs_list = [next_obs_dict[ag] for ag in agents]\n",
    "\n",
    "            replay.add(\n",
    "                state_np,\n",
    "                local_obs_list,\n",
    "                act_list,\n",
    "                reward,\n",
    "                next_state_np,\n",
    "                next_local_obs_list,\n",
    "                done\n",
    "            )\n",
    "\n",
    "            ep_ret += reward\n",
    "            state_np = next_state_np\n",
    "            obs_dict = next_obs_dict\n",
    "            step += 1\n",
    "            global_step += 1\n",
    "\n",
    "            # Trening\n",
    "            if (global_step > args.learning_starts) and (global_step % args.train_freq == 0):\n",
    "                # sample\n",
    "                (states_b, obs_b, acts_b, rews_b, next_states_b, next_obs_b, dones_b) = replay.sample(args.batch_size)\n",
    "                # states_b: tuple (batch_size, ), musimy skonwertować\n",
    "                batch_states = torch.FloatTensor(np.array(states_b))\n",
    "                batch_next_states = torch.FloatTensor(np.array(next_states_b))\n",
    "                batch_acts = torch.FloatTensor(np.array(acts_b))  # shape (B, N, act_dim_each)\n",
    "                batch_acts = batch_acts.view(args.batch_size, -1)  # (B, act_dim_total)\n",
    "                batch_rewards = torch.FloatTensor(np.array(rews_b)).unsqueeze(1)  # (B,1)\n",
    "                batch_dones = torch.FloatTensor(np.array(dones_b)).unsqueeze(1)   # (B,1)\n",
    "\n",
    "                # ---------- Oblicz target -----------\n",
    "                with torch.no_grad():\n",
    "                    # next akcje (z actorów)\n",
    "                    all_next_actions = []\n",
    "                    sum_log_prob_next = torch.zeros((args.batch_size, 1), device=device)\n",
    "\n",
    "                    for i in range(n_agents):\n",
    "                        agent_next_obs = []\n",
    "                        for b_idx in range(args.batch_size):\n",
    "                            agent_next_obs.append(next_obs_b[b_idx][i])\n",
    "                        agent_next_obs = torch.FloatTensor(np.array(agent_next_obs)).to(device)\n",
    "\n",
    "                        a_next_i, logp_next_i = actors[i].sample(agent_next_obs)\n",
    "                        all_next_actions.append(a_next_i)\n",
    "                        sum_log_prob_next += logp_next_i\n",
    "\n",
    "                    next_joint_actions = torch.cat(all_next_actions, dim=1)\n",
    "                    \n",
    "                    # Obliczamy Q^i z targetów i miksujemy\n",
    "                    all_qi_next = []\n",
    "                    for i in range(n_agents):\n",
    "                        inp = torch.cat([batch_next_states.to(device), next_joint_actions], dim=1)\n",
    "                        q1_val = target_critics1[i](inp)\n",
    "                        q2_val = target_critics2[i](inp)\n",
    "                        qi_next = torch.min(q1_val, q2_val)\n",
    "                        all_qi_next.append(qi_next)\n",
    "                    q_i_cat_next = torch.cat(all_qi_next, dim=1)\n",
    "                    q_tot_next = target_mixer(q_i_cat_next, batch_next_states.to(device))\n",
    "\n",
    "                    # --- ODJĘCIE alpha * SUMA LOG PROB ---\n",
    "                    # alpha = log_alpha.exp()  # dynamiczne alpha\n",
    "                    q_tot_next = q_tot_next + 0.2 * sum_log_prob_next\n",
    "\n",
    "                    # Bellman target y = rewards + gamma * (1 - done) * q_tot_next\n",
    "                    y = batch_rewards.to(device) + (1 - batch_dones.to(device)) * args.gamma * q_tot_next\n",
    "                    \n",
    "\n",
    "                # ---------- Oblicz Q^i current i Q_tot ----------\n",
    "                # critics1[i], critics2[i]\n",
    "                all_qi_1 = []\n",
    "                all_qi_2 = []\n",
    "                for i in range(n_agents):\n",
    "                    inp = torch.cat([batch_states, batch_acts], dim=1)\n",
    "                    q1_val = critics1[i](inp)\n",
    "                    q2_val = critics2[i](inp)\n",
    "                    all_qi_1.append(q1_val)\n",
    "                    all_qi_2.append(q2_val)\n",
    "\n",
    "                q1_cat = torch.cat(all_qi_1, dim=1)  # (B,N)\n",
    "                q2_cat = torch.cat(all_qi_2, dim=1)  # (B,N)\n",
    "                q_tot_1 = mixer(q1_cat, batch_states)  # (B,1)\n",
    "                q_tot_2 = mixer(q2_cat, batch_states)  # (B,1)\n",
    "                q_tot_current = torch.min(q_tot_1, q_tot_2)\n",
    "                writer.add_scalar(\"debug/q_tot_target\", y.mean().item(), global_step)\n",
    "                writer.add_scalar(\"debug/q_tot_diff\", (y - q_tot_current).mean().item(), global_step)\n",
    "\n",
    "\n",
    "                # critic_loss = F.mse_loss(q_tot_current, y)\n",
    "                # critic_loss = F.mse_loss(q_tot_current, y) + 0.001 * (q_tot_current ** 2).mean()\n",
    "                # critic_loss = F.mse_loss(q_tot_current, y) + 0.01 * (q_tot_current ** 2).mean()\n",
    "                # critic_loss = F.mse_loss(q_tot_current, y)\n",
    "                                \n",
    "                critic_loss = F.mse_loss(q_tot_current, y) + \\\n",
    "                            0.01 * sum(qi.pow(2).mean() for qi in all_qi_1) + \\\n",
    "                            0.01 * sum(qi.pow(2).mean() for qi in all_qi_2)\n",
    "                # Najpierw zero_grad i clip dla *każdego* krytyka osobno\n",
    "                for i in range(n_agents):\n",
    "                    critic_opts[i].zero_grad()\n",
    "                    torch.nn.utils.clip_grad_norm_(critics1[i].parameters(), 0.5)\n",
    "                    torch.nn.utils.clip_grad_norm_(critics2[i].parameters(), 0.5)\n",
    "\n",
    "                # mixer tez ma osobny optimizer, wiec:\n",
    "                mixer_opt.zero_grad()\n",
    "\n",
    "                critic_loss.backward()\n",
    "\n",
    "                # Teraz step\n",
    "                for i in range(n_agents):\n",
    "                    critic_opts[i].step()\n",
    "                mixer_opt.step()\n",
    "\n",
    "\n",
    "                # ---------- Update krytyków + mixera ----------\n",
    "                # for opt in critic_opts:\n",
    "                #     torch.nn.utils.clip_grad_norm_(critics1[i].parameters(), 0.5)\n",
    "                #     torch.nn.utils.clip_grad_norm_(critics2[i].parameters(), 0.5)\n",
    "                #     opt.zero_grad()\n",
    "                # mixer_opt.zero_grad()\n",
    "                # critic_loss.backward()\n",
    "                # for opt in critic_opts:\n",
    "                #     opt.step()\n",
    "                # mixer_opt.step()\n",
    "\n",
    "                # ---------- Update aktorów (SAC) ------------\n",
    "                # Podobnie jak w pseudo-kodzie: agent i\n",
    "                for i in range(n_agents):\n",
    "                    # \"Nową\" akcję daje actor i, reszta agentów => stara\n",
    "                    old_actions = batch_acts.clone()  # shape (B, N*act_dim_each)\n",
    "\n",
    "                    # Wyciągamy batch local_obs i\n",
    "                    agent_obs = []\n",
    "                    for b_idx in range(args.batch_size):\n",
    "                        agent_obs.append(obs_b[b_idx][i])\n",
    "                    agent_obs = torch.FloatTensor(np.array(agent_obs))\n",
    "\n",
    "                    new_action_i, new_logp_i = actors[i].sample(agent_obs)\n",
    "                    # Podmieniamy w old_actions\n",
    "                    # reshape -> (B, N, act_dim_each)\n",
    "                    old_actions_resh = old_actions.view(args.batch_size, n_agents, act_dim_each)\n",
    "                    old_actions_resh[:, i, :] = new_action_i\n",
    "                    new_joint_action = old_actions_resh.view(args.batch_size, -1)\n",
    "\n",
    "                    # Obliczamy Q^i z critics1[i], critics2[i]\n",
    "                    input_new = torch.cat([batch_states, new_joint_action], dim=1)\n",
    "                    q1_val_i = critics1[i](input_new)\n",
    "                    q2_val_i = critics2[i](input_new)\n",
    "                    q_val_i_min = torch.min(q1_val_i, q2_val_i)\n",
    "\n",
    "                    # Potrzebujemy Q^j dla j != i, bierzemy ze starych akcji?\n",
    "                    # Dla uproszczenia: liczymy on-the-fly\n",
    "                    # final wektor [Q^1, Q^2, ..., Q^N]\n",
    "                    q_current_list = []\n",
    "                    for j in range(n_agents):\n",
    "                        if j == i:\n",
    "                            q_current_list.append(q_val_i_min)  # (B,1)\n",
    "                        else:\n",
    "                            # stara akcja (B, N, act_dim_each)\n",
    "                            # w oryg. artykule QMIX jest robione w pętli,\n",
    "                            # by gradient nie przepływał przez aktor j\n",
    "                            # => ok, bierzemy \"no_grad\"?\n",
    "                            with torch.no_grad():\n",
    "                                inp_j = torch.cat([batch_states, batch_acts], dim=1)\n",
    "                                q1_j = critics1[j](inp_j)\n",
    "                                q2_j = critics2[j](inp_j)\n",
    "                                q_j_min = torch.min(q1_j, q2_j)\n",
    "                            q_current_list.append(q_j_min)\n",
    "                                        \n",
    "                    # ----------------- Dynamiczna aktualizacja alpha -----------------\n",
    "                    # alpha_loss = E[ -log_alpha * (log_prob + target_entropy) ]\n",
    "                    alpha = log_alpha.exp()\n",
    "                    alpha_loss = -(log_alpha * (new_logp_i.detach() + args.target_entropy)).mean()\n",
    "                    alpha_optim.zero_grad()\n",
    "                    alpha_loss.backward()\n",
    "                    alpha_optim.step()\n",
    "\n",
    "                    # alpha_loss = -(log_alpha * (new_logp_i.detach() + args.target_entropy)).mean()\n",
    "\n",
    "\n",
    "                    # alpha_optim.zero_grad()\n",
    "                    # alpha_loss.backward()\n",
    "                    # alpha_optim.step()\n",
    "                    \n",
    "                    \n",
    "                    # cat => (B, N)\n",
    "                    q_cat_actor = torch.cat(q_current_list, dim=1)\n",
    "                    # q_tot_actor = mixer(q_cat_actor, batch_states)\n",
    "                    with torch.no_grad():\n",
    "                        q_tot_actor = mixer(q_cat_actor, batch_states)\n",
    "\n",
    "                    # Przed obliczeniem actor_loss\n",
    "                    writer.add_scalar(f\"debug/agent_{i}_q_contribution\", q_tot_actor.mean().item(), global_step)\n",
    "                    writer.add_scalar(f\"debug/agent_{i}_entropy_contribution\", (alpha * new_logp_i).mean().item(), global_step)\n",
    "\n",
    "                    actor_loss = -(q_tot_actor - alpha * new_logp_i).mean()\n",
    "\n",
    "                    actor_opts[i].zero_grad()\n",
    "                    actor_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(actors[i].parameters(), 1.0)\n",
    "                    actor_opts[i].step()\n",
    "\n",
    "                    actor_grad_norm = sum(p.grad.norm().item() for p in actors[i].parameters() if p.grad is not None)\n",
    "                    writer.add_scalar(f\"debug/actor_{i}_grad_norm\", actor_grad_norm, global_step)\n",
    "\n",
    "\n",
    "                    writer.add_scalar(f\"debug/agent_{i}_entropy\", -new_logp_i.mean().item(), global_step)\n",
    "                    writer.add_scalar(f\"debug/agent_{i}_action_mean\", new_action_i.mean().item(), global_step)\n",
    "                    writer.add_scalar(f\"debug/agent_{i}_action_std\", new_action_i.std().item(), global_step)\n",
    "\n",
    "\n",
    "                    writer.add_scalar(f\"loss/actor_loss_agent_{i}\", actor_loss.item(), global_step)\n",
    "                    writer.add_scalar(\"loss/alpha_loss\", alpha_loss.item(), global_step)\n",
    "                    writer.add_scalar(\"alpha/value\", alpha.item(), global_step)\n",
    "\n",
    "                # ---------- Soft update targetów ------------\n",
    "                for i in range(n_agents):\n",
    "                    soft_update(critics1[i], target_critics1[i], args.tau)\n",
    "                    soft_update(critics2[i], target_critics2[i], args.tau)\n",
    "                soft_update(mixer, target_mixer, args.tau)\n",
    "\n",
    "                # ---------- logi do TB -----------\n",
    "                writer.add_scalar(\"loss/critic_loss\", critic_loss.item(), global_step)\n",
    "                writer.add_scalar(\"loss/qmix_loss\", critic_loss.item(), global_step)\n",
    "                writer.add_scalar(\"q_values/q_tot\", q_tot_current.mean().item(), global_step)\n",
    "                for i in range(n_agents):\n",
    "                    writer.add_scalar(f\"q_values/q_agent_{i}\", q1_cat[:, i].mean().item(), global_step)\n",
    "                    critic_grad_norm = sum(p.grad.norm().item() for p in critics1[i].parameters() if p.grad is not None)\n",
    "                    writer.add_scalar(f\"debug/critic_{i}_grad_norm\", critic_grad_norm, global_step)\n",
    "\n",
    "                writer.add_scalar(\"charts/SPS\", global_step / (time.time() - start_time), global_step)\n",
    "\n",
    "        # koniec epizodu\n",
    "        episode_rewards.append(ep_ret)\n",
    "        writer.add_scalar(\"charts/episodic_return\", ep_ret, ep)\n",
    "        writer.add_scalar(\"charts/average_return\", np.mean(episode_rewards[-100:]), ep)\n",
    "\n",
    "        # ewaluacja co X epizodów\n",
    "        if (ep+1) % args.eval_freq == 0:\n",
    "            evaluate_policy(ep+1)\n",
    "\n",
    "    env.close()\n",
    "    writer.close()\n",
    "\n",
    "    return episode_rewards\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 7. Uruchamianie\n",
    "# =====================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    args = get_args()\n",
    "    print(args)\n",
    "    rewards = run_sac_qmix(args)\n",
    "    print(\"Done. Last 10 episodes avg return:\", np.mean(rewards[-10:]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlzh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
