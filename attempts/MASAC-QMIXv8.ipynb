{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (2.4.1)\n",
      "Requirement already satisfied: numpy in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: supersuit in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (3.9.3)\n",
      "Requirement already satisfied: pettingzoo in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (1.24.3)\n",
      "Requirement already satisfied: pymunk in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (6.10.0)\n",
      "Requirement already satisfied: scipy in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (1.15.0)\n",
      "Requirement already satisfied: gymnasium in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (0.29.1)\n",
      "Requirement already satisfied: matplotlib in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: einops in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (0.8.0)\n",
      "Requirement already satisfied: tensorboard in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (2.18.0)\n",
      "Requirement already satisfied: wandb in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (0.19.4)\n",
      "Requirement already satisfied: imageio in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (2.35.0)\n",
      "Requirement already satisfied: cloudpickle in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from stable-baselines3) (3.0.0)\n",
      "Requirement already satisfied: pandas in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from stable-baselines3) (2.2.3)\n",
      "Requirement already satisfied: filelock in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: tinyscaler>=1.2.6 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from supersuit) (1.2.8)\n",
      "Requirement already satisfied: cffi>=1.17.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from pymunk) (1.17.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (1.69.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (5.29.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (72.2.0)\n",
      "Requirement already satisfied: six>1.9 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (4.2.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (6.0.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (2.10.5)\n",
      "Requirement already satisfied: pyyaml in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (2.20.0)\n",
      "Requirement already satisfied: setproctitle in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: pycparser in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from cffi>=1.17.1->pymunk) (2.22)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from pandas->stable-baselines3) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from pandas->stable-baselines3) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install stable-baselines3 numpy torch supersuit pettingzoo pymunk scipy gymnasium matplotlib einops tensorboard wandb imageio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(seed=1, total_timesteps=1000000, max_cycles=75, n_agents=2, buffer_size=5000, batch_size=32, gamma=0.99, tau=0.01, alpha=0.2, target_entropy=-3.0, lr_actor=0.0005, lr_critic=0.0003, lr_mixer=0.0003, lr_alpha=1e-05, l2=1e-05, learning_starts=400, train_freq=1, eval_freq=50, num_eval_episodes=4, exp_name='SAC_QMIX_spread_discrete', epsilon_start=1.0, epsilon_end=0.05, epsilon_decay_steps=50000, target_update_interval=200)\n",
      "Discrete(5)\n",
      "State dim: 24, Obs dim each: 12, Act dim each: 5, Total act dim: 10\n",
      "[EVAL] Episode 100, avg_return=-811.720\n",
      "[EVAL] Episode 150, avg_return=-510.639\n",
      "[EVAL] Episode 200, avg_return=-1168.641\n",
      "[EVAL] Episode 250, avg_return=-989.703\n",
      "[EVAL] Episode 300, avg_return=-859.092\n",
      "[EVAL] Episode 350, avg_return=-620.414\n",
      "[EVAL] Episode 400, avg_return=-1257.633\n",
      "[EVAL] Episode 450, avg_return=-1617.266\n",
      "[EVAL] Episode 500, avg_return=-1641.024\n",
      "[EVAL] Episode 550, avg_return=-1550.577\n",
      "[EVAL] Episode 600, avg_return=-1655.698\n",
      "[EVAL] Episode 650, avg_return=-1597.602\n",
      "[EVAL] Episode 700, avg_return=-1738.341\n",
      "[EVAL] Episode 750, avg_return=-1569.464\n",
      "[EVAL] Episode 800, avg_return=-1656.990\n",
      "[EVAL] Episode 850, avg_return=-1641.315\n",
      "[EVAL] Episode 900, avg_return=-1576.807\n",
      "[EVAL] Episode 950, avg_return=-1686.483\n",
      "[EVAL] Episode 1000, avg_return=-1540.527\n",
      "Timestep 1000, Avg Return: -1546.43\n",
      "[EVAL] Episode 1050, avg_return=-1574.507\n",
      "[EVAL] Episode 1100, avg_return=-1509.632\n",
      "[EVAL] Episode 1150, avg_return=-1657.661\n",
      "[EVAL] Episode 1200, avg_return=-1627.104\n",
      "[EVAL] Episode 1250, avg_return=-1707.029\n",
      "[EVAL] Episode 1300, avg_return=-1726.083\n",
      "[EVAL] Episode 1350, avg_return=-1578.381\n",
      "[EVAL] Episode 1400, avg_return=-1591.853\n",
      "[EVAL] Episode 1450, avg_return=-1628.738\n",
      "[EVAL] Episode 1500, avg_return=-1587.213\n",
      "[EVAL] Episode 1550, avg_return=-1598.624\n",
      "[EVAL] Episode 1600, avg_return=-1589.812\n",
      "[EVAL] Episode 1650, avg_return=-1627.586\n",
      "[EVAL] Episode 1700, avg_return=-1595.770\n",
      "[EVAL] Episode 1750, avg_return=-1467.765\n",
      "[EVAL] Episode 1800, avg_return=-1475.765\n",
      "[EVAL] Episode 1850, avg_return=-1725.914\n",
      "[EVAL] Episode 1900, avg_return=-1424.362\n",
      "[EVAL] Episode 1950, avg_return=-1645.605\n",
      "[EVAL] Episode 2000, avg_return=-1662.078\n",
      "Timestep 2000, Avg Return: -1477.98\n",
      "[EVAL] Episode 2050, avg_return=-1548.511\n",
      "[EVAL] Episode 2100, avg_return=-1486.834\n",
      "[EVAL] Episode 2150, avg_return=-1694.882\n",
      "[EVAL] Episode 2200, avg_return=-1491.354\n",
      "[EVAL] Episode 2250, avg_return=-1646.276\n",
      "[EVAL] Episode 2300, avg_return=-1615.722\n",
      "[EVAL] Episode 2350, avg_return=-1556.712\n",
      "[EVAL] Episode 2400, avg_return=-1615.127\n",
      "[EVAL] Episode 2450, avg_return=-1638.902\n",
      "[EVAL] Episode 2500, avg_return=-1610.902\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import deque\n",
    "from typing import List, Dict, NamedTuple\n",
    "import gymnasium as gym\n",
    "from pettingzoo.mpe import simple_spread_v3\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from argparse import Namespace\n",
    "import time\n",
    "\n",
    "# =====================================================================\n",
    "# 1. Parsing argumentów (Namespace) - przykładowa konfiguracja\n",
    "# =====================================================================\n",
    "def get_args():\n",
    "    args = Namespace()\n",
    "    args.seed = 1\n",
    "    # args.total_episodes = 1000000\n",
    "    args.total_timesteps = 1000000\n",
    "    args.max_cycles = 75 \n",
    "    args.n_agents = 2          # N=3 w simple_spread\n",
    "    args.buffer_size = 5000\n",
    "    args.batch_size = 32\n",
    "    args.gamma = 0.99\n",
    "    args.tau = 0.01\n",
    "    args.alpha = 0.2\n",
    "    args.target_entropy = -3.0\n",
    "    args.lr_actor = 5e-4 \n",
    "    args.lr_critic = 3e-4\n",
    "    args.lr_mixer = 3e-4\n",
    "    args.lr_alpha = 1e-5       \n",
    "    args.l2 = 1e-5             \n",
    "    args.learning_starts = 400\n",
    "    args.train_freq = 1        # co ile kroków/epizodów robić update\n",
    "    args.eval_freq = 50        # co ile epizodów robić ewaluację\n",
    "    args.num_eval_episodes = 4 # ile epizodów w ewaluacji\n",
    "    args.exp_name = \"SAC_QMIX_spread_discrete\"\n",
    "    args.epsilon_start = 1.0\n",
    "    args.epsilon_end = 0.05\n",
    "    args.epsilon_decay_steps = 50000\n",
    "    args.target_update_interval = 200\n",
    "    return args\n",
    "\n",
    "# =====================================================================\n",
    "# 2. Definicja Współdzielonej Sieci Aktora\n",
    "# =====================================================================\n",
    "class SharedActor(nn.Module):\n",
    "    def __init__(self, state_dim, agent_id_dim, act_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim + agent_id_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, act_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state_with_id):\n",
    "        logits = self.net(state_with_id)\n",
    "        return logits\n",
    "    \n",
    "    def sample(self, state_with_id):\n",
    "        logits = self.forward(state_with_id)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action).unsqueeze(1)\n",
    "        entropy = dist.entropy().unsqueeze(1)\n",
    "        return action, log_prob, entropy\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 3. Definicja Współdzielonej Sieci Krytyka\n",
    "# =====================================================================\n",
    "class SharedCritic(nn.Module):\n",
    "    def __init__(self, state_dim, act_dim, hidden_dim=64, gru_hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.input_dim = state_dim + act_dim\n",
    "        self.gru_hidden_dim = gru_hidden_dim\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.gru = nn.GRU(hidden_dim, gru_hidden_dim, batch_first=True)\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        \"\"\"Inicjalizuje stan ukryty\"\"\"\n",
    "        return torch.zeros(1, batch_size, self.gru_hidden_dim, device=device)\n",
    "    \n",
    "    def forward(self, state, action, hidden_state=None):\n",
    "        \"\"\"\n",
    "        state: (B, state_dim)\n",
    "        action: (B, act_dim)\n",
    "        hidden_state: (1, B, gru_hidden_dim) or None\n",
    "        Returns:\n",
    "            q_value: (B, 1)\n",
    "            next_hidden: (1, B, gru_hidden_dim)\n",
    "        \"\"\"\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        x = self.mlp(x)\n",
    "        x = x.unsqueeze(1)  # (B, 1, hidden_dim)\n",
    "        \n",
    "        if hidden_state is None:\n",
    "            batch_size = x.size(0)\n",
    "            hidden_state = torch.zeros(1, batch_size, self.gru_hidden_dim, device=x.device)\n",
    "        \n",
    "        hidden_state = hidden_state.detach()\n",
    "        \n",
    "        output, new_hidden = self.gru(x, hidden_state)\n",
    "        q_value = self.output(output.squeeze(1))\n",
    "        return q_value, new_hidden\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 3. Lokalny Krytyk Q dla agenta i (DRQCritic -> critic1[i], critic2[i])\n",
    "# =====================================================================\n",
    "class DRQCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Lokalny krytyk Q z rekurencyjną warstwą GRU.\n",
    "    Input: local_obs + last_action\n",
    "    Output: scalar Q^i\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim=256, gru_hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.input_dim = obs_dim + act_dim\n",
    "        self.gru_hidden_dim = gru_hidden_dim\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.gru = nn.GRU(hidden_dim, gru_hidden_dim, batch_first=True)\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        \"\"\"Inicjalizuje stan ukryty\"\"\"\n",
    "        return torch.zeros(1, batch_size, self.gru_hidden_dim, device=device)\n",
    "    \n",
    "    def forward(self, obs, action, hidden_state=None):\n",
    "        \"\"\"\n",
    "        obs: (B, obs_dim)\n",
    "        action: (B, act_dim)\n",
    "        hidden_state: (1, B, gru_hidden_dim) or None\n",
    "        Returns:\n",
    "            q_value: (B, 1)\n",
    "            next_hidden: (1, B, gru_hidden_dim)\n",
    "        \"\"\"\n",
    "        x = torch.cat([obs, action], dim=-1)\n",
    "        x = self.mlp(x)\n",
    "        x = x.unsqueeze(1)  # (B, 1, hidden_dim)\n",
    "        \n",
    "        # Inicjalizacja stanu ukrytego jeśli nie został podany\n",
    "        if hidden_state is None:\n",
    "            batch_size = x.size(0)\n",
    "            hidden_state = torch.zeros(1, batch_size, self.gru_hidden_dim, \n",
    "                                    device=x.device)\n",
    "        \n",
    "        # Tworzenie kopii stanu ukrytego\n",
    "        hidden_state = hidden_state.detach()\n",
    "        \n",
    "        output, new_hidden = self.gru(x, hidden_state)\n",
    "        q_value = self.output(output.squeeze(1))\n",
    "        return q_value, new_hidden\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 3. Krytyk Q dla agenta i (Twin Q -> critic1[i], critic2[i])\n",
    "# =====================================================================\n",
    "class QCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: cat(global_state, joint_action)\n",
    "    Output: scalar Q^i\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 4. Definicja Sieci Mieszającej (QMIXMixingNetwork)\n",
    "# =====================================================================\n",
    "class QMIXMixingNetwork(nn.Module):\n",
    "    def __init__(self, n_agents, state_dim, mixing_hidden_dim=8):\n",
    "        super().__init__()\n",
    "        self.n_agents = n_agents\n",
    "        self.state_dim = state_dim\n",
    "\n",
    "        # Hypernetwork for weights\n",
    "        self.hyper_w = nn.Sequential(\n",
    "            nn.Linear(state_dim, mixing_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mixing_hidden_dim, n_agents * 1),  # Output: n_agents * 1\n",
    "        )\n",
    "        \n",
    "        # Hypernetwork for bias\n",
    "        self.hyper_b = nn.Sequential(\n",
    "            nn.Linear(state_dim, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, q_values, state):\n",
    "        \"\"\"\n",
    "        q_values: (B, N)\n",
    "        state: (B, state_dim)\n",
    "        Zwraca: (B, 1)\n",
    "        \"\"\"\n",
    "        bs = q_values.size(0)\n",
    "\n",
    "        # Generate weights and bias\n",
    "        w = self.hyper_w(state).view(bs, self.n_agents, 1)  # (B, N, 1)\n",
    "        b = self.hyper_b(state).view(bs, 1, 1)            # (B, 1, 1)\n",
    "\n",
    "        # Expand q_values and compute Q_tot\n",
    "        q_values = q_values.unsqueeze(-1)  # (B, N, 1)\n",
    "        q_tot = torch.sum(q_values * w, dim=1) + b.squeeze(1)  # (B, 1)\n",
    "\n",
    "        q_tot = F.elu(q_tot)  # Zastosowanie ELU\n",
    "\n",
    "        return q_tot\n",
    "\n",
    "# =====================================================================\n",
    "# 5. Funkcja pomocnicza do konwersji akcji na one-hot\n",
    "# =====================================================================\n",
    "def actions_to_one_hot(actions, act_dim_each):\n",
    "    \"\"\"\n",
    "    actions: tensor o rozmiarze (B, N)\n",
    "    zwraca tensor o rozmiarze (B, N * act_dim_each)\n",
    "    \"\"\"\n",
    "    one_hot = F.one_hot(actions, num_classes=act_dim_each).float()\n",
    "    return one_hot.view(actions.size(0), -1)  # (B, N * act_dim_each)\n",
    "# =====================================================================\n",
    "# 5.1. Funkcja do generowania One-Hot Encoding ID Agenta\n",
    "# =====================================================================\n",
    "def get_agent_id(agent_index, n_agents):\n",
    "    agent_id = np.zeros(n_agents)\n",
    "    agent_id[agent_index] = 1\n",
    "    return agent_id\n",
    "\n",
    "# =====================================================================\n",
    "# 5.2. Funkcja pomocnicza do konwersji akcji na one-hot\n",
    "# =====================================================================\n",
    "def actions_to_one_hot(actions, act_dim_each):\n",
    "    \"\"\"\n",
    "    actions: tensor o rozmiarze (B, N)\n",
    "    zwraca tensor o rozmiarze (B, N * act_dim_each)\n",
    "    \"\"\"\n",
    "    one_hot = F.one_hot(actions, num_classes=act_dim_each).float()\n",
    "    return one_hot.view(actions.size(0), -1)  # (B, N * act_dim_each)\n",
    "def get_state_with_id(state, agent_id, batch_size=None):\n",
    "    \"\"\"\n",
    "    Properly reshapes and concatenates state with agent_id for both single and batch inputs.\n",
    "    \n",
    "    Args:\n",
    "        state: numpy array of shape (state_dim,) or (batch_size, state_dim)\n",
    "        agent_id: numpy array of shape (n_agents,)\n",
    "        batch_size: int or None\n",
    "    \n",
    "    Returns:\n",
    "        Combined state and agent_id tensor with proper dimensions\n",
    "    \"\"\"\n",
    "    if batch_size is None:\n",
    "        # Single state case\n",
    "        state = np.array(state).reshape(1, -1)  # (1, state_dim)\n",
    "        agent_id = agent_id.reshape(1, -1)      # (1, n_agents)\n",
    "    else:\n",
    "        # Batch case\n",
    "        state = np.array(state)                 # (batch_size, state_dim)\n",
    "        agent_id = np.tile(agent_id, (batch_size, 1))  # (batch_size, n_agents)\n",
    "    \n",
    "    return np.concatenate([state, agent_id], axis=1)  # (batch_size, state_dim + n_agents)\n",
    "\n",
    "# =====================================================================\n",
    "# 6. Replay buffer\n",
    "# =====================================================================\n",
    "class Transition(NamedTuple):\n",
    "    state: np.ndarray\n",
    "    obs: List[np.ndarray]  # local obs for each agent\n",
    "    action: List[int]       # Zmienione na List[int] dla akcji dyskretnych\n",
    "    reward: float\n",
    "    next_state: np.ndarray\n",
    "    next_obs: List[np.ndarray]\n",
    "    done: bool\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=100000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return None\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        return list(zip(*batch))\n",
    "        # Zwracamy listy/tuple T= (state, obs, action, reward, next_state, next_obs, done)\n",
    "\n",
    "# =====================================================================\n",
    "# 6. Główna klasa/wydzielona pętla treningowa - zmodyfikowana\n",
    "# =====================================================================\n",
    "def run_qmix_sac(args):\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # 6.1 Przygotowanie środowiska\n",
    "    env = simple_spread_v3.parallel_env(\n",
    "        N=args.n_agents,\n",
    "        local_ratio=0.2,\n",
    "        max_cycles=args.max_cycles,\n",
    "        continuous_actions=False\n",
    "    )\n",
    "    env.reset(seed=args.seed)\n",
    "    \n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    # 6.2 Ustalenie wymiarów\n",
    "    agents = env.possible_agents  # np. [agent_0, agent_1]\n",
    "    \n",
    "    n_agents = len(agents)\n",
    "    assert n_agents == args.n_agents, \"Nie zgadza się liczba agentów!\"\n",
    "    act_dim_each = env.action_space(agents[0]).n  # Liczba akcji dyskretnych\n",
    "    state_dim = np.prod(env.state().shape)       # Globalny stan\n",
    "    obs_dim_each = np.prod(env.observation_space(agents[0]).shape)\n",
    "\n",
    "    print(env.action_space(agents[0])) \n",
    "    print(f\"State dim: {state_dim}, Obs dim each: {obs_dim_each}, Act dim each: {act_dim_each}, Total act dim: {act_dim_each * n_agents}\")\n",
    "\n",
    "    # 6.3 Inicjalizacja współdzielonych aktora i krytyka\n",
    "    agent_id_dim = args.n_agents  # One-hot encoding ID agenta\n",
    "\n",
    "    shared_actor = SharedActor(state_dim, agent_id_dim, act_dim_each).to(device)\n",
    "    shared_critic = SharedCritic(state_dim, act_dim_each).to(device)\n",
    "\n",
    "    target_critic = SharedCritic(state_dim, act_dim_each).to(device)\n",
    "    target_critic.load_state_dict(shared_critic.state_dict())\n",
    "\n",
    "    mixer = QMIXMixingNetwork(n_agents, state_dim).to(device)\n",
    "    target_mixer = QMIXMixingNetwork(n_agents, state_dim).to(device)\n",
    "    target_mixer.load_state_dict(mixer.state_dict())\n",
    "\n",
    "    # 6.4 Optymalizatory\n",
    "    actor_opt = optim.RMSprop(shared_actor.parameters(), lr=args.lr_actor, weight_decay=args.l2)\n",
    "    critic_opt = optim.RMSprop(shared_critic.parameters(), lr=args.lr_critic, weight_decay=args.l2)\n",
    "    mixer_opt = optim.RMSprop(mixer.parameters(), lr=args.lr_mixer, weight_decay=args.l2)\n",
    "    \n",
    "    # Dynamiczne alpha\n",
    "    log_alpha = nn.Parameter(torch.log(torch.tensor(0.2, device=device)), requires_grad=True)\n",
    "    alpha_optim = optim.RMSprop([log_alpha], lr=args.lr_alpha)\n",
    "    with torch.no_grad():\n",
    "        alpha = torch.clamp(log_alpha.exp(), min=1e-3, max=1.0)\n",
    "\n",
    "    # 6.5 Bufor replay\n",
    "    replay = ReplayBuffer(max_size=args.buffer_size)\n",
    "\n",
    "    # 6.6 TensorBoard\n",
    "    writer = SummaryWriter(comment=f\"_{args.exp_name}\")\n",
    "    episode_rewards = []\n",
    "\n",
    "    # Epsilon-Greedy\n",
    "    class EpsilonGreedy:\n",
    "        def __init__(self, start=1.0, end=0.05, decay_steps=50000):\n",
    "            self.start = start\n",
    "            self.end = end\n",
    "            self.decay_steps = decay_steps\n",
    "            self.step = 0\n",
    "        \n",
    "        def get_epsilon(self):\n",
    "            epsilon = self.end + (self.start - self.end) * max(0, (self.decay_steps - self.step)) / self.decay_steps\n",
    "            self.step += 1\n",
    "            return epsilon\n",
    "\n",
    "    epsilon_greedy = EpsilonGreedy(start=args.epsilon_start, end=args.epsilon_end, decay_steps=args.epsilon_decay_steps)\n",
    "\n",
    "    def soft_update(source_net, target_net, tau):\n",
    "        for p, tp in zip(source_net.parameters(), target_net.parameters()):\n",
    "            tp.data.copy_(tau * p.data + (1.0 - tau) * tp.data)\n",
    "\n",
    "    def evaluate_policy(episode_idx):\n",
    "        \"\"\"\n",
    "        Uruchamia kilka epizodów w trybie testowym (bez noise), liczy średni zwrot.\n",
    "        Loguje do TB.\n",
    "        \"\"\"\n",
    "        eval_episodes = args.num_eval_episodes\n",
    "        returns = []\n",
    "        for _ in range(eval_episodes):\n",
    "            obs_dict, _ = env.reset()\n",
    "            done_dict = {ag: False for ag in agents}\n",
    "            ep_ret = 0.0\n",
    "            state_np = env.state()\n",
    "            while not all(done_dict.values()):\n",
    "                actions_dict = {}\n",
    "                for i, ag in enumerate(agents):\n",
    "                    agent_id = get_agent_id(i, args.n_agents)\n",
    "                    # state_with_id = np.concatenate([state_np, agent_id])\n",
    "                    # state_tensor = torch.FloatTensor(state_with_id).unsqueeze(0).to(device)\n",
    "                    state_with_id = get_state_with_id(state_np, agent_id)\n",
    "                    state_tensor = torch.FloatTensor(state_with_id).to(device)\n",
    "                    with torch.no_grad():\n",
    "                        logits = shared_actor.forward(state_tensor)\n",
    "                        action = torch.argmax(logits, dim=-1).item()\n",
    "                    actions_dict[ag] = action\n",
    "                    writer.add_scalar(f\"debug/eval_action_{ag}\", action, episode_idx)\n",
    "                next_obs_dict, rews, done_dict, _, _ = env.step(actions_dict)\n",
    "                ep_ret += sum(rews.values())\n",
    "                state_np = env.state()\n",
    "            returns.append(ep_ret)\n",
    "        avg_ret = np.mean(returns)\n",
    "        writer.add_scalar(\"evaluate/avg_return\", avg_ret, episode_idx)\n",
    "        print(f\"[EVAL] Episode {episode_idx}, avg_return={avg_ret:.3f}\")\n",
    "\n",
    "    start_time = time.time() \n",
    "\n",
    "    # =====================================================================\n",
    "    # 6.7 Główna pętla treningowa\n",
    "    # =====================================================================\n",
    "    for timestep in range(args.total_timesteps):\n",
    "        obs_dict, _ = env.reset()\n",
    "        done_dict = {ag: False for ag in agents}\n",
    "        state_np = env.state()\n",
    "        ep_ret = 0.0\n",
    "        step = 0\n",
    "        \n",
    "        while not all(done_dict.values()) and step < args.max_cycles:\n",
    "            actions_dict = {}\n",
    "            local_obs_list = []\n",
    "            for i, ag in enumerate(agents):\n",
    "                agent_id = get_agent_id(i, args.n_agents)\n",
    "                state_with_id = np.concatenate([state_np, agent_id])\n",
    "                state_tensor = torch.FloatTensor(state_with_id).unsqueeze(0).to(device)\n",
    "                \n",
    "                epsilon = epsilon_greedy.get_epsilon()\n",
    "                if random.random() < epsilon:\n",
    "                    action_i = random.randint(0, act_dim_each - 1)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        logits = shared_actor.forward(state_tensor)\n",
    "                        action_i = torch.argmax(logits, dim=-1).item()\n",
    "                \n",
    "                actions_dict[ag] = action_i\n",
    "                local_obs_list.append(obs_dict[ag])\n",
    "\n",
    "            next_obs_dict, rew_dict, done_dict, _, _ = env.step(actions_dict)\n",
    "            next_state_np = env.state()\n",
    "            reward = sum(rew_dict.values())\n",
    "            done = any(done_dict.values())  # ep. done\n",
    "\n",
    "            # Zapis do bufora\n",
    "            act_list = [actions_dict[ag] for ag in agents]\n",
    "            replay.add(\n",
    "                state_np,\n",
    "                local_obs_list,\n",
    "                act_list,\n",
    "                reward,\n",
    "                next_state_np,\n",
    "                [next_obs_dict[ag] for ag in agents],\n",
    "                done\n",
    "            )\n",
    "\n",
    "            ep_ret += reward\n",
    "            state_np = next_state_np\n",
    "            obs_dict = next_obs_dict\n",
    "            step += 1\n",
    "            timestep += 1\n",
    "\n",
    "            # =====================================================================\n",
    "            # 6.7 Główna pętla treningowa - część: Trening\n",
    "            # =====================================================================\n",
    "            if (timestep > args.learning_starts) and (timestep % args.train_freq == 0):\n",
    "                # Sample batch\n",
    "                batch = replay.sample(args.batch_size)\n",
    "                if batch is None:\n",
    "                    continue\n",
    "                (states_b, obs_b, acts_b, rews_b, next_states_b, next_obs_b, dones_b) = batch\n",
    "                \n",
    "                # Konwersja danych do tensorów\n",
    "                batch_states = torch.FloatTensor(np.array(states_b)).to(device)                    # (B, state_dim)\n",
    "                batch_next_states = torch.FloatTensor(np.array(next_states_b)).to(device)          # (B, state_dim)\n",
    "                batch_rewards = torch.FloatTensor(np.array(rews_b)).unsqueeze(1).to(device)        # (B, 1)\n",
    "                batch_dones = torch.FloatTensor(np.array(dones_b)).unsqueeze(1).to(device)          # (B, 1)\n",
    "                \n",
    "                # Konwersja akcji na tensor (B, N)\n",
    "                batch_acts = torch.LongTensor(np.array(acts_b)).to(device)                         # (B, N)\n",
    "                batch_acts_one_hot = actions_to_one_hot(batch_acts, act_dim_each)                  # (B, N * act_dim_each)\n",
    "                \n",
    "                # ---------- Obliczanie target Q_tot ----------\n",
    "                # 1. Update Critics and Mixer\n",
    "                with torch.no_grad():\n",
    "                    # Calculate next actions and Q-values\n",
    "                    all_next_actions = []\n",
    "                    for i in range(n_agents):\n",
    "                        agent_id = get_agent_id(i, n_agents)\n",
    "                        state_with_id = get_state_with_id(next_states_b, agent_id, batch_size=args.batch_size)\n",
    "                        state_with_id_tensor = torch.FloatTensor(state_with_id).to(device)\n",
    "                        logits = shared_actor.forward(state_with_id_tensor)\n",
    "                        next_action = torch.argmax(logits, dim=-1)\n",
    "                        all_next_actions.append(next_action)\n",
    "                    \n",
    "                    # Compute target Q-values\n",
    "                    q_i_next = []\n",
    "                    for i in range(n_agents):\n",
    "                        q_val, _ = target_critic.forward(\n",
    "                            batch_next_states, \n",
    "                            F.one_hot(all_next_actions[i], num_classes=act_dim_each).float()\n",
    "                        )\n",
    "                        q_i_next.append(q_val)\n",
    "                    q_i_cat_next = torch.cat(q_i_next, dim=1).detach()\n",
    "                    q_tot_next = target_mixer(q_i_cat_next, batch_next_states)\n",
    "                    \n",
    "                    # Compute targets\n",
    "                    y = batch_rewards + (1 - batch_dones) * args.gamma * q_tot_next\n",
    "\n",
    "                # ---------- Obliczanie obecnych Q_tot ----------\n",
    "                q_i_current = []\n",
    "                for i in range(n_agents):\n",
    "                    q_val, _ = shared_critic.forward(batch_states, F.one_hot(batch_acts[:, i], num_classes=act_dim_each).float())\n",
    "                    \n",
    "                    action_one_hot = F.one_hot(batch_acts[:, i], num_classes=act_dim_each).float().requires_grad_(True)\n",
    "                    q_i_min = torch.min(q_val, q_val)  # Twin krytycy\n",
    "                    q_i_current.append(q_i_min)\n",
    "                q_i_cat_current = torch.cat(q_i_current, dim=1)  # (B, N)\n",
    "                q_tot_current = mixer(q_i_cat_current, batch_states)  # (B, 1)\n",
    "                \n",
    "                # Strata dla krytyków i miksującej\n",
    "                # critic_loss = F.mse_loss(q_tot_current, y).detach()\n",
    "\n",
    "                critic_loss = F.mse_loss(q_tot_current, y)\n",
    "                \n",
    "                # Backpropagation dla krytyków i miksującej\n",
    "                critic_opt.zero_grad()\n",
    "                mixer_opt.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                critic_opt.step()\n",
    "                mixer_opt.step()\n",
    "                \n",
    "                # ---------- Update Actor Policy ----------\n",
    "                total_actor_loss = 0\n",
    "                total_entropy = 0\n",
    "                \n",
    "                # ---------- Aktualizacja Polityki Aktora (SAC) ------------\n",
    "                for i in range(n_agents):\n",
    "                    agent_id = get_agent_id(i, n_agents)\n",
    "                    state_with_id = get_state_with_id(states_b, agent_id, batch_size=args.batch_size)\n",
    "                    state_with_id_tensor = torch.FloatTensor(state_with_id).to(device)\n",
    "                    \n",
    "                    # Get action distribution\n",
    "                    logits = shared_actor.forward(state_with_id_tensor)\n",
    "                    dist = torch.distributions.Categorical(logits=logits)\n",
    "                    new_actions = dist.sample()\n",
    "                    log_probs = dist.log_prob(new_actions).unsqueeze(1)\n",
    "                    entropy = dist.entropy().unsqueeze(1)\n",
    "                    \n",
    "                    # Compute Q-values for new actions\n",
    "                    q_i_new = []\n",
    "                    for j in range(n_agents):\n",
    "                        if j == i:\n",
    "                            action_one_hot = F.one_hot(new_actions, num_classes=act_dim_each).float()\n",
    "                        else:\n",
    "                            action_one_hot = F.one_hot(batch_acts[:, j], num_classes=act_dim_each).float()\n",
    "                        q_val, _ = shared_critic.forward(batch_states, action_one_hot)\n",
    "                        q_val = q_val.detach()  # Detach Q-values\n",
    "                        q_i_new.append(q_val)\n",
    "                    \n",
    "                    q_tot_new = mixer(torch.cat(q_i_new, dim=1), batch_states)\n",
    "                    \n",
    "                    # Actor loss for this agent\n",
    "                    actor_loss = -(q_tot_new - args.alpha * entropy).mean()\n",
    "                    \n",
    "                    # Update actor for this agent\n",
    "                    actor_opt.zero_grad()\n",
    "                    actor_loss.backward(retain_graph=(i < n_agents - 1))  # retain_graph for all but last agent\n",
    "                    torch.nn.utils.clip_grad_norm_(shared_actor.parameters(), 0.5)\n",
    "                    actor_opt.step()\n",
    "                    \n",
    "                    # Accumulate losses and entropy for logging\n",
    "                    total_actor_loss += actor_loss.item()\n",
    "                    total_entropy += entropy.mean().item()\n",
    "                    \n",
    "                    # Log individual agent metrics\n",
    "                    writer.add_scalar(f\"loss/actor_loss_agent_{i}\", actor_loss.item(), timestep)\n",
    "                    writer.add_scalar(f\"entropy/agent_{i}\", entropy.mean().item(), timestep)\n",
    "\n",
    "                # ---------- Update Alpha ----------\n",
    "                alpha = log_alpha.exp()\n",
    "                alpha_loss = -(log_alpha * (total_entropy/n_agents + args.target_entropy)).mean()\n",
    "                \n",
    "\n",
    "                # Update alpha\n",
    "                alpha_optim.zero_grad()\n",
    "                alpha_loss.backward()\n",
    "                alpha_optim.step()\n",
    "\n",
    "                # Clamp alpha\n",
    "                with torch.no_grad():\n",
    "                    alpha = torch.clamp(log_alpha.exp(), min=1e-3, max=1.0)\n",
    "\n",
    "                # Log losses\n",
    "                writer.add_scalar(\"loss/critic_loss\", critic_loss.item(), timestep)\n",
    "                writer.add_scalar(\"loss/alpha_loss\", alpha_loss.item(), timestep)\n",
    "                writer.add_scalar(\"alpha/value\", alpha.item(), timestep)\n",
    "\n",
    "                # ---------- Soft update targetów ------------\n",
    "                if timestep % args.target_update_interval == 0:\n",
    "                    soft_update(shared_critic, target_critic, args.tau)\n",
    "                    soft_update(mixer, target_mixer, args.tau)\n",
    "\n",
    "                # ---------- Logowanie do TB -----------\n",
    "                writer.add_scalar(\"q_values/q_tot_current_mean\", q_tot_current.mean().item(), timestep)\n",
    "                writer.add_scalar(\"q_values/q_tot_next_mean\", q_tot_next.mean().item(), timestep)\n",
    "                writer.add_scalar(\"q_values/y_mean\", y.mean().item(), timestep)\n",
    "                writer.add_scalar(\"q_values/q_tot\", q_tot_current.mean().item(), timestep)\n",
    "\n",
    "        # Logowanie zwrotów epizodu\n",
    "        episode_rewards.append(ep_ret)\n",
    "        writer.add_scalar(\"charts/episodic_return\", ep_ret, timestep)\n",
    "        writer.add_scalar(\"charts/average_return\", np.mean(episode_rewards[-100:]), timestep)\n",
    "\n",
    "        # Ewaluacja co określoną liczbę epizodów\n",
    "        if (timestep + 1) % args.eval_freq == 0:\n",
    "            evaluate_policy(timestep + 1)\n",
    "\n",
    "        # Monitorowanie postępu\n",
    "        if timestep % 1000 == 0:\n",
    "            print(f\"Timestep {timestep}, Avg Return: {np.mean(episode_rewards[-100:]):.2f}\")\n",
    "\n",
    "    env.close()\n",
    "    writer.close()\n",
    "\n",
    "    return episode_rewards\n",
    "\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 7. Uruchamianie\n",
    "# =====================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    args = get_args()\n",
    "    print(args)\n",
    "    rewards = run_qmix_sac(args)\n",
    "    print(\"Done. Last 10 episodes avg return:\", np.mean(rewards[-10:]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlzh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
