{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install stable-baselines3 numpy torch supersuit pettingzoo pymunk scipy gymnasium matplotlib einops tensorboard wandb imageio \n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from stable_baselines3 import SAC\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import supersuit as ss\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from distutils.util import strtobool\n",
    "from typing import Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import einops\n",
    "import gymnasium as gym\n",
    "from pettingzoo import ParallelEnv\n",
    "from pettingzoo.mpe import simple_spread_v3\n",
    "from pettingzoo.mpe import simple_v3\n",
    "from pettingzoo.mpe import simple_adversary_v3\n",
    "\n",
    "from pettingzoo.butterfly import knights_archers_zombies_v10\n",
    "\n",
    "from pettingzoo.utils.env import AgentID, ObsType\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "import collections\n",
    "\n",
    "\n",
    "from MASAC.masac.utils import extract_agent_id\n",
    "\n",
    "\n",
    "# from MASAC.masac.ma_buffer import MAReplayBuffer, Experience\n",
    "# from MASAC.masac.masac import concat_id\n",
    "from argparse import Namespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_timesteps = 10000\n",
    "time_now = datetime.now()\n",
    "exp_name = 'MASAC_QMIX_Circle_simple_adversary'\n",
    "os.makedirs('output/'+exp_name, exist_ok=True)\n",
    "# Definicja obiektu args z wymaganymi parametrami\n",
    "args = Namespace(\n",
    "    exp_name=exp_name,                      # Nazwa eksperymentu\n",
    "    seed=1,                                # Seed dla losowości\n",
    "    torch_deterministic=True,               # Czy używać deterministycznych operacji w PyTorch\n",
    "    cuda=True,                              # Czy używać CUDA (GPU)\n",
    "    track=False,                            # Czy śledzić eksperyment (np. za pomocą wandb)\n",
    "    wandb_project_name=\"Project_\" + exp_name + str(time_now.hour) + \":\" + str(time_now.minute),        # Nazwa projektu w wandb\n",
    "    wandb_entity='Entity_' + exp_name,               # Entity w wandb\n",
    "    capture_video=False,                    # Czy przechwytywać wideo\n",
    "    total_timesteps= total_timesteps,                 # Całkowita liczba kroków treningowych\n",
    "    buffer_size=1000000,                    # Rozmiar bufora replay\n",
    "    gamma=0.98,                             # Discount factor\n",
    "    tau=0.01,                              # Współczynnik tau do aktualizacji sieci docelowych\n",
    "    batch_size=128,                         # Rozmiar batcha\n",
    "    learning_starts=total_timesteps/10,                   # Krok, po którym zaczyna się nauka\n",
    "    policy_lr=5e-4,                         # Learning rate dla polityki\n",
    "    q_lr=5e-4,                              # Learning rate dla Q-function\n",
    "    policy_frequency=1,                     # Częstotliwość aktualizacji polityki\n",
    "    target_network_frequency=1,             # Częstotliwość aktualizacji sieci docelowych\n",
    "    alpha=0.2,                              # Waga entropii\n",
    "    autotune=True,                           # Czy automatycznie dostrajać alpha\n",
    "    save_frequency = total_timesteps/10,                  # Częstotliwość zapisywania modeli\n",
    "    actor_path='output/'+exp_name+\"/\",                     # Ścieżka do zapisu modelu aktora\n",
    ")\n",
    "\n",
    "# env setup\n",
    "# env = simple_v3.parallel_env(render_mode=None,  continuous_actions=True)\n",
    "env = simple_spread_v3.parallel_env(render_mode=None, N=2, local_ratio=0.5, max_cycles=40, continuous_actions=True)\n",
    "# env = simple_adversary_v3.parallel_env(render_mode=None, N=2, max_cycles=20, continuous_actions=True)\n",
    "env.reset(seed=args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftQNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        x = torch.cat([x, a], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_STD_MAX = 2\n",
    "LOG_STD_MIN = -5\n",
    "def concat_id(observation, agent_id):\n",
    "    agent_id_encoded = np.array([int(agent_id.split('_')[-1])], dtype=np.float32)  # Extract numerical ID\n",
    "    return np.concatenate((observation, agent_id_encoded))\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, observation_space_shape, action_space_shape):\n",
    "        super().__init__()\n",
    "        self.input_dim = np.prod(observation_space_shape) + 1\n",
    "\n",
    "        self.action_dim = np.prod(action_space_shape)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc_mean = nn.Linear(256, self.action_dim)\n",
    "        self.fc_logstd = nn.Linear(256, self.action_dim)\n",
    "\n",
    "        # Action rescaling\n",
    "        self.register_buffer(\"action_scale\", torch.tensor(1.0))  # Placeholder, updated later\n",
    "        self.register_buffer(\"action_bias\", torch.tensor(0.0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mean = self.fc_mean(x)\n",
    "        log_std = self.fc_logstd(x)\n",
    "        log_std = torch.tanh(log_std)\n",
    "        return mean, log_std\n",
    "\n",
    "\n",
    "    def get_action(self, x):\n",
    "        mean, log_std = self(x)\n",
    "        std = log_std.exp()\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        x_t = normal.rsample()\n",
    "        y_t = torch.tanh(x_t)\n",
    "        action = y_t * self.action_scale + self.action_bias\n",
    "        log_prob = normal.log_prob(x_t)\n",
    "        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        return action, log_prob, mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = collections.namedtuple(\n",
    "    \"Experience\",\n",
    "    field_names=[\n",
    "        \"global_obs\", \n",
    "        \"local_obs\", \n",
    "        \"joint_actions\", \n",
    "        \"rewards\", \n",
    "        \"next_global_obs\", \n",
    "        \"next_local_obs\", \n",
    "        \"terminateds\", \n",
    "        \"q_values\",  # Nowe: wartości Q\n",
    "        \"log_probs\"   # Nowe: logarytmy prawdopodobieństw akcji\n",
    "    ]\n",
    ")\n",
    "\n",
    "class MAReplayBuffer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        global_obs_shape,\n",
    "        local_obs_shapes: Dict[str, tuple],\n",
    "        action_dim,\n",
    "        num_agents=1,\n",
    "        max_size=100000,\n",
    "        obs_dtype=np.float32,\n",
    "        action_dtype=np.float32,\n",
    "    ):\n",
    "        self.max_size = max_size\n",
    "        self.ptr, self.size = 0, 0\n",
    "        self.obs_type = obs_dtype\n",
    "        self.action_type = action_dtype\n",
    "        self.global_obs = np.zeros((max_size,) + global_obs_shape, dtype=obs_dtype)\n",
    "        self.local_obs = {\n",
    "            agent_id: np.zeros((max_size,) + shape, dtype=obs_dtype)\n",
    "            for agent_id, shape in local_obs_shapes.items()\n",
    "        }\n",
    "        self.next_global_obs = np.zeros((max_size,) + global_obs_shape, dtype=obs_dtype)\n",
    "        self.next_local_obs = {\n",
    "            agent_id: np.zeros((max_size,) + shape, dtype=obs_dtype)\n",
    "            for agent_id, shape in local_obs_shapes.items()\n",
    "        }\n",
    "        self.joint_actions = np.zeros((max_size, action_dim), dtype=action_dtype)\n",
    "\n",
    "        self.rewards = np.zeros((max_size,), dtype=np.float32)\n",
    "        self.terminateds = np.zeros((max_size, 1), dtype=np.float32)\n",
    "        self.q_values = np.zeros((max_size, num_agents), dtype=np.float32)\n",
    "        self.log_probs = np.zeros((max_size, num_agents), dtype=np.float32)\n",
    "\n",
    "    def add(\n",
    "        self,\n",
    "        global_obs: np.ndarray,\n",
    "        local_obs: Dict[str, np.ndarray],\n",
    "        joint_actions: np.ndarray,\n",
    "        reward: float,\n",
    "        next_global_obs: np.ndarray,\n",
    "        next_local_obs: Dict[str, np.ndarray],\n",
    "        terminated: bool,\n",
    "        q_values: np.ndarray,\n",
    "        log_probs: np.ndarray,\n",
    "    ):\n",
    "        self.global_obs[self.ptr] = np.array(global_obs, dtype=self.obs_type).copy()\n",
    "        self.next_global_obs[self.ptr] = np.array(next_global_obs, self.obs_type).copy()\n",
    "        for agent_id, obs in local_obs.items():\n",
    "            self.local_obs[agent_id][self.ptr] = np.array(obs, dtype=self.obs_type).copy()\n",
    "        for agent_id, obs in next_local_obs.items():\n",
    "            self.next_local_obs[agent_id][self.ptr] = np.array(obs, dtype=self.obs_type).copy()\n",
    "        self.joint_actions[self.ptr] = np.array(joint_actions, dtype=self.action_type).reshape(-1).copy()\n",
    "        self.rewards[self.ptr] = np.array(reward, dtype=np.float32).copy()\n",
    "        self.terminateds[self.ptr] = np.array(terminated).copy()\n",
    "        self.q_values[self.ptr] = np.array(q_values, dtype=np.float32).copy()\n",
    "        self.log_probs[self.ptr] = np.array(log_probs, dtype=np.float32).copy()\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size, replace=True, use_cer=False, to_tensor=False, add_id_to_local_obs=False, device=None):\n",
    "        \"\"\"Sample a batch of experiences from the buffer.\n",
    "\n",
    "        Args:\n",
    "            batch_size: Batch size\n",
    "            replace: Whether to sample with replacement\n",
    "            use_cer: Whether to use CER\n",
    "            to_tensor: Whether to convert the data to PyTorch tensors\n",
    "            add_id_to_local_obs: Whether to add the agent id to the local observations\n",
    "            device: Device to use\n",
    "\n",
    "        Returns:\n",
    "            An experience tuple:\n",
    "                global_obs: Global observations (batch_size, global_obs_shape)\n",
    "                local_obs:\n",
    "                    Local observations of each agent (batch_size, num_agents, local_obs_shape)\n",
    "                    (!) the dict is flattened into a vector\n",
    "                    If add_id_to_local_obs is True, the local observations vectors are concatenated with the agent id\n",
    "                joint_actions: Actions of all agents (batch_size, num_agents * action_dim)\n",
    "                rewards: Rewards (batch_size,)\n",
    "                next_global_obs: Next global observations (batch_size, global_obs_shape)\n",
    "                next_local_obs: Next local observations of each agent (batch_size, num_agents, local_obs_shape) (!) the dict is flattened into a vector\n",
    "                terminateds: Whether the episode is terminated or not (batch_size, 1)\n",
    "\n",
    "        \"\"\"\n",
    "        inds = np.random.choice(self.size, batch_size, replace=replace)\n",
    "        if use_cer:\n",
    "            inds[0] = self.ptr - 1  # always use last experience\n",
    "\n",
    "        def flatten_local_obss(local_obs_dict, inds, to_tensor=False, device=None, add_id_to_local_obs=False):\n",
    "            batch_local_obs = {agent_id: [] for agent_id in local_obs_dict.keys()}\n",
    "            for agent_id, obs_array in local_obs_dict.items():\n",
    "                if to_tensor:\n",
    "                    batch_local_obs[agent_id] = torch.tensor(obs_array[inds], dtype=torch.float32).to(device)\n",
    "                else:\n",
    "                    batch_local_obs[agent_id] = obs_array[inds]\n",
    "            return batch_local_obs\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"Indices: {inds}\")\n",
    "        # for agent_id, obs in local_obs.items():\n",
    "        #     print(f\"Agent {agent_id}, Observations shape: {obs.shape}, Selected indices shape: {obs[inds].shape}\")\n",
    "\n",
    "\n",
    "        if to_tensor:\n",
    "            return Experience(\n",
    "                global_obs=torch.tensor(self.global_obs[inds]).to(device),\n",
    "                local_obs=flatten_local_obss(self.local_obs, inds, to_tensor=True, device=device, add_id_to_local_obs=add_id_to_local_obs),\n",
    "                joint_actions=torch.tensor(self.joint_actions[inds]).to(device),\n",
    "                rewards=torch.tensor(self.rewards[inds]).to(device),\n",
    "                next_global_obs=torch.tensor(self.next_global_obs[inds]).to(device),\n",
    "                next_local_obs=flatten_local_obss(self.next_local_obs, inds, to_tensor=True, device=device, add_id_to_local_obs=add_id_to_local_obs),\n",
    "                terminateds=torch.tensor(self.terminateds[inds]).to(device),\n",
    "                q_values=torch.tensor(self.q_values[inds]).to(device),\n",
    "                log_probs=torch.tensor(self.log_probs[inds]).to(device),\n",
    "            )\n",
    "        else:\n",
    "            return Experience(\n",
    "                global_obs=self.global_obs[inds],\n",
    "                local_obs=flatten_local_obss(self.local_obs, inds, to_tensor=False, device=device, add_id_to_local_obs=add_id_to_local_obs),\n",
    "                joint_actions=self.joint_actions[inds],\n",
    "                rewards=self.rewards[inds],\n",
    "                next_global_obs=self.next_global_obs[inds],\n",
    "                next_local_obs=flatten_local_obss(self.next_local_obs, inds, to_tensor=False, device=device, add_id_to_local_obs=add_id_to_local_obs),\n",
    "                terminateds=self.terminateds[inds],\n",
    "                q_values=self.q_values[inds],\n",
    "                log_probs=self.log_probs[inds],\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Get the size of the buffer.\"\"\"\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QMIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixingNetwork(nn.Module):\n",
    "    \"\"\"Mixing Network to compute Q_tot from Q_a.\"\"\"\n",
    "    def __init__(self, num_agents, state_dim, mixing_hidden_dim=32):\n",
    "        super(MixingNetwork, self).__init__()\n",
    "        self.num_agents = num_agents\n",
    "\n",
    "        # Hypernetwork for weights and biases\n",
    "        self.hyper_w1 = nn.Sequential(\n",
    "            nn.Linear(state_dim, mixing_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mixing_hidden_dim, num_agents * mixing_hidden_dim)\n",
    "        )\n",
    "        self.hyper_b1 = nn.Linear(state_dim, mixing_hidden_dim)\n",
    "\n",
    "        self.hyper_w2 = nn.Sequential(\n",
    "            nn.Linear(state_dim, mixing_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mixing_hidden_dim, mixing_hidden_dim)\n",
    "        )\n",
    "        self.hyper_b2 = nn.Sequential(\n",
    "            nn.Linear(state_dim, mixing_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mixing_hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        # Nonlinear layer to mix agent Q-values\n",
    "        self.non_linear = nn.ReLU()\n",
    "\n",
    "    def forward(self, q_values, state):\n",
    "        \"\"\"Forward pass for computing Q_tot.\"\"\"\n",
    "        batch_size = q_values.size(0)\n",
    "\n",
    "        # Compute first layer weights and biases\n",
    "        w1 = self.hyper_w1(state).view(batch_size, self.num_agents, -1)\n",
    "        b1 = self.hyper_b1(state).view(batch_size, 1, -1)\n",
    "\n",
    "        # First mixing layer\n",
    "        hidden = self.non_linear(torch.bmm(q_values.unsqueeze(1), w1) + b1)\n",
    "\n",
    "        # Compute second layer weights and biases\n",
    "        w2 = self.hyper_w2(state).view(batch_size, -1, 1)\n",
    "        b2 = self.hyper_b2(state).view(batch_size, 1, 1)\n",
    "\n",
    "        # Second mixing layer\n",
    "        q_tot = torch.bmm(hidden, w2) + b2\n",
    "        return q_tot.squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klasa do zbierania i wyświetlania zwrotów z epizodów\n",
    "class Plotter:\n",
    "    def __init__(self):\n",
    "        self.returns_trained = []\n",
    "        self.episodes_trained = []\n",
    "        self.returns_random = []\n",
    "        self.episodes_random = []\n",
    "\n",
    "    def add_return(self, episode, return_value, agent_type='trained'):\n",
    "        if agent_type == 'trained':\n",
    "            self.episodes_trained.append(episode)\n",
    "            self.returns_trained.append(return_value)\n",
    "        elif agent_type == 'random':\n",
    "            self.episodes_random.append(episode)\n",
    "            self.returns_random.append(return_value)\n",
    "\n",
    "    def plot_returns(self):\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.plot(self.episodes_trained, self.returns_trained, label='Wytrenowany Agent', marker='o')\n",
    "        plt.plot(self.episodes_random, self.returns_random, label='Agent Losowy', marker='x')\n",
    "        plt.xlabel('Epizod')\n",
    "        plt.ylabel('Zwrot')\n",
    "        plt.title('Porównanie Zwrotów: Wytrenowany Agent vs Agent Losowy')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# Klasa do wizualizacji działania agenta\n",
    "class AgentVisualizer:\n",
    "    def __init__(self, actor_path, device='cpu'):\n",
    "        \"\"\"\n",
    "        Inicjalizuje wizualizatora agenta.\n",
    "\n",
    "        Args:\n",
    "            actor_path (str): Ścieżka do pliku z zapisanym modelem aktora (actor.pth).\n",
    "            device (str, optional): Urządzenie do obliczeń ('cpu' lub 'cuda'). Domyślnie 'cpu'.\n",
    "        \"\"\"\n",
    "        self.device = torch.device(device if torch.cuda.is_available() and device == 'cuda' else 'cpu')\n",
    "        # Używamy oryginalnego środowiska PettingZoo z renderowaniem 'rgb_array'\n",
    "\n",
    "        self.env = env\n",
    "        # self.env = simple_spread_v3.parallel_env(\n",
    "        #     N=3, \n",
    "        #     local_ratio=0.5, \n",
    "        #     max_cycles=25, \n",
    "        #     continuous_actions=True,\n",
    "        #     render_mode=\"rgb_array\"  # Poprawne ustawienie render_mode\n",
    "        # )\n",
    "        self.actor = Actor(self.env)\n",
    "        self.actor.load_state_dict(torch.load(actor_path, map_location=self.device))\n",
    "        self.actor.to(self.device)\n",
    "        self.actor.eval()\n",
    "        self.plotter = Plotter()\n",
    "\n",
    "        # Sprawdzenie dostępnych trybów renderowania\n",
    "        available_render_modes = self.env.metadata.get('render_modes', None)\n",
    "        print(\"Dostępne tryby renderowania:\", available_render_modes)\n",
    "\n",
    "    def run_episodes(self, num_episodes=10, agent_type='trained', render=False, save_gif=False, gif_path='agent_demo.gif'):\n",
    "        \"\"\"\n",
    "        Uruchamia określoną liczbę epizodów z wytrenowanym lub losowym agentem.\n",
    "\n",
    "        Args:\n",
    "            num_episodes (int, optional): Liczba epizodów do uruchomienia. Domyślnie 10.\n",
    "            agent_type (str, optional): Typ agenta ('trained' lub 'random'). Domyślnie 'trained'.\n",
    "            render (bool, optional): Czy renderować epizody. Domyślnie False.\n",
    "            save_gif (bool, optional): Czy zapisać renderowane epizody jako GIF. Domyślnie False.\n",
    "            gif_path (str, optional): Ścieżka do zapisu GIF. Domyślnie 'agent_demo.gif'.\n",
    "        \"\"\"\n",
    "        frames = []\n",
    "        for episode in range(1, num_episodes + 1):\n",
    "            obs, info = self.env.reset(seed=42 + episode)\n",
    "            global_return = 0.0\n",
    "            done = False\n",
    "            while not done:\n",
    "                actions = {}\n",
    "                if agent_type == 'trained':\n",
    "                    with torch.no_grad():\n",
    "                        # for agent_id in self.env.possible_agents:\n",
    "                        #     # Przygotowanie obserwacji z ID agenta\n",
    "                        #     obs_with_id = concat_id(obs[agent_id], agent_id)\n",
    "                        #     obs_tensor = torch.Tensor(obs_with_id).to(self.device)\n",
    "                        #     # Dodanie wymiaru batch (1, ...)\n",
    "                        #     obs_tensor = obs_tensor.unsqueeze(0)\n",
    "                        #     action, _, _ = self.actor.get_action(obs_tensor)\n",
    "                        #     # Przekonwertowanie akcji na numpy\n",
    "                        #     actions[agent_id] = action.cpu().numpy().flatten()\n",
    "                        for agent_id in self.env.possible_agents:\n",
    "                            obs_tensor = torch.Tensor(obs[agent_id]).to(self.device).unsqueeze(0)\n",
    "                            action, _, _ = actors[agent_id].get_action(obs_tensor)\n",
    "                            actions[agent_id] = action.cpu().numpy().flatten()\n",
    "\n",
    "                elif agent_type == 'random':\n",
    "                    for agent_id in self.env.possible_agents:\n",
    "                        # Przykładowe akcje losowe zgodne z przestrzenią akcji\n",
    "                        actions[agent_id] = self.env.action_space(agent_id).sample()\n",
    "                else:\n",
    "                    raise ValueError(\"Nieznany typ agenta. Użyj 'trained' lub 'random'.\")\n",
    "\n",
    "                # Wykonanie akcji w środowisku\n",
    "                next_obs, rewards, terminations, truncations, infos = self.env.step(actions)\n",
    "                done = any(terminations.values()) or any(truncations.values())\n",
    "\n",
    "                if render:\n",
    "                    try:\n",
    "                        # Renderowanie środowiska w trybie 'rgb_array'\n",
    "                        frame = self.env.render()\n",
    "                        if frame is not None:\n",
    "                            frames.append(frame)\n",
    "                        else:\n",
    "                            print(\"Renderowanie zwróciło None.\")\n",
    "                    except TypeError as e:\n",
    "                        print(f\"Nieudane renderowanie z mode='rgb_array': {e}\")\n",
    "                        print(\"Spróbuj wywołać render bez argumentów lub z innym trybem.\")\n",
    "                        try:\n",
    "                            frame = self.env.render()\n",
    "                            if frame is not None:\n",
    "                                frames.append(frame)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Nieudane renderowanie bez trybu: {e}\")\n",
    "\n",
    "                # Sumowanie zwrotów\n",
    "                global_return += sum(rewards.values())\n",
    "\n",
    "                # Aktualizacja obserwacji\n",
    "                obs = next_obs\n",
    "\n",
    "            # Dodanie zwrotu do Plotter\n",
    "            self.plotter.add_return(episode, global_return, agent_type=agent_type)\n",
    "            print(f\"Epizod {episode} ({agent_type}): Zwrot = {global_return}\")\n",
    "\n",
    "        if save_gif and frames:\n",
    "            try:\n",
    "                imageio.mimsave(gif_path, frames, fps=10)\n",
    "                print(f\"Zapisano wideo jako '{gif_path}'\")\n",
    "            except Exception as e:\n",
    "                print(f\"Nie udało się zapisać GIF: {e}\")\n",
    "\n",
    "        if render and frames:\n",
    "            # Wyświetlenie kilku pierwszych klatek jako przykładu\n",
    "            num_frames_to_show = min(5, len(frames))\n",
    "            for i in range(num_frames_to_show):\n",
    "                plt.figure(figsize=(5,5))\n",
    "                plt.imshow(frames[i])\n",
    "                plt.axis('off')\n",
    "                plt.title(f'Klatka {i+1}')\n",
    "                plt.show()\n",
    "\n",
    "    def plot_returns(self):\n",
    "        \"\"\"\n",
    "        Wyświetla wykres zwrotów z epizodów dla obu agentów.\n",
    "        \"\"\"\n",
    "        self.plotter.plot_returns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f\"{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "if args.track:\n",
    "    import wandb\n",
    "\n",
    "    wandb.init(\n",
    "        project=args.wandb_project_name,\n",
    "        entity=args.wandb_entity,\n",
    "        sync_tensorboard=True,\n",
    "        config=vars(args),\n",
    "        name=run_name,\n",
    "        monitor_gym=False,\n",
    "        save_code=True,\n",
    "    )\n",
    "writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "writer.add_text(\n",
    "    \"hyperparameters\",\n",
    "    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    ")\n",
    "\n",
    "# TRY NOT TO MODIFY: seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "# device = torch.device(\"mps\") if torch.backends.mps.is_available() else device\n",
    "\n",
    "\n",
    "env.reset(seed=args.seed)\n",
    "single_action_space = env.action_space(env.unwrapped.agents[0])\n",
    "single_observation_space = env.observation_space(env.unwrapped.agents[0])\n",
    "assert isinstance(single_action_space, gym.spaces.Box), \"only continuous action space is supported\"\n",
    "\n",
    "max_action = float(single_action_space.high[0])\n",
    "\n",
    "actors = {}\n",
    "for agent_id in env.possible_agents:\n",
    "    obs_shape = env.observation_space(agent_id).shape\n",
    "    act_shape = env.action_space(agent_id).shape\n",
    "    actors[agent_id] = Actor(observation_space_shape=obs_shape, action_space_shape=act_shape).to(device)\n",
    "\n",
    "total_action_dim = sum([np.prod(env.action_space(agent).shape) for agent in env.possible_agents])\n",
    "state_dim = np.prod(env.state().shape)\n",
    "action_dim = total_action_dim\n",
    "\n",
    "qf1 = SoftQNetwork(state_dim=state_dim, action_dim=action_dim).to(device)\n",
    "qf2 = SoftQNetwork(state_dim=state_dim, action_dim=action_dim).to(device)\n",
    "\n",
    "qf1_target = SoftQNetwork(state_dim=state_dim, action_dim=action_dim).to(device)\n",
    "qf2_target = SoftQNetwork(state_dim=state_dim, action_dim=action_dim).to(device)\n",
    "\n",
    "mixing_network = MixingNetwork(\n",
    "    num_agents=env.max_num_agents,\n",
    "    state_dim=np.prod(env.state().shape)\n",
    ").to(device)  # NEW: Instantiate the Mixing Network\n",
    "\n",
    "# Ładowanie starych wag\n",
    "# qf1_target.load_state_dict(qf1.state_dict())\n",
    "# qf2_target.load_state_dict(qf2.state_dict())\n",
    "q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)\n",
    "actor_optimizers = {\n",
    "    agent_id: optim.Adam(actors[agent_id].parameters(), lr=args.policy_lr)\n",
    "    for agent_id in env.possible_agents\n",
    "}\n",
    "\n",
    "mixing_optimizer = optim.Adam(mixing_network.parameters(), lr=args.q_lr)  # NEW: Optimizer for Mixing Network\n",
    "\n",
    "# Automatic entropy tuning\n",
    "if args.autotune:\n",
    "    target_entropy = -torch.prod(torch.Tensor(single_action_space.shape).to(device)).item()\n",
    "    log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "    alpha = log_alpha.exp().item()\n",
    "    a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)\n",
    "else:\n",
    "    alpha = args.alpha\n",
    "\n",
    "single_observation_space.dtype = np.float32\n",
    "\n",
    "local_obs_shapes = {\n",
    "    agent_id: env.observation_space(agent_id).shape\n",
    "    for agent_id in env.possible_agents\n",
    "}\n",
    "\n",
    "for agent_id in env.possible_agents:\n",
    "    print(f\"Agent: {agent_id}, Action space shape: {env.action_space(agent_id).shape}\")\n",
    "print(f\"Total action dimension: {action_dim}\")\n",
    "\n",
    "\n",
    "rb = MAReplayBuffer(\n",
    "    global_obs_shape=env.state().shape,\n",
    "    local_obs_shapes=local_obs_shapes,\n",
    "    action_dim=sum([np.prod(env.action_space(agent).shape) for agent in env.possible_agents]),\n",
    "    num_agents=len(env.possible_agents),\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# TRY NOT TO MODIFY: start the game\n",
    "obs, info = env.reset(seed=args.seed)\n",
    "global_return = 0.0\n",
    "global_obs: np.ndarray = env.state()\n",
    "for global_step in range(args.total_timesteps + 1):\n",
    "    # ALGO LOGIC: put action logic here\n",
    "    if global_step < args.learning_starts:\n",
    "        actions: Dict[str, np.ndarray] = {agent: env.action_space(agent).sample() for agent in env.possible_agents}\n",
    "    else:\n",
    "        actions: Dict[str, np.ndarray] = {}\n",
    "        with torch.no_grad():\n",
    "            for agent_id in env.possible_agents:\n",
    "                obs_with_id = concat_id(obs[agent_id], agent_id)  # Concatenate observation and agent ID\n",
    "                obs_tensor = torch.Tensor(obs_with_id).to(device).unsqueeze(0)\n",
    "                act, _, _ = actors[agent_id].get_action(obs_tensor)\n",
    "                actions[agent_id] = act.cpu().numpy().flatten()\n",
    "\n",
    "\n",
    "\n",
    "    # TRY NOT TO MODIFY: execute the game and log data.\n",
    "    next_obs: Dict[str, ObsType]\n",
    "    rewards: Dict[str, float]\n",
    "    next_obs, rewards, terminateds, truncateds, infos = env.step(actions)\n",
    "\n",
    "    terminated: bool = any(terminateds.values())\n",
    "    truncated: bool = any(truncateds.values())\n",
    "\n",
    "    # TRY NOT TO MODIFY: save data to replay buffer; handle `final_observation`\n",
    "    real_next_obs = next_obs\n",
    "    # TODO PZ doesn't have that yet\n",
    "    # if truncated:\n",
    "    #     real_next_obs = infos[\"final_observation\"].copy()\n",
    "    q_values = []\n",
    "    log_probs = []\n",
    "    # all_actions = np.concatenate([actions[agent] for agent in env.possible_agents], axis=-1)\n",
    "    # all_actions_tensor = torch.tensor(all_actions, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    all_actions = np.concatenate([actions[agent] for agent in env.possible_agents], axis=-1)\n",
    "    all_actions_tensor = torch.tensor(all_actions, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    q_value = torch.min(\n",
    "        qf1(torch.Tensor(global_obs).to(device).unsqueeze(0), all_actions_tensor),\n",
    "        qf2(torch.Tensor(global_obs).to(device).unsqueeze(0), all_actions_tensor),\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for agent_id in env.possible_agents:\n",
    "            obs_with_id = torch.Tensor(concat_id(obs[agent_id], agent_id)).to(device)\n",
    "            act, log_prob, _ = actors[agent_id].get_action(obs_with_id.unsqueeze(0))  # Access the specific actor instance\n",
    "            q_value = torch.min(\n",
    "                qf1(\n",
    "                    torch.Tensor(global_obs).to(device).unsqueeze(0), \n",
    "                    all_actions_tensor  \n",
    "                ),\n",
    "                qf2(\n",
    "                    torch.Tensor(global_obs).to(device).unsqueeze(0),\n",
    "                    all_actions_tensor\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Min(Q1, Q2)\n",
    "            q_values.append(q_value.item())\n",
    "            log_probs.append(log_prob.item())\n",
    "\n",
    "\n",
    "    rb.add(\n",
    "        global_obs=global_obs,\n",
    "        local_obs=obs,\n",
    "        joint_actions=np.array(list(actions.values())).flatten(),\n",
    "        reward=np.array(list(rewards.values())).sum(),\n",
    "        next_global_obs=env.state(),\n",
    "        next_local_obs=real_next_obs,\n",
    "        terminated=terminated,\n",
    "        q_values=np.array(q_values),  # Dodano q_values\n",
    "        log_probs=np.array(log_probs),  # Dodano log_probs\n",
    "    )\n",
    "\n",
    "    # TRY NOT TO MODIFY: CRUCIAL step easy to overlook\n",
    "    obs = next_obs\n",
    "    global_return += sum(rewards.values())\n",
    "    global_obs = env.state()\n",
    "\n",
    "\n",
    "    # ALGO LOGIC: training.\n",
    "    if global_step > args.learning_starts:\n",
    "        data: Experience = rb.sample(args.batch_size, to_tensor=True, device=device, add_id_to_local_obs=True)\n",
    "        with torch.no_grad():\n",
    "            # Determine the maximum shape across all agent observations\n",
    "            max_shape = np.array([obs.shape for obs in data.next_local_obs.values()]).max(axis=0)\n",
    "\n",
    "            # Pad all observations to match the maximum shape\n",
    "            padded_next_local_obs = []\n",
    "            for obs in data.next_local_obs.values():\n",
    "                # Calculate padding for each dimension\n",
    "                padding = [(0, max_size - obs.size(dim)) for dim, max_size in enumerate(max_shape)]\n",
    "                # Flatten padding list\n",
    "                flat_padding = [p for pair in reversed(padding) for p in pair]  # Reverse for torch padding order\n",
    "                padded_obs = torch.nn.functional.pad(torch.Tensor(obs).to(device), flat_padding)\n",
    "                padded_next_local_obs.append(padded_obs)\n",
    "\n",
    "            # Concatenate padded observations\n",
    "            flattened_next_local_obs = torch.cat(padded_next_local_obs, dim=0)\n",
    "\n",
    "            # Reshape for batch processing\n",
    "            flattened_next_local_obs = flattened_next_local_obs.reshape(\n",
    "                (args.batch_size * len(env.possible_agents), -1)  # Adjust shape dynamically\n",
    "            )\n",
    "\n",
    "            # Forward pass to get next actions and log probabilities\n",
    "            next_state_actions, next_state_log_pi, _ = actors.get_action(flattened_next_local_obs)\n",
    "            next_joint_actions = next_state_actions.reshape(\n",
    "                (args.batch_size, np.prod(single_action_space.shape) * len(env.possible_agents))\n",
    "            )\n",
    "\n",
    "            # Sums the log probabilities of the actions in the agent dimension to get the joint log probability\n",
    "            next_state_log_pi = einops.reduce(\n",
    "                next_state_log_pi.reshape((args.batch_size, len(env.possible_agents))), \"b a -> b ()\", \"sum\"\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # SAC Bellman equation\n",
    "            qf1_next_target = qf1_target(data.next_global_obs, next_joint_actions)\n",
    "            qf2_next_target = qf2_target(data.next_global_obs, next_joint_actions)\n",
    "            min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi\n",
    "            next_q_value = data.rewards.flatten() + (1 - data.terminateds.flatten()) * args.gamma * (\n",
    "                min_qf_next_target\n",
    "            ).view(-1)\n",
    "\n",
    "        # Computes q loss\n",
    "        qf1_a_values = qf1(data.global_obs, data.joint_actions).view(-1)\n",
    "        qf2_a_values = qf2(data.global_obs, data.joint_actions).view(-1)\n",
    "        qf1_loss = F.mse_loss(qf1_a_values, next_q_value)\n",
    "        qf2_loss = F.mse_loss(qf2_a_values, next_q_value)\n",
    "        \n",
    "        qf_loss = qf1_loss + qf2_loss\n",
    "\n",
    "        # NEW: Dodanie Mixing Network\n",
    "        q_tot = mixing_network(q_values=data.q_values, state=data.global_obs)\n",
    "        mixing_loss_term = F.mse_loss(q_tot, next_q_value.unsqueeze(1))\n",
    "        qf_loss = qf1_loss + qf2_loss + mixing_loss_term\n",
    "\n",
    "        q_optimizer.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        q_optimizer.step()\n",
    "\n",
    "        # NEW: Compute Q_tot using Mixing Network\n",
    "        q_tot = mixing_network(q_values=data.q_values, state=data.global_obs)\n",
    "        mixing_loss = F.mse_loss(q_tot, next_q_value.unsqueeze(1))  # Match Q_tot with target Q\n",
    "\n",
    "        mixing_optimizer.zero_grad()\n",
    "        mixing_loss.backward()\n",
    "        mixing_optimizer.step()\n",
    "\n",
    "        if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support\n",
    "            for _ in range(\n",
    "                args.policy_frequency\n",
    "            ):  # compensate for the delay by doing 'actor_update_interval' instead of 1\n",
    "                # flatten data.local_obs to forward for all agents at once\n",
    "                flattened_local_obs = data.local_obs.reshape(\n",
    "                    (args.batch_size * env.unwrapped.max_num_agents, np.prod(single_observation_space.shape) + 1)\n",
    "                )\n",
    "                # forward pass to get next actions and log probs\n",
    "                pi, log_pi, _ = actors.get_action(flattened_local_obs)\n",
    "                next_joint_actions = pi.reshape(\n",
    "                    (args.batch_size, np.prod(single_action_space.shape) * env.unwrapped.max_num_agents)\n",
    "                )\n",
    "                # Sums the log probs of the actions in the agent dimension to get the joint log prob\n",
    "                log_pi = einops.reduce(\n",
    "                    log_pi.reshape((args.batch_size, env.unwrapped.max_num_agents)), \"b a -> b ()\", \"sum\"\n",
    "                )\n",
    "\n",
    "                # SAC pi update\n",
    "                qf1_pi = qf1(data.global_obs, next_joint_actions)\n",
    "                qf2_pi = qf2(data.global_obs, next_joint_actions)\n",
    "                min_qf_pi = torch.min(qf1_pi, qf2_pi).view(-1)\n",
    "                actor_loss = ((alpha * log_pi) - min_qf_pi).mean()\n",
    "\n",
    "                actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                actor_optimizer.step()\n",
    "\n",
    "                if args.autotune:\n",
    "                    with torch.no_grad():\n",
    "                        # Oblicz log_probs dla bieżących akcji\n",
    "                        _, log_pi, _ = actors.get_action(flattened_local_obs)\n",
    "                        log_pi = einops.reduce(\n",
    "                            log_pi.reshape((args.batch_size, env.unwrapped.max_num_agents)), \"b a -> b ()\", \"sum\"\n",
    "                        )\n",
    "\n",
    "                        # NEW: Pobierz wartość Q_tot z Mixing Network\n",
    "                        q_tot = mixing_network(q_values=data.q_values, state=data.global_obs)\n",
    "\n",
    "                    # NEW: Modyfikacja straty entropii z użyciem Q_tot\n",
    "                    alpha_loss = (-log_alpha * (log_pi + target_entropy) + q_tot.mean()).mean()\n",
    "\n",
    "                    a_optimizer.zero_grad()\n",
    "                    alpha_loss.backward()\n",
    "                    a_optimizer.step()\n",
    "                    alpha = log_alpha.exp().item()\n",
    "\n",
    "\n",
    "        # update the target networks\n",
    "        if global_step % args.target_network_frequency == 0:\n",
    "            for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):\n",
    "                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "            for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):\n",
    "                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "\n",
    "        if global_step % 100 == 0:\n",
    "            writer.add_scalar(\"losses/qf1_values\", qf1_a_values.mean().item(), global_step)\n",
    "            writer.add_scalar(\"losses/qf2_values\", qf2_a_values.mean().item(), global_step)\n",
    "            writer.add_scalar(\"losses/qf1_loss\", qf1_loss.item(), global_step)\n",
    "            writer.add_scalar(\"losses/qf2_loss\", qf2_loss.item(), global_step)\n",
    "            writer.add_scalar(\"losses/qf_loss\", qf_loss.item() / 2.0, global_step)\n",
    "            writer.add_scalar(\"losses/actor_loss\", actor_loss.item(), global_step)\n",
    "            writer.add_scalar(\"losses/alpha\", alpha, global_step)\n",
    "            writer.add_scalar(\"losses/mixing_loss\", mixing_loss.item(), global_step)  # NEW: Log mixing loss\n",
    "            writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "            if args.autotune:\n",
    "                writer.add_scalar(\"losses/alpha_loss\", alpha_loss.item(), global_step)\n",
    "\n",
    "    if global_step % args.save_frequency == 0:\n",
    "        for agent_id, actor in actors.items():\n",
    "            torch.save(actor.state_dict(), f\"{args.actor_path}/actor_{agent_id}_latest.pth\")\n",
    "\n",
    "        torch.save(qf1.state_dict(), f\"{args.actor_path}/qf1_latest.pth\")\n",
    "        torch.save(qf2.state_dict(), f\"{args.actor_path}/qf2_latest.pth\")\n",
    "        if args.autotune:\n",
    "            torch.save(log_alpha, f\"{args.actor_path}/log_alpha_latest.pth\")\n",
    "        print(f\"Saved latest models at step {global_step}\")\n",
    "\n",
    "       \n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "        writer.add_scalar(\"charts/return\", global_return, global_step)\n",
    "        global_return = 0.0\n",
    "        global_obs = env.state()\n",
    "\n",
    "save_point = f\"{args.actor_path}{global_step}.pth\"\n",
    "\n",
    "# Saves the trained actor for execution\n",
    "torch.save(actors.state_dict(), save_point)\n",
    "print(f\"Training finished. Model saved at {save_point}\")\n",
    "env.close()\n",
    "writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Inicjalizacja wizualizatora agenta\n",
    "print(f\"Loading model from: {save_point}\")\n",
    "render = True\n",
    "# env = simple_v3.parallel_env(render_mode=\"rgb_array\" if render else None,  continuous_actions=True)\n",
    "# env = simple_spread_v3.parallel_env(render_mode=\"rgb_array\" if render else None, N=2, local_ratio=0.5, max_cycles=20, continuous_actions=True)\n",
    "env = simple_adversary_v3.parallel_env(render_mode=\"rgb_array\" if render else None, N=2, max_cycles=20, continuous_actions=True)\n",
    "env.reset(seed=args.seed)  # Ensure the environment is reset\n",
    "visualizer = AgentVisualizer(actor_path=save_point, device='cuda')  # lub 'cpu' jeśli nie masz GPU\n",
    "\n",
    "# %%\n",
    "# Sprawdzenie dostępnych trybów renderowania\n",
    "print(\"Available render modes:\", visualizer.env.metadata.get('render_modes', 'No render modes available'))\n",
    "\n",
    "num_episodess=20\n",
    "\n",
    "# %%\n",
    "# Uruchomienie epizodów z wytrenowanym agentem\n",
    "print(\"\\n--- Uruchomienie epizodów z wytrenowanym agentem ---\")\n",
    "visualizer.run_episodes(\n",
    "    num_episodes=num_episodess, \n",
    "    agent_type='trained', \n",
    "    render=render, \n",
    "    save_gif=True, \n",
    "    gif_path=f\"{args.actor_path}{global_step}_Trained.gif\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Uruchomienie epizodów z agentem losowym\n",
    "print(\"\\n--- Uruchomienie epizodów z agentem losowym ---\")\n",
    "visualizer.run_episodes(\n",
    "    num_episodes=num_episodess, \n",
    "    agent_type='random', \n",
    "    render=render, \n",
    "    save_gif=True, \n",
    "    gif_path=f\"{args.actor_path}{global_step}_Random.gif\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Wyświetlenie wykresu zwrotów z obu agentów\n",
    "print(\"\\n--- Wykres porównujący zwroty wytrenowanego agenta z agentem losowym ---\")\n",
    "visualizer.plot_returns()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlzh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
