{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (2.4.1)\n",
      "Requirement already satisfied: numpy in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: supersuit in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (3.9.3)\n",
      "Requirement already satisfied: pettingzoo in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (1.24.3)\n",
      "Requirement already satisfied: pymunk in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (6.10.0)\n",
      "Requirement already satisfied: scipy in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (1.15.0)\n",
      "Requirement already satisfied: gymnasium in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (0.29.1)\n",
      "Requirement already satisfied: matplotlib in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: einops in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (0.8.0)\n",
      "Requirement already satisfied: tensorboard in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (2.18.0)\n",
      "Requirement already satisfied: wandb in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (0.19.4)\n",
      "Requirement already satisfied: imageio in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (2.35.0)\n",
      "Requirement already satisfied: cloudpickle in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from stable-baselines3) (3.0.0)\n",
      "Requirement already satisfied: pandas in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from stable-baselines3) (2.2.3)\n",
      "Requirement already satisfied: filelock in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: tinyscaler>=1.2.6 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from supersuit) (1.2.8)\n",
      "Requirement already satisfied: cffi>=1.17.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from pymunk) (1.17.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (1.69.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (5.29.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (72.2.0)\n",
      "Requirement already satisfied: six>1.9 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (4.2.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (6.0.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (2.10.5)\n",
      "Requirement already satisfied: pyyaml in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (2.20.0)\n",
      "Requirement already satisfied: setproctitle in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: pycparser in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from cffi>=1.17.1->pymunk) (2.22)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from pandas->stable-baselines3) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from pandas->stable-baselines3) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install stable-baselines3 numpy torch supersuit pettingzoo pymunk scipy gymnasium matplotlib einops tensorboard wandb imageio \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import time\n",
    "from collections import namedtuple, deque\n",
    "from typing import Dict, Any\n",
    "\n",
    "import gymnasium as gym\n",
    "from pettingzoo.mpe import simple_reference_v3\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original observation shape: (12,)\n",
      "Calculated obs_dim: 12\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 569\u001b[0m\n\u001b[1;32m    567\u001b[0m env \u001b[38;5;241m=\u001b[39m simple_spread_v3\u001b[38;5;241m.\u001b[39mparallel_env(render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, local_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, max_cycles\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m, continuous_actions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    568\u001b[0m env\u001b[38;5;241m.\u001b[39mreset(seed\u001b[38;5;241m=\u001b[39mSEED)\n\u001b[0;32m--> 569\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mQMIX_SAC_Agents\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m rewards \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain(total_episodes\u001b[38;5;241m=\u001b[39mTIMESTAMPS, max_steps\u001b[38;5;241m=\u001b[39mMAX_STEPS)\n",
      "Cell \u001b[0;32mIn[5], line 241\u001b[0m, in \u001b[0;36mQMIX_SAC_Agents.__init__\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculated obs_dim: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# obs_dim = np.prod(self.env.observation_space[self.agents[0]].shape)\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# self.obs_dim = len(example_obs)\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# self.obs_dim = np.prod(self.env.observation_space(self.agents[0]).shape)  # Flattened observation\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magents\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m            \u001b[38;5;66;03m# Action dimension\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# Dla ciągłych akcji: act_dim (np. 2D w MPE?)\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# Dla dyskretnych - trzeba zmodyfikować!\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# self.act_dim = env.action_space[self.agents[0]].shape[0]\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# self.act_dim = 1  # np. 1, 2, 5, itp., w zależności od środowiska\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Tworzymy Actor i TwinQCritic dla każdego agenta\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactors \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from pettingzoo.mpe import simple_spread_v3\n",
    "# from pettingzoo.sisl import multiwalker_v9\n",
    "# from pettingzoo.butterfly import pistonball_v6\n",
    "# =====================================================================\n",
    "# 1. PARAMETRY HIPER\n",
    "# =====================================================================\n",
    "TIMESTAMPS = 1000\n",
    "SEED = 82\n",
    "LEARNING_STARTS = TIMESTAMPS // 10\n",
    "LR_ACTOR = 1e-3\n",
    "LR_CRITIC = 1e-3\n",
    "LR_MIXER = 1e-3\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 256\n",
    "ALPHA = 0.2            # początkowe alpha (entropia w SAC) - może być uczone\n",
    "TAU = 0.005            # do aktualizacji wag \"soft update\"\n",
    "REPLAY_SIZE = 100000\n",
    "MAX_EPISODES = 1000\n",
    "MAX_STEPS = 50         # max liczba kroków na epizod\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# =====================================================================\n",
    "# 2. SIEC AKTORA (SAC)\n",
    "# =====================================================================\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim=128):\n",
    "        super(Actor, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Mu i log_sigma dla ciągłej polityki\n",
    "        self.mu_layer = nn.Linear(hidden_dim, act_dim)\n",
    "        self.log_sigma_layer = nn.Linear(hidden_dim, act_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = self.net(obs)\n",
    "        mu = self.mu_layer(x)\n",
    "        log_sigma = self.log_sigma_layer(x)\n",
    "        log_sigma = torch.clamp(log_sigma, min=-20, max=2)  # ograniczenie zakresu\n",
    "        return mu, log_sigma\n",
    "\n",
    "    def sample(self, obs):\n",
    "        \"\"\"Zwraca akcję i log_prob dla SAC\"\"\"\n",
    "        mu, log_sigma = self.forward(obs)\n",
    "        sigma = log_sigma.exp()\n",
    "        dist = Normal(mu, sigma)\n",
    "        z = dist.rsample()  # reparametrization trick\n",
    "        action = torch.tanh(z)  # ograniczamy akcję do (-1,1)\n",
    "        log_prob = dist.log_prob(z) - torch.log(1 - action.pow(2) + 1e-7)\n",
    "        return action, log_prob.sum(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 3. SIECI KRYTYKA (SAC) - TWIN Q\n",
    "# =====================================================================\n",
    "class QCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim=128):\n",
    "        super(QCritic, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim + act_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        obs = obs.view(obs.size(0), -1)  # Flatten the observations\n",
    "        x = torch.cat([obs, act], dim=-1)\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 4. QMIX - Mixing Network\n",
    "# =====================================================================\n",
    "class MixingNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Łączy lokalne Q_a (dla każdego agenta) w Q_tot w sposób monotoniczny.\n",
    "    Tutaj używamy prostej sieci i hiper-sieci generującej wagi i bias.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_agents, state_dim, hidden_dim=32):\n",
    "        super(MixingNetwork, self).__init__()\n",
    "        self.n_agents = n_agents\n",
    "        self.state_dim = state_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Hiper-sieci generujące wagi i biasy\n",
    "        self.hyper_w_1 = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim * n_agents),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * n_agents, n_agents * hidden_dim)\n",
    "        )\n",
    "        self.hyper_b_1 = nn.Linear(state_dim, hidden_dim)\n",
    "\n",
    "        self.hyper_w_2 = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.hyper_b_2 = nn.Linear(state_dim, 1)\n",
    "\n",
    "    def forward(self, q_values, state):\n",
    "        \"\"\"\n",
    "        q_values: (batch_size, n_agents)\n",
    "        state: (batch_size, state_dim)\n",
    "        \"\"\"\n",
    "        bs = q_values.size(0)\n",
    "\n",
    "        # layer 1\n",
    "        w1 = self.hyper_w_1(state)  # (bs, n_agents * hidden_dim)\n",
    "        b1 = self.hyper_b_1(state)  # (bs, hidden_dim)\n",
    "        w1 = w1.view(bs, self.n_agents, self.hidden_dim)\n",
    "\n",
    "        # (bs, n_agents) -> (bs, n_agents, 1)\n",
    "        q_values = q_values.unsqueeze(-1)\n",
    "\n",
    "        # Mnożenie i dodawanie bias (monotoniczność zależy m.in. od konstrukcji w1 >= 0)\n",
    "        # Aby zagwarantować monotoniczność, można np. wymusić ReLU na w1:\n",
    "        w1 = torch.abs(w1)  # prosty hack, by zachować >= 0\n",
    "        \n",
    "        hidden = torch.bmm(q_values.transpose(1,2), w1).squeeze(1) + b1  # (bs, hidden_dim)\n",
    "\n",
    "        # layer 2\n",
    "        w2 = self.hyper_w_2(state)  # (bs, hidden_dim)\n",
    "        w2 = torch.abs(w2)          # wymuszenie >= 0\n",
    "        b2 = self.hyper_b_2(state)  # (bs, 1)\n",
    "\n",
    "        # (bs, hidden_dim) * (bs, hidden_dim) -> (bs, 1)\n",
    "        q_tot = torch.sum(hidden * w2, dim=1, keepdim=True) + b2\n",
    "        return q_tot\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 5. BUFOR REPLAY\n",
    "# =====================================================================\n",
    "class MultiAgentReplayBuffer:\n",
    "    def __init__(self, max_size=REPLAY_SIZE, n_agents=1):\n",
    "        self.max_size = max_size\n",
    "        self.n_agents = n_agents\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        self.states = []\n",
    "        self.obs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        self.next_obs = []\n",
    "        self.dones = []\n",
    "\n",
    "    def add(self, state, obs, action, reward, next_state, next_obs, done):\n",
    "        # Konwersja danych do formatu NumPy\n",
    "        state = np.array(state)\n",
    "        obs = [np.array(o) for o in obs]\n",
    "        action = np.array(action)\n",
    "        reward = np.array(reward)\n",
    "        next_state = np.array(next_state)\n",
    "        next_obs = [np.array(o) for o in next_obs]\n",
    "        done = np.array(done)\n",
    "\n",
    "        if self.size < self.max_size:\n",
    "            self.states.append(state)\n",
    "            self.obs.append(obs)\n",
    "            self.actions.append(action)\n",
    "            self.rewards.append(reward)\n",
    "            self.next_states.append(next_state)\n",
    "            self.next_obs.append(next_obs)\n",
    "            self.dones.append(done)\n",
    "            self.size += 1\n",
    "        else:\n",
    "            idx = self.ptr\n",
    "            self.states[idx] = state\n",
    "            self.obs[idx] = obs\n",
    "            self.actions[idx] = action\n",
    "            self.rewards[idx] = reward\n",
    "            self.next_states[idx] = next_state\n",
    "            self.next_obs[idx] = next_obs\n",
    "            self.dones[idx] = done\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "\n",
    "    def sample(self, batch_size=BATCH_SIZE):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        \n",
    "        # Konwersja i spłaszczanie danych w momencie próbkowania\n",
    "        batch_state = torch.FloatTensor(np.array([self.states[i] for i in idxs])).to(DEVICE)\n",
    "        batch_obs = torch.FloatTensor(\n",
    "            np.array([np.stack(self.obs[i]).reshape(self.n_agents, -1) for i in idxs])\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        batch_action = torch.FloatTensor(np.array([self.actions[i] for i in idxs])).to(DEVICE)\n",
    "        batch_reward = torch.FloatTensor(np.array([self.rewards[i] for i in idxs])).to(DEVICE)\n",
    "        batch_next_state = torch.FloatTensor(np.array([self.next_states[i] for i in idxs])).to(DEVICE)\n",
    "        batch_next_obs = torch.FloatTensor(np.array([np.concatenate(self.next_obs[i]) for i in idxs])).to(DEVICE)\n",
    "        batch_next_obs = torch.FloatTensor(\n",
    "            np.array([np.stack(self.next_obs[i]).reshape(self.n_agents, -1) for i in idxs])\n",
    "        ).to(DEVICE)\n",
    "        batch_done = torch.FloatTensor(np.array([self.dones[i] for i in idxs])).to(DEVICE)\n",
    "        \n",
    "        return batch_state, batch_obs, batch_action, batch_reward, batch_next_state, batch_next_obs, batch_done\n",
    "\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 6. FUNKCJE POMOCNICZE (soft update)\n",
    "# =====================================================================\n",
    "def soft_update(net, target_net, tau=TAU):\n",
    "    for param, target_param in zip(net.parameters(), target_net.parameters()):\n",
    "        target_param.data.copy_(\n",
    "            tau * param.data + (1 - tau) * target_param.data\n",
    "        )\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 7. TRENER\n",
    "# =====================================================================\n",
    "class QMIX_SAC_Agents:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.agents = env.agents\n",
    "        self.n_agents = len(self.agents)\n",
    "        \n",
    "        # Zakładamy, że obs_dim i act_dim takie same dla wszystkich agentów\n",
    "        # example_obs, _ = env.reset()[self.agents[0]]\n",
    "        obs_dict, _ = env.reset(seed=SEED)\n",
    "        example_obs = obs_dict[self.agents[0]]\n",
    "        print(f\"Original observation shape: {example_obs.shape}\")\n",
    "        self.obs_dim = np.prod(example_obs.shape)  # Flattened observation dimension\n",
    "        print(f\"Calculated obs_dim: {self.obs_dim}\")\n",
    "\n",
    "        # obs_dim = np.prod(self.env.observation_space[self.agents[0]].shape)\n",
    "        # self.obs_dim = len(example_obs)\n",
    "        # self.obs_dim = np.prod(self.env.observation_space(self.agents[0]).shape)  # Flattened observation\n",
    "        self.act_dim = self.env.action_space(self.agents[0]).shape[0]            # Action dimension\n",
    "\n",
    "        # Dla ciągłych akcji: act_dim (np. 2D w MPE?)\n",
    "        # Dla dyskretnych - trzeba zmodyfikować!\n",
    "        # self.act_dim = env.action_space[self.agents[0]].shape[0]\n",
    "        # self.act_dim = 1  # np. 1, 2, 5, itp., w zależności od środowiska\n",
    "        \n",
    "        # Tworzymy Actor i TwinQCritic dla każdego agenta\n",
    "        self.actors = []\n",
    "        self.critic1 = []\n",
    "        self.critic2 = []\n",
    "        self.target_critic1 = []\n",
    "        self.target_critic2 = []\n",
    "\n",
    "        self.actor_opt = []\n",
    "        self.critic1_opt = []\n",
    "        self.critic2_opt = []\n",
    "\n",
    "        for _ in range(self.n_agents):\n",
    "            actor = Actor(self.obs_dim, self.act_dim).to(DEVICE)\n",
    "            critic1 = QCritic(self.obs_dim, self.act_dim).to(DEVICE)\n",
    "            critic2 = QCritic(self.obs_dim, self.act_dim).to(DEVICE)\n",
    "            t_critic1 = QCritic(self.obs_dim, self.act_dim).to(DEVICE)\n",
    "            t_critic2 = QCritic(self.obs_dim, self.act_dim).to(DEVICE)\n",
    "            t_critic1.load_state_dict(critic1.state_dict())\n",
    "            t_critic2.load_state_dict(critic2.state_dict())\n",
    "\n",
    "            self.actors.append(actor)\n",
    "            self.critic1.append(critic1)\n",
    "            self.critic2.append(critic2)\n",
    "            self.target_critic1.append(t_critic1)\n",
    "            self.target_critic2.append(t_critic2)\n",
    "\n",
    "            self.actor_opt.append(optim.Adam(actor.parameters(), lr=LR_ACTOR))\n",
    "            self.critic1_opt.append(optim.Adam(critic1.parameters(), lr=LR_CRITIC))\n",
    "            self.critic2_opt.append(optim.Adam(critic2.parameters(), lr=LR_CRITIC))\n",
    "\n",
    "        # Mixing Network\n",
    "        # Globalny stan ma dimension: np. sumaryczna obserwacja (zależy od środowiska)\n",
    "        # W \"simple_spread\" np. stan globalny to złożenie pozycji agentów, landmarków itp.\n",
    "        # Tu dla uproszczenia załóżmy, że bierzemy po prostu concatenation lokalnych obserwacji (n_agents * obs_dim).\n",
    "        self.global_state_dim = self.n_agents * self.obs_dim\n",
    "        self.mixer = MixingNetwork(self.n_agents, self.global_state_dim).to(DEVICE)\n",
    "        self.mixer_opt = optim.Adam(self.mixer.parameters(), lr=LR_MIXER)\n",
    "\n",
    "        # Stała entropii w SAC (może być uczona, tu uproszczenie)\n",
    "        self.alpha = ALPHA\n",
    "\n",
    "        # self.replay_buffer = MultiAgentReplayBuffer(REPLAY_SIZE)\n",
    "        self.replay_buffer = MultiAgentReplayBuffer(max_size=REPLAY_SIZE, n_agents=self.n_agents)\n",
    "        self.target_mixer = MixingNetwork(self.n_agents, self.global_state_dim).to(DEVICE)\n",
    "        self.target_mixer.load_state_dict(self.mixer.state_dict())\n",
    "    \n",
    "\n",
    "    def select_actions(self, obs_n, evaluate=False):\n",
    "        \"\"\"\n",
    "        obs_n: lista/tuple obserwacji (po jednej dla każdego agenta),\n",
    "               lub dict {agent_name: obs}\n",
    "        Zwraca listę akcji (torch niekoniecznie, bo tu step w env).\n",
    "        \"\"\"\n",
    "        actions = []\n",
    "        for i, obs in enumerate(obs_n):\n",
    "            obs_t = torch.FloatTensor(obs.flatten()).unsqueeze(0).to(DEVICE)\n",
    "            # obs_t = torch.FloatTensor(obs).unsqueeze(0).to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                if not evaluate:\n",
    "                    action, _ = self.actors[i].sample(obs_t)\n",
    "                else:\n",
    "                    # deterministycznie (mu zamiast sample)\n",
    "                    mu, _ = self.actors[i](obs_t)\n",
    "                    # action = torch.sigmoid(mu) # dla akcji w zakresie (0,1)\n",
    "                    action = torch.tanh(mu)  # dla akcji w zakresie (-1,1)\n",
    "                    \n",
    "            # action = action.clip(-1, 1).cpu().numpy().flatten()\n",
    "            # actions.append(action)\n",
    "            actions.append(action.cpu().numpy().flatten())\n",
    "        return actions\n",
    "\n",
    "    def update(self):\n",
    "        # Sprawdzamy czy mamy wystarczająco próbek\n",
    "        if self.replay_buffer.size < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        (batch_state,  # shape: (B, n_agents * obs_dim)\n",
    "         batch_obs,    # shape: (B, n_agents, obs_dim)\n",
    "         batch_action, # shape: (B, n_agents, act_dim)\n",
    "         batch_reward, # shape: (B, n_agents)\n",
    "         batch_next_state, \n",
    "         batch_next_obs,\n",
    "         batch_done) = self.replay_buffer.sample(BATCH_SIZE)\n",
    "        # --- DODAJEMY PRINTY DO DIAGNOZY ---\n",
    "        # print(\"batch_state shape:\", batch_state.shape)       # powinno być (B, n_agents*obs_dim)\n",
    "        # print(\"batch_obs shape:\", batch_obs.shape)           # (B, n_agents, obs_dim)\n",
    "        # print(\"batch_action shape:\", batch_action.shape)     # (B, n_agents, act_dim)\n",
    "        # print(\"batch_reward shape:\", batch_reward.shape)     # (B, n_agents)\n",
    "        # print(\"batch_done shape:\", batch_done.shape)         # (B, n_agents)\n",
    "\n",
    "        # =========================\n",
    "        # 1. Oblicz lokalne Q_i\n",
    "        # =========================\n",
    "        # batch_obs: (B, n_agents, obs_dim)\n",
    "        # musimy pętlować agentów i skleić\n",
    "        q_values1 = []\n",
    "        q_values2 = []\n",
    "        next_q_values1 = []\n",
    "        next_q_values2 = []\n",
    "\n",
    "        all_next_actions = []\n",
    "        all_next_log_probs = []\n",
    "\n",
    "        for i in range(self.n_agents):\n",
    "            obs_i = batch_obs[:, i, :].reshape(BATCH_SIZE, -1)  # Spłaszczenie obserwacji\n",
    "            obs_i = batch_obs[:, i, :].view(BATCH_SIZE, -1)  \n",
    "            act_i = batch_action[:, i, :].view(BATCH_SIZE, -1)  # Rozszerzenie wymiaru akcji\n",
    "\n",
    "            # obs_i = batch_obs[:, i, :]\n",
    "            # act_i = batch_action[:, i, :]\n",
    "            # Obliczamy Q_i(obs_i, act_i)\n",
    "            q1_i = self.critic1[i](obs_i, act_i)\n",
    "            q2_i = self.critic2[i](obs_i, act_i)\n",
    "            q_values1.append(q1_i)\n",
    "            q_values2.append(q2_i)\n",
    "\n",
    "            # Dla targetów\n",
    "            with torch.no_grad():\n",
    "                next_obs_i = batch_next_obs[:, i, :]\n",
    "                next_action_i, next_log_prob_i = self.actors[i].sample(next_obs_i)\n",
    "                nq1_i = self.target_critic1[i](next_obs_i, next_action_i)\n",
    "                nq2_i = self.target_critic2[i](next_obs_i, next_action_i)\n",
    "                next_q_values1.append(nq1_i)\n",
    "                next_q_values2.append(nq2_i)\n",
    "                all_next_actions.append(next_action_i)\n",
    "                all_next_log_probs.append(next_log_prob_i)\n",
    "\n",
    "        # stack i uzyskujemy shape (B, n_agents)\n",
    "        q_values1 = torch.cat(q_values1, dim=1)  # (B, n_agents)\n",
    "        q_values2 = torch.cat(q_values2, dim=1)  # (B, n_agents)\n",
    "\n",
    "        next_q_values1 = torch.cat(next_q_values1, dim=1)  # (B, n_agents)\n",
    "        next_q_values2 = torch.cat(next_q_values2, dim=1)  # (B, n_agents)\n",
    "\n",
    "        # =========================\n",
    "        # 2. QMIX: Q_tot i target Q_tot\n",
    "        # =========================\n",
    "        with torch.no_grad():\n",
    "            # Mix values from both critics using target mixer\n",
    "            next_Q_tot1 = self.target_mixer(next_q_values1, batch_next_state)\n",
    "            next_Q_tot2 = self.target_mixer(next_q_values2, batch_next_state)\n",
    "            next_Q_tot = torch.min(next_Q_tot1, next_Q_tot2)\n",
    "            \n",
    "            # Compute entropy term for all agents\n",
    "            next_log_probs = torch.cat(all_next_log_probs, dim=1)\n",
    "            entropy_term = self.alpha * torch.sum(next_log_probs, dim=1, keepdim=True)\n",
    "            \n",
    "            # Apply entropy after mixing\n",
    "            next_Q_tot_sac = next_Q_tot - entropy_term\n",
    "            \n",
    "            # Compute target with rewards and discounting\n",
    "            sum_rewards = torch.sum(batch_reward, dim=1, keepdim=True)\n",
    "            done_mask = torch.mean(batch_done, dim=1, keepdim=True)\n",
    "            target_Q_tot = sum_rewards + GAMMA * (1 - done_mask) * next_Q_tot_sac\n",
    "\n",
    "        # Obliczamy Q_tot z obecnych Q_values\n",
    "        # min_q_current = torch.min(q_values1, q_values2)\n",
    "        # Q_tot_current = self.mixer(min_q_current, batch_state)  # (B,1)\n",
    "\n",
    "\n",
    "        Q_tot1 = self.mixer(q_values1, batch_state)\n",
    "        Q_tot2 = self.mixer(q_values2, batch_state)\n",
    "        Q_tot_current = torch.min(Q_tot1, Q_tot2)\n",
    "\n",
    "        # =================================================================\n",
    "        # 2.a. Loss mixer (krok uczący QMIX i krytyków)\n",
    "        # =================================================================\n",
    "        # MSE pomiędzy Q_tot a targetem\n",
    "        td_error = (Q_tot_current - target_Q_tot.detach())\n",
    "        mixer_loss = (td_error ** 2).mean()\n",
    "\n",
    "        # Optymalizacja mixer i krytyków (łączymy, by propagować gradient)\n",
    "        self.mixer_opt.zero_grad()\n",
    "        for i in range(self.n_agents):\n",
    "            self.critic1_opt[i].zero_grad()\n",
    "            self.critic2_opt[i].zero_grad()\n",
    "\n",
    "        mixer_loss.backward()\n",
    "        self.mixer_opt.step()\n",
    "        for i in range(self.n_agents):\n",
    "            self.critic1_opt[i].step()\n",
    "            self.critic2_opt[i].step()\n",
    "\n",
    "        # =================================================================\n",
    "        # 2.b. Loss dla aktorów (SAC-style, ale z Q_tot w pętli)\n",
    "        # =================================================================\n",
    "        # Polityka każdego agenta stara się maksymalizować Q_tot\n",
    "        # Trzeba ponownie wygenerować akcje i obliczyć Q_tot\n",
    "        actor_losses = []\n",
    "        for i in range(self.n_agents):\n",
    "            obs_i = batch_obs[:, i, :]\n",
    "            action_i, log_prob_i = self.actors[i].sample(obs_i)\n",
    "            # Oblicz Q_i dla akcji próbnych\n",
    "            q1_i = self.critic1[i](obs_i, action_i)\n",
    "            q2_i = self.critic2[i](obs_i, action_i)\n",
    "            min_q_i = torch.min(q1_i, q2_i)\n",
    "\n",
    "            # Musimy złożyć min_q_i dla wszystkich agentów (zastąpić tylko i-tego,\n",
    "            # a pozostałe zostawić tak jak w batch_action).\n",
    "            # Sposób uproszczony:\n",
    "            all_Q = []\n",
    "            for j in range(self.n_agents):\n",
    "                if j == i:\n",
    "                    all_Q.append(min_q_i)\n",
    "                else:\n",
    "                    # Q_j(obs_j, action_j z batch_action)\n",
    "                    obs_j = batch_obs[:, j, :]\n",
    "                    act_j = batch_action[:, j, :]\n",
    "                    q1_j = self.critic1[j](obs_j, act_j)\n",
    "                    q2_j = self.critic2[j](obs_j, act_j)\n",
    "                    min_q_j = torch.min(q1_j, q2_j)\n",
    "                    all_Q.append(min_q_j)\n",
    "            # (B, n_agents)\n",
    "            all_Q = torch.cat(all_Q, dim=1)\n",
    "            # Entropia: log_prob_i -> shape (B,1), \n",
    "            # Możemy też sumować log_prob innych agentów, w zależności od projektu.\n",
    "            # Dla uproszczenia – w SAC standardowo odejmujemy alpha * log_prob.\n",
    "            # W wieloagentowym można by użyć również Q_mix -> zobaczyć jak log_prob i\n",
    "            # wpływa na Q_tot. Wymaga to nieco zagnieżdżonych obliczeń.\n",
    "            # Tutaj demonstrujemy prosty wariant: \n",
    "            # Q'_tot = mixer(all_Q - alpha * log_prob_i, state)\n",
    "\n",
    "            # Tworzymy tensor all_Q_sac, gdzie i-ta kolumna = min_q_i - alpha * log_prob_i,\n",
    "            # reszta kolumn = min_q_j (bez zmiany).\n",
    "            alpha_term = torch.zeros_like(all_Q)\n",
    "            alpha_term[:, i] = self.alpha * log_prob_i.squeeze(-1)\n",
    "            all_Q_sac = all_Q - alpha_term\n",
    "\n",
    "            Q_tot_actor = self.mixer(all_Q_sac, batch_state)\n",
    "            # Maksymalizujemy Q_tot, czyli minimalizujemy -Q_tot\n",
    "            actor_loss = (-Q_tot_actor).mean()\n",
    "\n",
    "            self.actor_opt[i].zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_opt[i].step()\n",
    "\n",
    "            actor_losses.append(actor_loss.item())\n",
    "\n",
    "        # =================================================================\n",
    "        # 2.c. Soft update targetów\n",
    "        # =================================================================\n",
    "        for i in range(self.n_agents):\n",
    "            soft_update(self.critic1[i], self.target_critic1[i], TAU)\n",
    "            soft_update(self.critic2[i], self.target_critic2[i], TAU)\n",
    "            soft_update(self.mixer, self.target_mixer, TAU)\n",
    "\n",
    "\n",
    "    def train(self, total_episodes=MAX_EPISODES, max_steps=MAX_STEPS):\n",
    "        ep_rewards = []\n",
    "        episode = 0\n",
    "        for ep in range(total_episodes):\n",
    "            obs_dict, info_dict = env.reset(seed=SEED)\n",
    "            done_dict = {agent: False for agent in self.agents}\n",
    "            step = 0\n",
    "            ep_reward = np.zeros(self.n_agents, dtype=np.float32)\n",
    "\n",
    "            while not all(done_dict.values()) and step < max_steps:\n",
    "                # Przygotowanie list (obs, actions)\n",
    "                current_obs = []\n",
    "                for agent_i in self.agents:\n",
    "                    current_obs.append(obs_dict[agent_i])\n",
    "\n",
    "                # Wybór akcji\n",
    "                actions = self.select_actions(current_obs, evaluate=False if episode < LEARNING_STARTS else True)\n",
    "\n",
    "                # Stworzenie słownika akcji do przekazania do env\n",
    "                action_dict = {}\n",
    "                for i, agent_i in enumerate(self.agents):\n",
    "                    action_dict[agent_i] = actions[i]\n",
    "\n",
    "                next_obs_dict, reward_dict, done_dict, _, _= self.env.step(action_dict)\n",
    "                # observations, rewards, terminations, truncations, infos\n",
    "\n",
    "                # Zapis do bufora\n",
    "                # global_state = prosta konkatenacja, w realnym projekcie: pełen stan\n",
    "                # global_state = np.concatenate(current_obs, axis=0)  # (n_agents*obs_dim,)\n",
    "                global_state = np.concatenate(\n",
    "                    [obs_dict[a].flatten() for a in self.agents], axis=0\n",
    "                )\n",
    "                next_global_state = np.concatenate(\n",
    "                    [next_obs_dict[a].flatten() for a in self.agents], axis=0\n",
    "                )\n",
    "\n",
    "                # Konwertujemy do list\n",
    "                reward_list = np.array([reward_dict[a] for a in self.agents])\n",
    "                done_list = np.array([done_dict[a] for a in self.agents], dtype=np.float32)\n",
    "\n",
    "                self.replay_buffer.add(\n",
    "                    global_state,\n",
    "                    current_obs,\n",
    "                    actions,\n",
    "                    reward_list,\n",
    "                    next_global_state,\n",
    "                    [next_obs_dict[a] for a in self.agents],\n",
    "                    done_list\n",
    "                )\n",
    "\n",
    "                # Sumaryczna nagroda w epizodzie\n",
    "                ep_reward += reward_list\n",
    "\n",
    "                # update obserwacji\n",
    "                obs_dict = next_obs_dict\n",
    "                step += 1\n",
    "\n",
    "                # Krok treningowy (off-policy)\n",
    "                self.update()\n",
    "            \n",
    "            # Koniec epizodu\n",
    "            # elavuate\n",
    "            episode += 1\n",
    "\n",
    "            ep_rewards.append(np.sum(ep_reward))\n",
    "            if (ep+1) % 10 == 0:\n",
    "                print(f\"Epizod: {ep+1}, reward: {np.mean(ep_rewards[-10:])}\")\n",
    "\n",
    "        return ep_rewards\n",
    "\n",
    "# =====================================================================\n",
    "# 8. URUCHOMIENIE\n",
    "# =====================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Dzięki temu środowisko ma 3 agentów, każdy z dwuwymiarowymi akcjami ciągłymi w przedziale [−1,1]\n",
    "    env = simple_spread_v3.parallel_env(render_mode=None, N=2, local_ratio=0.5, max_cycles=40, continuous_actions=True)\n",
    "    env.reset(seed=SEED)\n",
    "    trainer = QMIX_SAC_Agents(env)\n",
    "    rewards = trainer.train(total_episodes=TIMESTAMPS, max_steps=MAX_STEPS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# 9. TESTOWANIE WYUCZONEGO MODELU\n",
    "# =====================================================================\n",
    "def evaluate_trainer(trainer, env, episodes=10, max_steps=MAX_STEPS, render=False):\n",
    "    \"\"\"\n",
    "    Funkcja do testowania wytrenowanego modelu (trainer) w środowisku wieloagentowym.\n",
    "    \n",
    "    Arguments:\n",
    "    trainer -- obiekt wytrenowanego modelu (QMIX_SAC_Agents)\n",
    "    env -- środowisko wieloagentowe\n",
    "    episodes -- liczba epizodów do przetestowania\n",
    "    max_steps -- maksymalna liczba kroków w każdym epizodzie\n",
    "    render -- czy wyświetlać środowisko (jeśli obsługiwane)\n",
    "\n",
    "    Returns:\n",
    "    ep_rewards -- lista sumarycznych nagród dla każdego epizodu\n",
    "    \"\"\"\n",
    "    ep_rewards = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        obs_dict, info_dict = trainer.env.reset(seed=SEED)\n",
    "        print(f\"Epizod {ep+1}: trainer.env.agents = {trainer.env.agents}\")\n",
    "        if not trainer.env.agents:\n",
    "            raise ValueError(\"Lista agentów `trainer.env.agents` jest pusta po resecie środowiska.\")\n",
    "        \n",
    "        done_dict = {agent: False for agent in trainer.env.agents}\n",
    "        step = 0\n",
    "\n",
    "        # Inicjalizacja epizodycznej nagrody\n",
    "        num_agents = len(trainer.env.agents)\n",
    "        ep_reward = np.zeros(num_agents, dtype=np.float32)\n",
    "\n",
    "        while not all(done_dict.values()) and step < max_steps:\n",
    "            if render:\n",
    "                trainer.env.render()\n",
    "\n",
    "            # Sprawdzamy, czy są jeszcze agenci w środowisku\n",
    "            if len(trainer.env.agents) == 0:\n",
    "                print(\"Brak agentów w środowisku. Kończymy epizod.\")\n",
    "                break\n",
    "\n",
    "            # Pobranie aktualnych obserwacji\n",
    "            current_obs = [obs_dict[agent] for agent in trainer.env.agents]\n",
    "\n",
    "            # Wybór akcji przez wytrenowanego trenera\n",
    "            actions = trainer.select_actions(current_obs, evaluate=True)\n",
    "\n",
    "            # Stworzenie słownika akcji do przekazania dotrainer.env \n",
    "            action_dict = {agent: actions[i] for i, agent in enumerate(trainer.env.agents)}\n",
    "\n",
    "            # Krok środowiska\n",
    "            next_obs_dict, reward_dict, done_dict, _, _ = trainer.env.step(action_dict)\n",
    "\n",
    "            # Debugowanie: sprawdź `reward_dict` i `trainer.env.agents`\n",
    "            print(f\"reward_dict keys: {list(reward_dict.keys())}, trainer.env.agents: {trainer.env.agents}\")\n",
    "\n",
    "            # Zapis nagród dla tego epizodu\n",
    "            try:\n",
    "                ep_reward[:len(trainer.env.agents)] += np.array([reward_dict[agent] for agent in trainer.env.agents])\n",
    "            except KeyError as e:\n",
    "                raise ValueError(f\"Klucz {e} nie istnieje w `reward_dict`. Sprawdź `trainer.env.agents` i `reward_dict`.\") from e\n",
    "\n",
    "            # Aktualizacja obserwacji\n",
    "            obs_dict = next_obs_dict\n",
    "            step += 1\n",
    "\n",
    "        # Zapis sumarycznej nagrody dla epizodu\n",
    "        ep_rewards.append(np.sum(ep_reward))\n",
    "        print(f\"Epizod testowy: {ep+1}, reward: {np.sum(ep_reward)}\")\n",
    "\n",
    "    if render:\n",
    "        trainer.env.close()\n",
    "\n",
    "    print(f\"Średnia nagroda w testach: {np.mean(ep_rewards):.2f}\")\n",
    "    return ep_rewards\n",
    "\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 10. URUCHOMIENIE TESTÓW\n",
    "# =====================================================================\n",
    "print(\"Testowanie agenta...\")\n",
    "print(trainer.env)\n",
    "print(env.agents)\n",
    "test_rewards = evaluate_trainer(trainer, env, episodes=5, render=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlzh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
