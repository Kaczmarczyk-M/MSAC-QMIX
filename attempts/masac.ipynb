{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (2.4.1)\n",
      "Requirement already satisfied: numpy in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: supersuit in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (3.9.3)\n",
      "Requirement already satisfied: pettingzoo in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (1.24.3)\n",
      "Requirement already satisfied: pymunk in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (6.10.0)\n",
      "Requirement already satisfied: scipy in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (1.15.0)\n",
      "Requirement already satisfied: gymnasium in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (0.29.1)\n",
      "Requirement already satisfied: matplotlib in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: einops in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (0.8.0)\n",
      "Requirement already satisfied: tensorboard in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (2.18.0)\n",
      "Requirement already satisfied: wandb in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (0.19.4)\n",
      "Requirement already satisfied: imageio in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (2.35.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement collections (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for collections\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install stable-baselines3 numpy torch supersuit pettingzoo pymunk scipy gymnasium matplotlib einops tensorboard wandb imageio collections\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from stable_baselines3 import SAC\n",
    "from __future__ import annotations\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import supersuit as ss\n",
    "from pettingzoo.mpe import simple_adversary_v3\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from distutils.util import strtobool\n",
    "from typing import Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import einops\n",
    "import gymnasium as gym\n",
    "from pettingzoo import ParallelEnv\n",
    "from pettingzoo.mpe import simple_spread_v3\n",
    "from pettingzoo.mpe import simple_v3\n",
    "from pettingzoo.butterfly import knights_archers_zombies_v10\n",
    "\n",
    "from pettingzoo.utils.env import AgentID, ObsType\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "import collections\n",
    "\n",
    "\n",
    "from MASAC.masac.utils import extract_agent_id\n",
    "\n",
    "\n",
    "from MASAC.masac.ma_buffer import MAReplayBuffer, Experience\n",
    "from MASAC.masac.masac import concat_id\n",
    "from argparse import Namespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'agent_0': array([ 0.        ,  0.        , -0.12300815, -1.5349231 ], dtype=float32)},\n",
       " {'agent_0': {}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "total_timesteps = 5000\n",
    "time_now = datetime.now()\n",
    "exp_name = 'Circle_simple'\n",
    "os.makedirs('output/'+exp_name, exist_ok=True)\n",
    "# Definicja obiektu args z wymaganymi parametrami\n",
    "args = Namespace(\n",
    "    exp_name=exp_name,                      # Nazwa eksperymentu\n",
    "    seed=12,                                # Seed dla losowości\n",
    "    torch_deterministic=True,               # Czy używać deterministycznych operacji w PyTorch\n",
    "    cuda=True,                              # Czy używać CUDA (GPU)\n",
    "    track=False,                            # Czy śledzić eksperyment (np. za pomocą wandb)\n",
    "    wandb_project_name=\"Project_\" + exp_name + str(time_now.hour) + \":\" + str(time_now.minute),        # Nazwa projektu w wandb\n",
    "    wandb_entity='Entity_' + exp_name,               # Entity w wandb\n",
    "    capture_video=False,                    # Czy przechwytywać wideo\n",
    "    total_timesteps= total_timesteps,                 # Całkowita liczba kroków treningowych\n",
    "    buffer_size=1000000,                    # Rozmiar bufora replay\n",
    "    gamma=0.995,                             # Discount factor\n",
    "    tau=0.01,                              # Współczynnik tau do aktualizacji sieci docelowych\n",
    "    batch_size=2054,                         # Rozmiar batcha\n",
    "    learning_starts=total_timesteps/10,                   # Krok, po którym zaczyna się nauka\n",
    "    policy_lr=5e-4,                         # Learning rate dla polityki\n",
    "    q_lr=5e-4,                              # Learning rate dla Q-function\n",
    "    policy_frequency=1,                     # Częstotliwość aktualizacji polityki\n",
    "    target_network_frequency=1,             # Częstotliwość aktualizacji sieci docelowych\n",
    "    alpha=0.2,                              # Waga entropii\n",
    "    autotune=True,                           # Czy automatycznie dostrajać alpha\n",
    "    save_frequency = total_timesteps/10,                  # Częstotliwość zapisywania modeli\n",
    "    actor_path='output/'+exp_name+\"/\",                     # Ścieżka do zapisu modelu aktora\n",
    ")\n",
    "\n",
    "# env setup\n",
    "env = simple_v3.parallel_env(render_mode=None,  continuous_actions=True)\n",
    "# env = simple_spread_v3.parallel_env(render_mode=None, N=3, local_ratio=0.5, max_cycles=25, continuous_actions=True)\n",
    "env.reset(seed=args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftQNetwork(nn.Module):\n",
    "    def __init__(self, env: ParallelEnv):\n",
    "        super().__init__()\n",
    "        single_action_space = env.action_space(env.agents[0])\n",
    "        # Global state, joint actions space -> ... -> Q value\n",
    "        self.fc1 = nn.Linear(np.array(env.state().shape).prod() + np.prod(single_action_space.shape) * env.num_agents, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        x = torch.cat([x, a], 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_STD_MAX = 2\n",
    "LOG_STD_MIN = -5\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, env: ParallelEnv):\n",
    "        super().__init__()\n",
    "        if not hasattr(env, 'possible_agents'):\n",
    "            raise AttributeError(\"Środowisko nie ma atrybutu 'possible_agents'. Upewnij się, że używasz PettingZoo parallel_env.\")\n",
    "\n",
    "        single_action_space = env.action_space(env.agents[0])\n",
    "        single_observation_space = env.observation_space(env.agents[0])\n",
    "        # Local state, agent id -> ... -> local action\n",
    "        self.fc1 = nn.Linear(np.array(single_observation_space.shape).prod() + 1, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc_mean = nn.Linear(256, np.prod(single_action_space.shape))\n",
    "        self.fc_logstd = nn.Linear(256, np.prod(single_action_space.shape))\n",
    "        # action rescaling\n",
    "        self.register_buffer(\n",
    "            \"action_scale\", torch.tensor((single_action_space.high - single_action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\", torch.tensor((single_action_space.high + single_action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mean = self.fc_mean(x)\n",
    "        log_std = self.fc_logstd(x)\n",
    "        log_std = torch.tanh(log_std)\n",
    "        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats\n",
    "\n",
    "        return mean, log_std\n",
    "\n",
    "    def get_action(self, x):\n",
    "        mean, log_std = self(x)\n",
    "        std = log_std.exp()\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))\n",
    "        y_t = torch.tanh(x_t)\n",
    "        action = y_t * self.action_scale + self.action_bias\n",
    "        log_prob = normal.log_prob(x_t)\n",
    "        # Enforcing Action Bound\n",
    "        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "        return action, log_prob, mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klasa do zbierania i wyświetlania zwrotów z epizodów\n",
    "class Plotter:\n",
    "    def __init__(self):\n",
    "        self.returns_trained = []\n",
    "        self.episodes_trained = []\n",
    "        self.returns_random = []\n",
    "        self.episodes_random = []\n",
    "\n",
    "    def add_return(self, episode, return_value, agent_type='trained'):\n",
    "        if agent_type == 'trained':\n",
    "            self.episodes_trained.append(episode)\n",
    "            self.returns_trained.append(return_value)\n",
    "        elif agent_type == 'random':\n",
    "            self.episodes_random.append(episode)\n",
    "            self.returns_random.append(return_value)\n",
    "\n",
    "    def plot_returns(self):\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.plot(self.episodes_trained, self.returns_trained, label='Wytrenowany Agent', marker='o')\n",
    "        plt.plot(self.episodes_random, self.returns_random, label='Agent Losowy', marker='x')\n",
    "        plt.xlabel('Epizod')\n",
    "        plt.ylabel('Zwrot')\n",
    "        plt.title('Porównanie Zwrotów: Wytrenowany Agent vs Agent Losowy')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# Klasa do wizualizacji działania agenta\n",
    "class AgentVisualizer:\n",
    "    def __init__(self, actor_path, device='cpu'):\n",
    "        \"\"\"\n",
    "        Inicjalizuje wizualizatora agenta.\n",
    "\n",
    "        Args:\n",
    "            actor_path (str): Ścieżka do pliku z zapisanym modelem aktora (actor.pth).\n",
    "            device (str, optional): Urządzenie do obliczeń ('cpu' lub 'cuda'). Domyślnie 'cpu'.\n",
    "        \"\"\"\n",
    "        self.device = torch.device(device if torch.cuda.is_available() and device == 'cuda' else 'cpu')\n",
    "        # Używamy oryginalnego środowiska PettingZoo z renderowaniem 'rgb_array'\n",
    "\n",
    "        self.env = env\n",
    "        # self.env = simple_spread_v3.parallel_env(\n",
    "        #     N=3, \n",
    "        #     local_ratio=0.5, \n",
    "        #     max_cycles=25, \n",
    "        #     continuous_actions=True,\n",
    "        #     render_mode=\"rgb_array\"  # Poprawne ustawienie render_mode\n",
    "        # )\n",
    "        self.actor = Actor(self.env)\n",
    "        self.actor.load_state_dict(torch.load(actor_path, map_location=self.device))\n",
    "        self.actor.to(self.device)\n",
    "        self.actor.eval()\n",
    "        self.plotter = Plotter()\n",
    "\n",
    "        # Sprawdzenie dostępnych trybów renderowania\n",
    "        available_render_modes = self.env.metadata.get('render_modes', None)\n",
    "        print(\"Dostępne tryby renderowania:\", available_render_modes)\n",
    "\n",
    "    def run_episodes(self, num_episodes=10, agent_type='trained', render=False, save_gif=False, gif_path='agent_demo.gif'):\n",
    "        \"\"\"\n",
    "        Uruchamia określoną liczbę epizodów z wytrenowanym lub losowym agentem.\n",
    "\n",
    "        Args:\n",
    "            num_episodes (int, optional): Liczba epizodów do uruchomienia. Domyślnie 10.\n",
    "            agent_type (str, optional): Typ agenta ('trained' lub 'random'). Domyślnie 'trained'.\n",
    "            render (bool, optional): Czy renderować epizody. Domyślnie False.\n",
    "            save_gif (bool, optional): Czy zapisać renderowane epizody jako GIF. Domyślnie False.\n",
    "            gif_path (str, optional): Ścieżka do zapisu GIF. Domyślnie 'agent_demo.gif'.\n",
    "        \"\"\"\n",
    "        frames = []\n",
    "        for episode in range(1, num_episodes + 1):\n",
    "            obs, info = self.env.reset(seed=42 + episode)\n",
    "            global_return = 0.0\n",
    "            done = False\n",
    "            while not done:\n",
    "                actions = {}\n",
    "                if agent_type == 'trained':\n",
    "                    with torch.no_grad():\n",
    "                        for agent_id in self.env.possible_agents:\n",
    "                            # Przygotowanie obserwacji z ID agenta\n",
    "                            obs_with_id = concat_id(obs[agent_id], agent_id)\n",
    "                            obs_tensor = torch.Tensor(obs_with_id).to(self.device)\n",
    "                            # Dodanie wymiaru batch (1, ...)\n",
    "                            obs_tensor = obs_tensor.unsqueeze(0)\n",
    "                            action, _, _ = self.actor.get_action(obs_tensor)\n",
    "                            # Przekonwertowanie akcji na numpy\n",
    "                            actions[agent_id] = action.cpu().numpy().flatten()\n",
    "                elif agent_type == 'random':\n",
    "                    for agent_id in self.env.possible_agents:\n",
    "                        # Przykładowe akcje losowe zgodne z przestrzenią akcji\n",
    "                        actions[agent_id] = self.env.action_space(agent_id).sample()\n",
    "                else:\n",
    "                    raise ValueError(\"Nieznany typ agenta. Użyj 'trained' lub 'random'.\")\n",
    "\n",
    "                # Wykonanie akcji w środowisku\n",
    "                next_obs, rewards, terminations, truncations, infos = self.env.step(actions)\n",
    "                done = any(terminations.values()) or any(truncations.values())\n",
    "\n",
    "                if render:\n",
    "                    try:\n",
    "                        # Renderowanie środowiska w trybie 'rgb_array'\n",
    "                        frame = self.env.render()\n",
    "                        if frame is not None:\n",
    "                            frames.append(frame)\n",
    "                        else:\n",
    "                            print(\"Renderowanie zwróciło None.\")\n",
    "                    except TypeError as e:\n",
    "                        print(f\"Nieudane renderowanie z mode='rgb_array': {e}\")\n",
    "                        print(\"Spróbuj wywołać render bez argumentów lub z innym trybem.\")\n",
    "                        try:\n",
    "                            frame = self.env.render()\n",
    "                            if frame is not None:\n",
    "                                frames.append(frame)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Nieudane renderowanie bez trybu: {e}\")\n",
    "\n",
    "                # Sumowanie zwrotów\n",
    "                global_return += sum(rewards.values())\n",
    "\n",
    "                # Aktualizacja obserwacji\n",
    "                obs = next_obs\n",
    "\n",
    "            # Dodanie zwrotu do Plotter\n",
    "            self.plotter.add_return(episode, global_return, agent_type=agent_type)\n",
    "            print(f\"Epizod {episode} ({agent_type}): Zwrot = {global_return}\")\n",
    "\n",
    "        if save_gif and frames:\n",
    "            try:\n",
    "                imageio.mimsave(gif_path, frames, fps=10)\n",
    "                print(f\"Zapisano wideo jako '{gif_path}'\")\n",
    "            except Exception as e:\n",
    "                print(f\"Nie udało się zapisać GIF: {e}\")\n",
    "\n",
    "        if render and frames:\n",
    "            # Wyświetlenie kilku pierwszych klatek jako przykładu\n",
    "            num_frames_to_show = min(5, len(frames))\n",
    "            for i in range(num_frames_to_show):\n",
    "                plt.figure(figsize=(5,5))\n",
    "                plt.imshow(frames[i])\n",
    "                plt.axis('off')\n",
    "                plt.title(f'Klatka {i+1}')\n",
    "                plt.show()\n",
    "\n",
    "    def plot_returns(self):\n",
    "        \"\"\"\n",
    "        Wyświetla wykres zwrotów z epizodów dla obu agentów.\n",
    "        \"\"\"\n",
    "        self.plotter.plot_returns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = collections.namedtuple(\n",
    "    \"Experience\",\n",
    "    field_names=[\n",
    "        \"global_obs\", \n",
    "        \"local_obs\", \n",
    "        \"joint_actions\", \n",
    "        \"rewards\", \n",
    "        \"next_global_obs\", \n",
    "        \"next_local_obs\", \n",
    "        \"terminateds\", \n",
    "        \"q_values\",  # Nowe: wartości Q\n",
    "        \"log_probs\"   # Nowe: logarytmy prawdopodobieństw akcji\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class MAReplayBuffer:\n",
    "    \"\"\"Multi-agent replay buffer for multi-agent reinforcement learning.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        global_obs_shape,\n",
    "        local_obs_shape,\n",
    "        action_dim,\n",
    "        num_agents=1,\n",
    "        max_size=100000,\n",
    "        obs_dtype=np.float32,\n",
    "        action_dtype=np.float32,\n",
    "    ):\n",
    "        \"\"\"Initialize the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            global_obs_shape: Shape of the global observations\n",
    "            local_obs_shape: Shape of the locals observations\n",
    "            action_dim: Dimension of the actions\n",
    "            num_agents: Number of agents\n",
    "            max_size: Maximum size of the buffer\n",
    "            obs_dtype: Data type of the observations\n",
    "            action_dtype: Data type of the actions\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.ptr, self.size = 0, 0\n",
    "        self.obs_type = obs_dtype\n",
    "        self.action_type = action_dtype\n",
    "        self.global_obs = np.zeros((max_size,) + global_obs_shape, dtype=obs_dtype)\n",
    "        self.local_obs = np.zeros((max_size, num_agents) + local_obs_shape, dtype=obs_dtype)\n",
    "        self.next_global_obs = np.zeros((max_size,) + global_obs_shape, dtype=obs_dtype)\n",
    "        self.next_local_obs = np.zeros((max_size, num_agents) + local_obs_shape, dtype=obs_dtype)\n",
    "        self.joint_actions = np.zeros(\n",
    "            (max_size, num_agents * action_dim), dtype=action_dtype\n",
    "        )  # joint actions are flattened into a single vector\n",
    "        self.rewards = np.zeros((max_size,), dtype=np.float32)\n",
    "        self.terminateds = np.zeros((max_size, 1), dtype=np.float32)\n",
    "\n",
    "        \n",
    "        # Nowe pamięci dla Q-values i log_probs\n",
    "        self.q_values = np.zeros((max_size, num_agents), dtype=np.float32)\n",
    "        self.log_probs = np.zeros((max_size, num_agents), dtype=np.float32)\n",
    "\n",
    "    def add(\n",
    "        self,\n",
    "        global_obs: np.ndarray,\n",
    "        local_obs: Dict[str, np.ndarray],\n",
    "        joint_actions: np.ndarray,\n",
    "        reward: float,\n",
    "        next_global_obs: np.ndarray,\n",
    "        next_local_obs: Dict[str, np.ndarray],\n",
    "        terminated: bool,\n",
    "        q_values: np.ndarray,\n",
    "        log_probs: np.ndarray,\n",
    "    ):\n",
    "        \"\"\"Add a new experience to the buffer.\n",
    "\n",
    "        Args:\n",
    "            global_obs: Global observation\n",
    "            local_obs: Local observation of each agent\n",
    "            joint_actions: Actions of all agents (flattened into a single vector)\n",
    "            reward: global reward\n",
    "            next_global_obs: Next global observation\n",
    "            next_local_obs: Next local observation of each agent\n",
    "            terminated: Env is terminated or not\n",
    "        \"\"\"\n",
    "        self.global_obs[self.ptr] = np.array(global_obs, dtype=self.obs_type).copy()\n",
    "        self.next_global_obs[self.ptr] = np.array(next_global_obs, self.obs_type).copy()\n",
    "        for agent_id, obs in local_obs.items():\n",
    "            self.local_obs[self.ptr][extract_agent_id(agent_id)] = np.array(obs, dtype=self.obs_type).copy()\n",
    "        for agent_id, obs in next_local_obs.items():\n",
    "            self.next_local_obs[self.ptr][extract_agent_id(agent_id)] = np.array(obs, dtype=self.obs_type).copy()\n",
    "        self.joint_actions[self.ptr] = np.array(joint_actions, dtype=self.action_type).copy()\n",
    "        self.rewards[self.ptr] = np.array(reward, dtype=np.float32).copy()\n",
    "        self.terminateds[self.ptr] = np.array(terminated).copy()\n",
    "        \n",
    "        #Nowe \n",
    "        self.q_values[self.ptr] = np.array(q_values, dtype=np.float32).copy()\n",
    "        self.log_probs[self.ptr] = np.array(log_probs, dtype=np.float32).copy()\n",
    "\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size, replace=True, use_cer=False, to_tensor=False, add_id_to_local_obs=False, device=None):\n",
    "        \"\"\"Sample a batch of experiences from the buffer.\n",
    "\n",
    "        Args:\n",
    "            batch_size: Batch size\n",
    "            replace: Whether to sample with replacement\n",
    "            use_cer: Whether to use CER\n",
    "            to_tensor: Whether to convert the data to PyTorch tensors\n",
    "            add_id_to_local_obs: Whether to add the agent id to the local observations\n",
    "            device: Device to use\n",
    "\n",
    "        Returns:\n",
    "            An experience tuple:\n",
    "                global_obs: Global observations (batch_size, global_obs_shape)\n",
    "                local_obs:\n",
    "                    Local observations of each agent (batch_size, num_agents, local_obs_shape)\n",
    "                    (!) the dict is flattened into a vector\n",
    "                    If add_id_to_local_obs is True, the local observations vectors are concatenated with the agent id\n",
    "                joint_actions: Actions of all agents (batch_size, num_agents * action_dim)\n",
    "                rewards: Rewards (batch_size,)\n",
    "                next_global_obs: Next global observations (batch_size, global_obs_shape)\n",
    "                next_local_obs: Next local observations of each agent (batch_size, num_agents, local_obs_shape) (!) the dict is flattened into a vector\n",
    "                terminateds: Whether the episode is terminated or not (batch_size, 1)\n",
    "\n",
    "        \"\"\"\n",
    "        inds = np.random.choice(self.size, batch_size, replace=replace)\n",
    "        if use_cer:\n",
    "            inds[0] = self.ptr - 1  # always use last experience\n",
    "\n",
    "        def flatten_local_obss(local_obs, inds):\n",
    "            batch_local_obs = []\n",
    "            for local_obs_ind in local_obs[inds]:\n",
    "                local_obs_index = []\n",
    "                for agent_id, local_obs in enumerate(local_obs_ind):\n",
    "                    # This is very convenient for learning\n",
    "                    if add_id_to_local_obs:\n",
    "                        local_obs = np.concatenate((local_obs, np.array([agent_id], dtype=self.obs_type)))\n",
    "                    if to_tensor:\n",
    "                        local_obs_index.append(torch.tensor(local_obs).to(device))\n",
    "                    else:\n",
    "                        local_obs_index.append(np.array(local_obs))\n",
    "                if to_tensor:\n",
    "                    batch_local_obs.append(torch.stack(local_obs_index).to(device))\n",
    "                else:\n",
    "                    batch_local_obs.append(np.array(local_obs_index))\n",
    "            if to_tensor:\n",
    "                return torch.stack(batch_local_obs).to(device)\n",
    "            else:\n",
    "                return np.array(batch_local_obs)\n",
    "\n",
    "        if to_tensor:\n",
    "            return Experience(\n",
    "                global_obs=torch.tensor(self.global_obs[inds]).to(device),\n",
    "                local_obs=flatten_local_obss(self.local_obs, inds),\n",
    "                joint_actions=torch.tensor(self.joint_actions[inds]).to(device),\n",
    "                rewards=torch.tensor(self.rewards[inds]).to(device),\n",
    "                next_global_obs=torch.tensor(self.next_global_obs[inds]).to(device),\n",
    "                next_local_obs=flatten_local_obss(self.next_local_obs, inds),\n",
    "                terminateds=torch.tensor(self.terminateds[inds]).to(device),\n",
    "                q_values=torch.tensor(self.q_values[inds]).to(device),\n",
    "                log_probs=torch.tensor(self.log_probs[inds]).to(device),\n",
    "\n",
    "            )\n",
    "        else:\n",
    "            return Experience(\n",
    "                global_obs=self.global_obs[inds],\n",
    "                local_obs=flatten_local_obss(self.local_obs, inds),\n",
    "                joint_actions=self.joint_actions[inds],\n",
    "                rewards=self.rewards[inds],\n",
    "                next_global_obs=self.next_global_obs[inds],\n",
    "                next_local_obs=flatten_local_obss(self.next_local_obs, inds),\n",
    "                terminateds=self.terminateds[inds],\n",
    "                q_values=self.q_values[inds],\n",
    "                log_probs=self.log_probs[inds],\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Get the size of the buffer.\"\"\"\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QMIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixingNetwork(nn.Module):\n",
    "    \"\"\"Mixing Network to compute Q_tot from Q_a.\"\"\"\n",
    "    def __init__(self, num_agents, state_dim, mixing_hidden_dim=32):\n",
    "        super(MixingNetwork, self).__init__()\n",
    "        self.num_agents = num_agents\n",
    "\n",
    "        # Hypernetwork for weights and biases\n",
    "        self.hyper_w1 = nn.Sequential(\n",
    "            nn.Linear(state_dim, mixing_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mixing_hidden_dim, num_agents * mixing_hidden_dim)\n",
    "        )\n",
    "        self.hyper_b1 = nn.Linear(state_dim, mixing_hidden_dim)\n",
    "\n",
    "        self.hyper_w2 = nn.Sequential(\n",
    "            nn.Linear(state_dim, mixing_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mixing_hidden_dim, mixing_hidden_dim)\n",
    "        )\n",
    "        self.hyper_b2 = nn.Sequential(\n",
    "            nn.Linear(state_dim, mixing_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mixing_hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        # Nonlinear layer to mix agent Q-values\n",
    "        self.non_linear = nn.ReLU()\n",
    "\n",
    "    def forward(self, q_values, state):\n",
    "        \"\"\"Forward pass for computing Q_tot.\"\"\"\n",
    "        batch_size = q_values.size(0)\n",
    "\n",
    "        # Compute first layer weights and biases\n",
    "        w1 = self.hyper_w1(state).view(batch_size, self.num_agents, -1)\n",
    "        b1 = self.hyper_b1(state).view(batch_size, 1, -1)\n",
    "\n",
    "        # First mixing layer\n",
    "        hidden = self.non_linear(torch.bmm(q_values.unsqueeze(1), w1) + b1)\n",
    "\n",
    "        # Compute second layer weights and biases\n",
    "        w2 = self.hyper_w2(state).view(batch_size, -1, 1)\n",
    "        b2 = self.hyper_b2(state).view(batch_size, 1, 1)\n",
    "\n",
    "        # Second mixing layer\n",
    "        q_tot = torch.bmm(hidden, w2) + b2\n",
    "        return q_tot.squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klasa do zbierania i wyświetlania zwrotów z epizodów\n",
    "class Plotter:\n",
    "    def __init__(self):\n",
    "        self.returns_trained = []\n",
    "        self.episodes_trained = []\n",
    "        self.returns_random = []\n",
    "        self.episodes_random = []\n",
    "\n",
    "    def add_return(self, episode, return_value, agent_type='trained'):\n",
    "        if agent_type == 'trained':\n",
    "            self.episodes_trained.append(episode)\n",
    "            self.returns_trained.append(return_value)\n",
    "        elif agent_type == 'random':\n",
    "            self.episodes_random.append(episode)\n",
    "            self.returns_random.append(return_value)\n",
    "\n",
    "    def plot_returns(self):\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.plot(self.episodes_trained, self.returns_trained, label='Wytrenowany Agent', marker='o')\n",
    "        plt.plot(self.episodes_random, self.returns_random, label='Agent Losowy', marker='x')\n",
    "        plt.xlabel('Epizod')\n",
    "        plt.ylabel('Zwrot')\n",
    "        plt.title('Porównanie Zwrotów: Wytrenowany Agent vs Agent Losowy')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# Klasa do wizualizacji działania agenta\n",
    "class AgentVisualizer:\n",
    "    def __init__(self, actor_path, device='cpu'):\n",
    "        \"\"\"\n",
    "        Inicjalizuje wizualizatora agenta.\n",
    "\n",
    "        Args:\n",
    "            actor_path (str): Ścieżka do pliku z zapisanym modelem aktora (actor.pth).\n",
    "            device (str, optional): Urządzenie do obliczeń ('cpu' lub 'cuda'). Domyślnie 'cpu'.\n",
    "        \"\"\"\n",
    "        self.device = torch.device(device if torch.cuda.is_available() and device == 'cuda' else 'cpu')\n",
    "        # Używamy oryginalnego środowiska PettingZoo z renderowaniem 'rgb_array'\n",
    "\n",
    "        self.env = env\n",
    "        # self.env = simple_spread_v3.parallel_env(\n",
    "        #     N=3, \n",
    "        #     local_ratio=0.5, \n",
    "        #     max_cycles=25, \n",
    "        #     continuous_actions=True,\n",
    "        #     render_mode=\"rgb_array\"  # Poprawne ustawienie render_mode\n",
    "        # )\n",
    "        self.actor = Actor(self.env)\n",
    "        self.actor.load_state_dict(torch.load(actor_path, map_location=self.device))\n",
    "        self.actor.to(self.device)\n",
    "        self.actor.eval()\n",
    "        self.plotter = Plotter()\n",
    "\n",
    "        # Sprawdzenie dostępnych trybów renderowania\n",
    "        available_render_modes = self.env.metadata.get('render_modes', None)\n",
    "        print(\"Dostępne tryby renderowania:\", available_render_modes)\n",
    "\n",
    "    def run_episodes(self, num_episodes=10, agent_type='trained', render=False, save_gif=False, gif_path='agent_demo.gif'):\n",
    "        \"\"\"\n",
    "        Uruchamia określoną liczbę epizodów z wytrenowanym lub losowym agentem.\n",
    "\n",
    "        Args:\n",
    "            num_episodes (int, optional): Liczba epizodów do uruchomienia. Domyślnie 10.\n",
    "            agent_type (str, optional): Typ agenta ('trained' lub 'random'). Domyślnie 'trained'.\n",
    "            render (bool, optional): Czy renderować epizody. Domyślnie False.\n",
    "            save_gif (bool, optional): Czy zapisać renderowane epizody jako GIF. Domyślnie False.\n",
    "            gif_path (str, optional): Ścieżka do zapisu GIF. Domyślnie 'agent_demo.gif'.\n",
    "        \"\"\"\n",
    "        frames = []\n",
    "        for episode in range(1, num_episodes + 1):\n",
    "            obs, info = self.env.reset(seed=42 + episode)\n",
    "            global_return = 0.0\n",
    "            done = False\n",
    "            while not done:\n",
    "                actions = {}\n",
    "                if agent_type == 'trained':\n",
    "                    with torch.no_grad():\n",
    "                        for agent_id in self.env.possible_agents:\n",
    "                            # Przygotowanie obserwacji z ID agenta\n",
    "                            obs_with_id = concat_id(obs[agent_id], agent_id)\n",
    "                            obs_tensor = torch.Tensor(obs_with_id).to(self.device)\n",
    "                            # Dodanie wymiaru batch (1, ...)\n",
    "                            obs_tensor = obs_tensor.unsqueeze(0)\n",
    "                            action, _, _ = self.actor.get_action(obs_tensor)\n",
    "                            # Przekonwertowanie akcji na numpy\n",
    "                            actions[agent_id] = action.cpu().numpy().flatten()\n",
    "                elif agent_type == 'random':\n",
    "                    for agent_id in self.env.possible_agents:\n",
    "                        # Przykładowe akcje losowe zgodne z przestrzenią akcji\n",
    "                        actions[agent_id] = self.env.action_space(agent_id).sample()\n",
    "                else:\n",
    "                    raise ValueError(\"Nieznany typ agenta. Użyj 'trained' lub 'random'.\")\n",
    "\n",
    "                # Wykonanie akcji w środowisku\n",
    "                next_obs, rewards, terminations, truncations, infos = self.env.step(actions)\n",
    "                done = any(terminations.values()) or any(truncations.values())\n",
    "\n",
    "                if render:\n",
    "                    try:\n",
    "                        # Renderowanie środowiska w trybie 'rgb_array'\n",
    "                        frame = self.env.render()\n",
    "                        if frame is not None:\n",
    "                            frames.append(frame)\n",
    "                        else:\n",
    "                            print(\"Renderowanie zwróciło None.\")\n",
    "                    except TypeError as e:\n",
    "                        print(f\"Nieudane renderowanie z mode='rgb_array': {e}\")\n",
    "                        print(\"Spróbuj wywołać render bez argumentów lub z innym trybem.\")\n",
    "                        try:\n",
    "                            frame = self.env.render()\n",
    "                            if frame is not None:\n",
    "                                frames.append(frame)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Nieudane renderowanie bez trybu: {e}\")\n",
    "\n",
    "                # Sumowanie zwrotów\n",
    "                global_return += sum(rewards.values())\n",
    "\n",
    "                # Aktualizacja obserwacji\n",
    "                obs = next_obs\n",
    "\n",
    "            # Dodanie zwrotu do Plotter\n",
    "            self.plotter.add_return(episode, global_return, agent_type=agent_type)\n",
    "            print(f\"Epizod {episode} ({agent_type}): Zwrot = {global_return}\")\n",
    "\n",
    "        if save_gif and frames:\n",
    "            try:\n",
    "                imageio.mimsave(gif_path, frames, fps=10)\n",
    "                print(f\"Zapisano wideo jako '{gif_path}'\")\n",
    "            except Exception as e:\n",
    "                print(f\"Nie udało się zapisać GIF: {e}\")\n",
    "\n",
    "        if render and frames:\n",
    "            # Wyświetlenie kilku pierwszych klatek jako przykładu\n",
    "            num_frames_to_show = min(5, len(frames))\n",
    "            for i in range(num_frames_to_show):\n",
    "                plt.figure(figsize=(5,5))\n",
    "                plt.imshow(frames[i])\n",
    "                plt.axis('off')\n",
    "                plt.title(f'Klatka {i+1}')\n",
    "                plt.show()\n",
    "\n",
    "    def plot_returns(self):\n",
    "        \"\"\"\n",
    "        Wyświetla wykres zwrotów z epizodów dla obu agentów.\n",
    "        \"\"\"\n",
    "        self.plotter.plot_returns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved latest models at step 0\n",
      "Saved latest models at step 500\n",
      "Saved latest models at step 1000\n",
      "Saved latest models at step 1500\n",
      "Saved latest models at step 2000\n",
      "Saved latest models at step 2500\n",
      "Saved latest models at step 3000\n",
      "Saved latest models at step 3500\n",
      "Saved latest models at step 4000\n",
      "Saved latest models at step 4500\n",
      "Saved latest models at step 5000\n",
      "Training finished. Model saved at output/Circle_simple/5000.pth\n"
     ]
    }
   ],
   "source": [
    "run_name = f\"{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "if args.track:\n",
    "    import wandb\n",
    "\n",
    "    wandb.init(\n",
    "        project=args.wandb_project_name,\n",
    "        entity=args.wandb_entity,\n",
    "        sync_tensorboard=True,\n",
    "        config=vars(args),\n",
    "        name=run_name,\n",
    "        monitor_gym=False,\n",
    "        save_code=True,\n",
    "    )\n",
    "writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "writer.add_text(\n",
    "    \"hyperparameters\",\n",
    "    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    ")\n",
    "\n",
    "# TRY NOT TO MODIFY: seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "# device = torch.device(\"mps\") if torch.backends.mps.is_available() else device\n",
    "\n",
    "\n",
    "env.reset(seed=args.seed)\n",
    "single_action_space = env.action_space(env.unwrapped.agents[0])\n",
    "single_observation_space = env.observation_space(env.unwrapped.agents[0])\n",
    "assert isinstance(single_action_space, gym.spaces.Box), \"only continuous action space is supported\"\n",
    "\n",
    "max_action = float(single_action_space.high[0])\n",
    "\n",
    "actor = Actor(env).to(device)\n",
    "qf1 = SoftQNetwork(env).to(device)\n",
    "qf2 = SoftQNetwork(env).to(device)\n",
    "qf1_target = SoftQNetwork(env).to(device)\n",
    "qf2_target = SoftQNetwork(env).to(device)\n",
    "\n",
    "# Ładowanie starych wag\n",
    "# qf1_target.load_state_dict(qf1.state_dict())\n",
    "# qf2_target.load_state_dict(qf2.state_dict())\n",
    "q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)\n",
    "actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)\n",
    "\n",
    "# Automatic entropy tuning\n",
    "if args.autotune:\n",
    "    target_entropy = -torch.prod(torch.Tensor(single_action_space.shape).to(device)).item()\n",
    "    log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "    alpha = log_alpha.exp().item()\n",
    "    a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)\n",
    "else:\n",
    "    alpha = args.alpha\n",
    "\n",
    "single_observation_space.dtype = np.float32\n",
    "rb = MAReplayBuffer(\n",
    "    global_obs_shape=env.state().shape,\n",
    "    local_obs_shape=single_observation_space.shape,\n",
    "    action_dim=single_action_space.shape[0],\n",
    "    num_agents=env.max_num_agents,\n",
    ")\n",
    "start_time = time.time()\n",
    "\n",
    "# TRY NOT TO MODIFY: start the game\n",
    "obs, info = env.reset(seed=args.seed)\n",
    "global_return = 0.0\n",
    "global_obs: np.ndarray = env.state()\n",
    "for global_step in range(args.total_timesteps + 1):\n",
    "    # ALGO LOGIC: put action logic here\n",
    "    if global_step < args.learning_starts:\n",
    "        actions: Dict[str, np.ndarray] = {agent: env.action_space(agent).sample() for agent in env.possible_agents}\n",
    "    else:\n",
    "        actions: Dict[str, np.ndarray] = {}\n",
    "        with torch.no_grad():\n",
    "            for agent_id in env.possible_agents:\n",
    "                obs_with_id = torch.Tensor(concat_id(obs[agent_id], agent_id)).to(device)\n",
    "                act, _, _ = actor.get_action(obs_with_id.unsqueeze(0))\n",
    "                act = act.detach().cpu().numpy()\n",
    "                actions[agent_id] = act.flatten()\n",
    "\n",
    "\n",
    "    # TRY NOT TO MODIFY: execute the game and log data.\n",
    "    next_obs: Dict[str, ObsType]\n",
    "    rewards: Dict[str, float]\n",
    "    next_obs, rewards, terminateds, truncateds, infos = env.step(actions)\n",
    "\n",
    "    terminated: bool = any(terminateds.values())\n",
    "    truncated: bool = any(truncateds.values())\n",
    "\n",
    "    # TRY NOT TO MODIFY: save data to replay buffer; handle `final_observation`\n",
    "    real_next_obs = next_obs\n",
    "    # TODO PZ doesn't have that yet\n",
    "    # if truncated:\n",
    "    #     real_next_obs = infos[\"final_observation\"].copy()\n",
    "    rb.add(\n",
    "        global_obs=global_obs,\n",
    "        local_obs=obs,\n",
    "        joint_actions=np.array(list(actions.values())).flatten(),\n",
    "        reward=np.array(list(rewards.values())).sum(),\n",
    "        next_global_obs=env.state(),\n",
    "        next_local_obs=real_next_obs,\n",
    "        terminated=terminated,\n",
    "    )\n",
    "\n",
    "    # TRY NOT TO MODIFY: CRUCIAL step easy to overlook\n",
    "    obs = next_obs\n",
    "    global_return += sum(rewards.values())\n",
    "    global_obs = env.state()\n",
    "\n",
    "    # ALGO LOGIC: training.\n",
    "    if global_step > args.learning_starts:\n",
    "        data: Experience = rb.sample(args.batch_size, to_tensor=True, device=device, add_id_to_local_obs=True)\n",
    "        with torch.no_grad():\n",
    "            # Computes q value from target networks\n",
    "            # flatten data.next_local_obs to forward for all agents at once\n",
    "            flattened_next_local_obs = data.next_local_obs.reshape(\n",
    "                (args.batch_size * env.unwrapped.max_num_agents, np.prod(single_observation_space.shape) + 1)\n",
    "            )\n",
    "            # forward pass to get next actions and log probs\n",
    "            next_state_actions, next_state_log_pi, _ = actor.get_action(flattened_next_local_obs)\n",
    "            next_joint_actions = next_state_actions.reshape(\n",
    "                (args.batch_size, np.prod(single_action_space.shape) * env.unwrapped.max_num_agents)\n",
    "            )\n",
    "            # Sums the log probs of the actions in the agent dimension to get the joint log prob\n",
    "            next_state_log_pi = einops.reduce(\n",
    "                next_state_log_pi.reshape((args.batch_size, env.unwrapped.max_num_agents)), \"b a -> b ()\", \"sum\"\n",
    "            )\n",
    "\n",
    "            # SAC Bellman equation\n",
    "            qf1_next_target = qf1_target(data.next_global_obs, next_joint_actions)\n",
    "            qf2_next_target = qf2_target(data.next_global_obs, next_joint_actions)\n",
    "            min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi\n",
    "            next_q_value = data.rewards.flatten() + (1 - data.terminateds.flatten()) * args.gamma * (\n",
    "                min_qf_next_target\n",
    "            ).view(-1)\n",
    "\n",
    "        # Computes q loss\n",
    "        qf1_a_values = qf1(data.global_obs, data.joint_actions).view(-1)\n",
    "        qf2_a_values = qf2(data.global_obs, data.joint_actions).view(-1)\n",
    "        qf1_loss = F.mse_loss(qf1_a_values, next_q_value)\n",
    "        qf2_loss = F.mse_loss(qf2_a_values, next_q_value)\n",
    "        qf_loss = qf1_loss + qf2_loss\n",
    "\n",
    "        q_optimizer.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        q_optimizer.step()\n",
    "\n",
    "        if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support\n",
    "            for _ in range(\n",
    "                args.policy_frequency\n",
    "            ):  # compensate for the delay by doing 'actor_update_interval' instead of 1\n",
    "                # flatten data.local_obs to forward for all agents at once\n",
    "                flattened_local_obs = data.local_obs.reshape(\n",
    "                    (args.batch_size * env.unwrapped.max_num_agents, np.prod(single_observation_space.shape) + 1)\n",
    "                )\n",
    "                # forward pass to get next actions and log probs\n",
    "                pi, log_pi, _ = actor.get_action(flattened_local_obs)\n",
    "                next_joint_actions = pi.reshape(\n",
    "                    (args.batch_size, np.prod(single_action_space.shape) * env.unwrapped.max_num_agents)\n",
    "                )\n",
    "                # Sums the log probs of the actions in the agent dimension to get the joint log prob\n",
    "                # TODO check if this is correct\n",
    "                log_pi = einops.reduce(\n",
    "                    log_pi.reshape((args.batch_size, env.unwrapped.max_num_agents)), \"b a -> b ()\", \"sum\"\n",
    "                )\n",
    "\n",
    "                # SAC pi update\n",
    "                qf1_pi = qf1(data.global_obs, next_joint_actions)\n",
    "                qf2_pi = qf2(data.global_obs, next_joint_actions)\n",
    "                min_qf_pi = torch.min(qf1_pi, qf2_pi).view(-1)\n",
    "                actor_loss = ((alpha * log_pi) - min_qf_pi).mean()\n",
    "\n",
    "                actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                actor_optimizer.step()\n",
    "           \n",
    "                if args.autotune:\n",
    "                    with torch.no_grad():\n",
    "                        _, log_pi, _ = actor.get_action(flattened_local_obs)\n",
    "                        log_pi = einops.reduce(\n",
    "                            log_pi.reshape((args.batch_size, env.unwrapped.max_num_agents)), \"b a -> b ()\", \"sum\"\n",
    "                        )\n",
    "                    alpha_loss = (-log_alpha * (log_pi + target_entropy)).mean()\n",
    "                    \n",
    "\n",
    "                    a_optimizer.zero_grad()\n",
    "                    alpha_loss.backward()\n",
    "                    a_optimizer.step()\n",
    "                    alpha = log_alpha.exp().item()\n",
    "\n",
    "\n",
    "        # update the target networks\n",
    "        if global_step % args.target_network_frequency == 0:\n",
    "            for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):\n",
    "                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "            for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):\n",
    "                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "\n",
    "        if global_step % 100 == 0:\n",
    "            writer.add_scalar(\"losses/qf1_values\", qf1_a_values.mean().item(), global_step)\n",
    "            writer.add_scalar(\"losses/qf2_values\", qf2_a_values.mean().item(), global_step)\n",
    "            writer.add_scalar(\"losses/qf1_loss\", qf1_loss.item(), global_step)\n",
    "            writer.add_scalar(\"losses/qf2_loss\", qf2_loss.item(), global_step)\n",
    "            writer.add_scalar(\"losses/qf_loss\", qf_loss.item() / 2.0, global_step)\n",
    "            writer.add_scalar(\"losses/actor_loss\", actor_loss.item(), global_step)\n",
    "            writer.add_scalar(\"losses/alpha\", alpha, global_step)\n",
    "            writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "            if args.autotune:\n",
    "                writer.add_scalar(\"losses/alpha_loss\", alpha_loss.item(), global_step)\n",
    "\n",
    "    if global_step % args.save_frequency == 0:\n",
    "        torch.save(actor.state_dict(), f\"{args.actor_path}/actor_latest.pth\")\n",
    "        torch.save(qf1.state_dict(), f\"{args.actor_path}/qf1_latest.pth\")\n",
    "        torch.save(qf2.state_dict(), f\"{args.actor_path}/qf2_latest.pth\")\n",
    "        if args.autotune:\n",
    "            torch.save(log_alpha, f\"{args.actor_path}/log_alpha_latest.pth\")\n",
    "        print(f\"Saved latest models at step {global_step}\")\n",
    "\n",
    "       \n",
    "    \n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "        writer.add_scalar(\"charts/return\", global_return, global_step)\n",
    "        global_return = 0.0\n",
    "        global_obs = env.state()\n",
    "\n",
    "save_point = f\"{args.actor_path}{global_step}.pth\"\n",
    "\n",
    "# Saves the trained actor for execution\n",
    "torch.save(actor.state_dict(), save_point)\n",
    "print(f\"Training finished. Model saved at {save_point}\")\n",
    "env.close()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dostępne tryby renderowania: ['human', 'rgb_array']\n",
      "Available render modes: ['human', 'rgb_array']\n",
      "\n",
      "--- Uruchomienie epizodów z wytrenowanym agentem ---\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Epizod 1 (trained): Zwrot = -23.713346287583786\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Epizod 2 (trained): Zwrot = -13.570020194806503\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Epizod 3 (trained): Zwrot = -2.127799612005996\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Epizod 4 (trained): Zwrot = -15.291615397101785\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Epizod 5 (trained): Zwrot = -10.262373430389307\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Epizod 6 (trained): Zwrot = -0.4380495894423764\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n",
      "Renderowanie zwróciło None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Uruchomienie epizodów z wytrenowanym agentem\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Uruchomienie epizodów z wytrenowanym agentem ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mvisualizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_episodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_episodess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrained\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_gif\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgif_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_Trained.gif\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     28\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Uruchomienie epizodów z agentem losowym\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Uruchomienie epizodów z agentem losowym ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 96\u001b[0m, in \u001b[0;36mAgentVisualizer.run_episodes\u001b[0;34m(self, num_episodes, agent_type, render, save_gif, gif_path)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNieznany typ agenta. Użyj \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m lub \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Wykonanie akcji w środowisku\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m next_obs, rewards, terminations, truncations, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(terminations\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28many\u001b[39m(truncations\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m render:\n",
      "File \u001b[0;32m~/miniconda/envs/drlzh/lib/python3.11/site-packages/pettingzoo/utils/conversions.py:207\u001b[0m, in \u001b[0;36maec_to_parallel_wrapper.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    204\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m got agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maec_env\u001b[38;5;241m.\u001b[39magent_selection\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Parallel environment wrapper expects agents to step in a cycle.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m         )\n\u001b[1;32m    206\u001b[0m obs, rew, termination, truncation, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maec_env\u001b[38;5;241m.\u001b[39mlast()\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maec_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maec_env\u001b[38;5;241m.\u001b[39magents:\n\u001b[1;32m    209\u001b[0m     rewards[agent] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maec_env\u001b[38;5;241m.\u001b[39mrewards[agent]\n",
      "File \u001b[0;32m~/miniconda/envs/drlzh/lib/python3.11/site-packages/pettingzoo/utils/wrappers/order_enforcing.py:96\u001b[0m, in \u001b[0;36mOrderEnforcingWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/drlzh/lib/python3.11/site-packages/pettingzoo/utils/wrappers/base.py:47\u001b[0m, in \u001b[0;36mBaseWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActionType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/drlzh/lib/python3.11/site-packages/pettingzoo/utils/wrappers/clip_out_of_bounds.py:52\u001b[0m, in \u001b[0;36mClipOutOfBoundsWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     43\u001b[0m     EnvLogger\u001b[38;5;241m.\u001b[39mwarn_action_out_of_bound(\n\u001b[1;32m     44\u001b[0m         action\u001b[38;5;241m=\u001b[39maction, action_space\u001b[38;5;241m=\u001b[39mspace, backup_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclipping to space\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m     )\n\u001b[1;32m     46\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(\n\u001b[1;32m     47\u001b[0m         action,  \u001b[38;5;66;03m# pyright: ignore[reportGeneralTypeIssues]\u001b[39;00m\n\u001b[1;32m     48\u001b[0m         space\u001b[38;5;241m.\u001b[39mlow,  \u001b[38;5;66;03m# pyright: ignore[reportGeneralTypeIssues]\u001b[39;00m\n\u001b[1;32m     49\u001b[0m         space\u001b[38;5;241m.\u001b[39mhigh,  \u001b[38;5;66;03m# pyright: ignore[reportGeneralTypeIssues]\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/drlzh/lib/python3.11/site-packages/pettingzoo/utils/wrappers/base.py:47\u001b[0m, in \u001b[0;36mBaseWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActionType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/drlzh/lib/python3.11/site-packages/pettingzoo/mpe/_mpe_utils/simple_env.py:264\u001b[0m, in \u001b[0;36mSimpleEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accumulate_rewards()\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/drlzh/lib/python3.11/site-packages/pettingzoo/mpe/_mpe_utils/simple_env.py:287\u001b[0m, in \u001b[0;36mSimpleEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    286\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Inicjalizacja wizualizatora agenta\n",
    "# Upewnij się, że 'actor.pth' jest w tym samym katalogu co notebook lub podaj pełną ścieżkę\n",
    "visualizer = AgentVisualizer(actor_path=save_point, device='cuda')  # lub 'cpu' jeśli nie masz GPU\n",
    "\n",
    "# %%\n",
    "# Sprawdzenie dostępnych trybów renderowania\n",
    "print(\"Available render modes:\", visualizer.env.metadata.get('render_modes', 'No render modes available'))\n",
    "\n",
    "num_episodess=40\n",
    "\n",
    "# %%\n",
    "# Uruchomienie epizodów z wytrenowanym agentem\n",
    "print(\"\\n--- Uruchomienie epizodów z wytrenowanym agentem ---\")\n",
    "visualizer.run_episodes(\n",
    "    num_episodes=num_episodess, \n",
    "    agent_type='trained', \n",
    "    render=False, \n",
    "    save_gif=True, \n",
    "    gif_path=f\"{args.actor_path}{global_step}_Trained.gif\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Uruchomienie epizodów z agentem losowym\n",
    "print(\"\\n--- Uruchomienie epizodów z agentem losowym ---\")\n",
    "visualizer.run_episodes(\n",
    "    num_episodes=num_episodess, \n",
    "    agent_type='random', \n",
    "    render=False, \n",
    "    save_gif=True, \n",
    "    gif_path=f\"{args.actor_path}{global_step}_Random.gif\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Wyświetlenie wykresu zwrotów z obu agentów\n",
    "print(\"\\n--- Wykres porównujący zwroty wytrenowanego agenta z agentem losowym ---\")\n",
    "visualizer.plot_returns()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlzh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
