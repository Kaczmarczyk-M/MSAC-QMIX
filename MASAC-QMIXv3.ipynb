{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (2.4.1)\n",
      "Requirement already satisfied: numpy in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: supersuit in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (3.9.3)\n",
      "Requirement already satisfied: pettingzoo in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (1.24.3)\n",
      "Requirement already satisfied: pymunk in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (6.10.0)\n",
      "Requirement already satisfied: scipy in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (1.15.0)\n",
      "Requirement already satisfied: gymnasium in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (0.29.1)\n",
      "Requirement already satisfied: matplotlib in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: einops in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (0.8.0)\n",
      "Requirement already satisfied: tensorboard in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (2.18.0)\n",
      "Requirement already satisfied: wandb in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (0.19.4)\n",
      "Requirement already satisfied: imageio in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (2.35.0)\n",
      "Requirement already satisfied: cloudpickle in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from stable-baselines3) (3.0.0)\n",
      "Requirement already satisfied: pandas in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from stable-baselines3) (2.2.3)\n",
      "Requirement already satisfied: filelock in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: tinyscaler>=1.2.6 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from supersuit) (1.2.8)\n",
      "Requirement already satisfied: cffi>=1.17.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from pymunk) (1.17.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (1.69.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (5.29.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (72.2.0)\n",
      "Requirement already satisfied: six>1.9 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (4.2.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (6.0.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (2.10.5)\n",
      "Requirement already satisfied: pyyaml in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (2.20.0)\n",
      "Requirement already satisfied: setproctitle in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: pycparser in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from cffi>=1.17.1->pymunk) (2.22)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from pandas->stable-baselines3) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from pandas->stable-baselines3) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install stable-baselines3 numpy torch supersuit pettingzoo pymunk scipy gymnasium matplotlib einops tensorboard wandb imageio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(seed=42, total_episodes=1000000, max_cycles=25, n_agents=3, buffer_size=2000000, batch_size=256, gamma=0.98, tau=0.05, alpha=0.005, lr_actor=0.0001, lr_critic=0.0001, lr_mixer=0.0001, learning_starts=200, train_freq=1, eval_freq=50, num_eval_episodes=4, exp_name='SAC_QMIX_spread')\n",
      "Box(0.0, 1.0, (5,), float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 515\u001b[0m\n\u001b[1;32m    513\u001b[0m args \u001b[38;5;241m=\u001b[39m get_args()\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28mprint\u001b[39m(args)\n\u001b[0;32m--> 515\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43mrun_sac_qmix\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone. Last 10 episodes avg return:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(rewards[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m:]))\n",
      "Cell \u001b[0;32mIn[5], line 351\u001b[0m, in \u001b[0;36mrun_sac_qmix\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;66;03m# Trening\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (global_step \u001b[38;5;241m>\u001b[39m args\u001b[38;5;241m.\u001b[39mlearning_starts) \u001b[38;5;129;01mand\u001b[39;00m (global_step \u001b[38;5;241m%\u001b[39m args\u001b[38;5;241m.\u001b[39mtrain_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;66;03m# sample\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     (states_b, obs_b, acts_b, rews_b, next_states_b, next_obs_b, dones_b) \u001b[38;5;241m=\u001b[39m \u001b[43mreplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;66;03m# states_b: tuple (batch_size, ), musimy skonwertować\u001b[39;00m\n\u001b[1;32m    353\u001b[0m     batch_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(np\u001b[38;5;241m.\u001b[39marray(states_b))\n",
      "Cell \u001b[0;32mIn[5], line 185\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_size):\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# print(\"Sampling from buffer\")\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;66;03m# print(\"Buffer size:\", len(self.buffer))\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# print(\"Batch size:\", batch_size)\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))\n",
      "File \u001b[0;32m~/miniconda/envs/drlzh/lib/python3.11/random.py:456\u001b[0m, in \u001b[0;36mRandom.sample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    454\u001b[0m randbelow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_randbelow\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m k \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n:\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample larger than population or is negative\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    457\u001b[0m result \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m k\n\u001b[1;32m    458\u001b[0m setsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m21\u001b[39m        \u001b[38;5;66;03m# size of a small set minus size of an empty list\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import deque\n",
    "from typing import List, Dict, NamedTuple\n",
    "import gymnasium as gym\n",
    "from pettingzoo.mpe import simple_spread_v3\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from argparse import Namespace\n",
    "import time\n",
    "\n",
    "# =====================================================================\n",
    "# 1. Parsing argumentów (Namespace) - przykładowa konfiguracja\n",
    "# =====================================================================\n",
    "def get_args():\n",
    "    args = Namespace()\n",
    "    args.seed = 42\n",
    "    args.total_episodes = 1000000\n",
    "    args.max_cycles = 25       # maksymalna liczba kroków w epizodzie (w MPE: max_cycles)\n",
    "    args.n_agents = 3          # N=3 w simple_spread\n",
    "    args.buffer_size = 2000000\n",
    "    args.batch_size = 128 \n",
    "    args.gamma = 0.98\n",
    "    args.tau = 0.05\n",
    "    args.alpha = 0.005           # stała entropii w SAC\n",
    "    args.lr_actor = 1e-4\n",
    "    args.lr_critic = 1e-4\n",
    "    args.lr_mixer = 1e-4\n",
    "    args.learning_starts = 200 # musi byc wiekszy niz batch_size\n",
    "    # args.learning_starts = args.total_episodes // 10\n",
    "    args.train_freq = 1        # co ile kroków/epizodów robić update\n",
    "    args.eval_freq = 50 # co ile epizodów robić ewaluację\n",
    "    # args.eval_freq = args.total_episodes // 100       # co ile epizodów robić ewaluację\n",
    "    args.num_eval_episodes = 4 # ile epizodów w ewaluacji\n",
    "    args.exp_name = \"SAC_QMIX_spread\"\n",
    "    return args\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 2. Sieć aktora - do akcji ciągłych (SAC)\n",
    "# =====================================================================\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.mean_head = nn.Linear(hidden_dim, act_dim)\n",
    "        self.logstd_head = nn.Linear(hidden_dim, act_dim)\n",
    "        self.LOG_STD_MIN = -5\n",
    "        self.LOG_STD_MAX = 2\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = self.net(obs)\n",
    "        mean = self.mean_head(x)\n",
    "        log_std = self.logstd_head(x)\n",
    "        # ograniczamy zakres log_std\n",
    "        log_std = torch.clamp(log_std, min=self.LOG_STD_MIN, max=self.LOG_STD_MAX)\n",
    "        return mean, log_std\n",
    "\n",
    "    def sample(self, obs):\n",
    "        \"\"\"\n",
    "        Zwraca (action, log_prob), z:\n",
    "        - action w (-1,1)\n",
    "        - log_prob łączny (po wymiarach akcji)\n",
    "        \"\"\"\n",
    "        mean, log_std = self.forward(obs)\n",
    "        std = log_std.exp()\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        z = dist.rsample()  # reparametrization\n",
    "        action = torch.tanh(z) # od -1 do 1\n",
    "        action = .5 * (action + 1)  # od 0 do 1\n",
    "\n",
    "        # log_prob: korekta za tanh\n",
    "        # log_prob = dist.log_prob(z) - torch.log(1 - action.pow(2) + 1e-7)\n",
    "        log_prob = dist.log_prob(z) - torch.log(torch.clamp(1 - action.pow(2), min=1e-7))\n",
    "        log_prob = log_prob.sum(dim=-1, keepdim=True)\n",
    "        return action, log_prob\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 3. Krytyk Q dla agenta i (Twin Q -> critic1[i], critic2[i])\n",
    "# =====================================================================\n",
    "class QCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: cat(global_state, joint_action)\n",
    "    Output: scalar Q^i\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 4. Mixing Network (QMIX)\n",
    "# =====================================================================\n",
    "class MixingNetwork(nn.Module):\n",
    "    def __init__(self, n_agents, state_dim, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.n_agents = n_agents\n",
    "        self.state_dim = state_dim\n",
    "\n",
    "        self.hyper_w1 = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_agents * hidden_dim)\n",
    "        )\n",
    "        self.hyper_b1 = nn.Linear(state_dim, hidden_dim)\n",
    "\n",
    "        self.hyper_w2 = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.hyper_b2 = nn.Linear(state_dim, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, q_values, state):\n",
    "        \"\"\"\n",
    "        q_values: (batch_size, n_agents)\n",
    "        state: (batch_size, state_dim)\n",
    "        Zwraca (batch_size, 1)\n",
    "        \"\"\"\n",
    "        bs = q_values.size(0)\n",
    "        # 1 warstwa\n",
    "        w1 = self.hyper_w1(state).view(bs, self.n_agents, -1)\n",
    "        b1 = self.hyper_b1(state).view(bs, 1, -1)\n",
    "\n",
    "        q_values = q_values.unsqueeze(1)  # (bs, 1, n_agents)\n",
    "        hidden = torch.bmm(q_values, w1) + b1  # => (bs, 1, hidden_dim)\n",
    "        hidden = self.relu(hidden)\n",
    "\n",
    "        # 2 warstwa\n",
    "        w2 = self.hyper_w2(state).view(bs, -1, 1)\n",
    "        b2 = self.hyper_b2(state).view(bs, 1, 1)\n",
    "\n",
    "        q_tot = torch.bmm(hidden, w2) + b2  # => (bs,1,1)\n",
    "        return q_tot.view(bs, 1)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 5. Replay buffer\n",
    "# =====================================================================\n",
    "class Transition(NamedTuple):\n",
    "    state: np.ndarray\n",
    "    obs: List[np.ndarray]  # local obs for each agent\n",
    "    action: List[np.ndarray]\n",
    "    reward: float\n",
    "    next_state: np.ndarray\n",
    "    next_obs: List[np.ndarray]\n",
    "    done: bool\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=100000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # print(\"Sampling from buffer\")\n",
    "        # print(\"Buffer size:\", len(self.buffer))\n",
    "        # print(\"Batch size:\", batch_size)\n",
    "\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        return list(zip(*batch))\n",
    "        # Zwracamy listy/tuple T= (state, obs, action, reward, next_state, next_obs, done)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 6. Główna klasa/wydzielona pętla treningowa\n",
    "# =====================================================================\n",
    "def run_sac_qmix(args):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # 6.1 Przygotowanie środowiska\n",
    "    env = simple_spread_v3.parallel_env(\n",
    "        N=args.n_agents,\n",
    "        local_ratio=0.8,\n",
    "        max_cycles=args.max_cycles,\n",
    "        continuous_actions=True\n",
    "    )\n",
    "    env.reset(seed=args.seed)\n",
    "    \n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    # 6.2 Ustalenie wymiarów\n",
    "    agents = env.possible_agents  # np. [agent_0, agent_1, agent_2]\n",
    "    \n",
    "    n_agents = len(agents)\n",
    "    # Sprawdzamy, czy n_agents == args.n_agents\n",
    "    assert n_agents == args.n_agents, \"Nie zgadza się liczba agentów!\"\n",
    "    act_dim_each = env.action_space(agents[0]).shape[0]  # np. 2\n",
    "    state_dim = np.prod(env.state().shape)               # global state\n",
    "    # local obs\n",
    "    obs_dim_each = np.prod(env.observation_space(agents[0]).shape)\n",
    "\n",
    "    print(env.action_space(agents[0])) \n",
    "    # Joint action dimension\n",
    "    act_dim_total = act_dim_each * n_agents\n",
    "\n",
    "    # 6.3 Inicjalizacja aktorów i krytyków (plus targety)\n",
    "    #  - actor[i], critic1[i], critic2[i], target_critic1[i], target_critic2[i]\n",
    "    #  - mixing network\n",
    "    #  - target mixing network\n",
    "    actors = []\n",
    "    critics1 = []\n",
    "    critics2 = []\n",
    "    target_critics1 = []\n",
    "    target_critics2 = []\n",
    "\n",
    "    for i in range(n_agents):\n",
    "        actor = Actor(obs_dim_each, act_dim_each).to(device)\n",
    "        actors.append(actor)\n",
    "\n",
    "        c1 = QCritic(state_dim + act_dim_total).to(device)\n",
    "        c2 = QCritic(state_dim + act_dim_total).to(device)\n",
    "        tc1 = QCritic(state_dim + act_dim_total).to(device)\n",
    "        tc1.load_state_dict(c1.state_dict())\n",
    "        tc2 = QCritic(state_dim + act_dim_total).to(device)\n",
    "        tc2.load_state_dict(c2.state_dict())\n",
    "\n",
    "        critics1.append(c1)\n",
    "        critics2.append(c2)\n",
    "        target_critics1.append(tc1)\n",
    "        target_critics2.append(tc2)\n",
    "\n",
    "    mixer = MixingNetwork(n_agents, state_dim).to(device)\n",
    "    target_mixer = MixingNetwork(n_agents, state_dim).to(device)\n",
    "    target_mixer.load_state_dict(mixer.state_dict())\n",
    "\n",
    "    # 6.4 Optimizers\n",
    "    actor_opts = [optim.Adam(actors[i].parameters(), lr=args.lr_actor) for i in range(n_agents)]\n",
    "    critic_opts = []\n",
    "    for i in range(n_agents):\n",
    "        params = list(critics1[i].parameters()) + list(critics2[i].parameters())\n",
    "        critic_opts.append(optim.Adam(params, lr=args.lr_critic))\n",
    "    mixer_opt = optim.Adam(mixer.parameters(), lr=args.lr_mixer)\n",
    "\n",
    "    # 6.5 Bufor replay\n",
    "    replay = ReplayBuffer(max_size=args.buffer_size)\n",
    "\n",
    "    # 6.6 TensorBoard\n",
    "    writer = SummaryWriter(comment=f\"_{args.exp_name}\")\n",
    "    global_step = 0\n",
    "    episode_rewards = []\n",
    "\n",
    "    def soft_update(source_net, target_net, tau):\n",
    "        for p, tp in zip(source_net.parameters(), target_net.parameters()):\n",
    "            tp.data.copy_(tau*p.data + (1.0 - tau)*tp.data)\n",
    "\n",
    "    def evaluate_policy(episode_idx):\n",
    "        \"\"\"\n",
    "        Uruchamia kilka epizodów w trybie testowym (bez noise), liczy średni zwrot.\n",
    "        Loguje do TB.\n",
    "        \"\"\"\n",
    "        eval_episodes = args.num_eval_episodes\n",
    "        returns = []\n",
    "        for _ in range(eval_episodes):\n",
    "            obs_dict, _ = env.reset()\n",
    "            done_dict = {ag: False for ag in agents}\n",
    "            ep_ret = 0.0\n",
    "            while not all(done_dict.values()):\n",
    "                actions_dict = {}\n",
    "                for i, ag in enumerate(agents):\n",
    "                    # Deterministycznie: bierzemy mean sieci\n",
    "                    obs_i = torch.FloatTensor(obs_dict[ag]).unsqueeze(0)\n",
    "                    with torch.no_grad():\n",
    "                        mean, _ = actors[i].forward(obs_i)\n",
    "                        action = torch.tanh(mean) # od -1 do 1\n",
    "                        action = .5 * (action + 1)  # od 0 do 1\n",
    "                    actions_dict[ag] = action.cpu().numpy().flatten()\n",
    "\n",
    "                next_obs, rews, done_dict, _, _ = env.step(actions_dict)\n",
    "                ep_ret += sum(rews.values())\n",
    "                obs_dict = next_obs\n",
    "            returns.append(ep_ret)\n",
    "        avg_ret = np.mean(returns)\n",
    "        writer.add_scalar(\"evaluate/avg_return\", avg_ret, episode_idx)\n",
    "        print(f\"[EVAL] Episode {episode_idx}, avg_return={avg_ret:.3f}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 6.7 Główna pętla treningowa\n",
    "    for ep in range(args.total_episodes):\n",
    "        obs_dict, _ = env.reset()\n",
    "        done_dict = {ag: False for ag in agents}\n",
    "        state_np = env.state()\n",
    "        ep_ret = 0.0\n",
    "        step = 0\n",
    "\n",
    "        while not all(done_dict.values()) and step < args.max_cycles:\n",
    "            actions_dict = {}\n",
    "            local_obs_list = []\n",
    "            for i, ag in enumerate(agents):\n",
    "                obs_i = torch.FloatTensor(obs_dict[ag]).unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    act_i, _ = actors[i].sample(obs_i)\n",
    "                actions_dict[ag] = act_i.cpu().numpy().flatten()\n",
    "                local_obs_list.append(obs_dict[ag])\n",
    "\n",
    "            next_obs_dict, rew_dict, done_dict, _, _ = env.step(actions_dict)\n",
    "            next_state_np = env.state()\n",
    "            reward = sum(rew_dict.values())\n",
    "            done = any(done_dict.values())  # ep. done\n",
    "\n",
    "            # Zapis do bufora\n",
    "            act_list = [actions_dict[ag] for ag in agents]\n",
    "            next_local_obs_list = [next_obs_dict[ag] for ag in agents]\n",
    "\n",
    "            replay.add(\n",
    "                state_np,\n",
    "                local_obs_list,\n",
    "                act_list,\n",
    "                reward,\n",
    "                next_state_np,\n",
    "                next_local_obs_list,\n",
    "                done\n",
    "            )\n",
    "\n",
    "            ep_ret += reward\n",
    "            state_np = next_state_np\n",
    "            obs_dict = next_obs_dict\n",
    "            step += 1\n",
    "            global_step += 1\n",
    "\n",
    "            # Trening\n",
    "            if (global_step > args.learning_starts) and (global_step % args.train_freq == 0):\n",
    "                # sample\n",
    "                (states_b, obs_b, acts_b, rews_b, next_states_b, next_obs_b, dones_b) = replay.sample(args.batch_size)\n",
    "                # states_b: tuple (batch_size, ), musimy skonwertować\n",
    "                batch_states = torch.FloatTensor(np.array(states_b))\n",
    "                batch_next_states = torch.FloatTensor(np.array(next_states_b))\n",
    "                batch_acts = torch.FloatTensor(np.array(acts_b))  # shape (B, N, act_dim_each)\n",
    "                batch_acts = batch_acts.view(args.batch_size, -1)  # (B, act_dim_total)\n",
    "                batch_rewards = torch.FloatTensor(np.array(rews_b)).unsqueeze(1)  # (B,1)\n",
    "                batch_dones = torch.FloatTensor(np.array(dones_b)).unsqueeze(1)   # (B,1)\n",
    "\n",
    "                # ---------- Oblicz target -----------\n",
    "                with torch.no_grad():\n",
    "                    # next akcje (z actorów)\n",
    "                    all_next_actions = []\n",
    "                    sum_log_prob_next = torch.zeros((args.batch_size,1))\n",
    "                    for i in range(n_agents):\n",
    "                        # Z next_obs_b\n",
    "                        agent_next_obs = []\n",
    "                        for b_idx in range(args.batch_size):\n",
    "                            agent_next_obs.append(next_obs_b[b_idx][i])\n",
    "                        agent_next_obs = torch.FloatTensor(np.array(agent_next_obs))\n",
    "                        # sample\n",
    "                        a_next_i, logp_next_i = actors[i].sample(agent_next_obs)\n",
    "                        all_next_actions.append(a_next_i)\n",
    "                        sum_log_prob_next += logp_next_i\n",
    "\n",
    "                    # joint action\n",
    "                    next_joint_actions = torch.cat(all_next_actions, dim=1)\n",
    "                    # Q^i z targetów\n",
    "                    all_qi_next = []\n",
    "                    for i in range(n_agents):\n",
    "                        inp = torch.cat([batch_next_states, next_joint_actions], dim=1)\n",
    "                        q1_val = target_critics1[i](inp)\n",
    "                        q2_val = target_critics2[i](inp)\n",
    "                        qi_next = torch.min(q1_val, q2_val)  # (B,1)\n",
    "                        all_qi_next.append(qi_next)\n",
    "                    q_i_cat_next = torch.cat(all_qi_next, dim=1)  # (B,N)\n",
    "                    q_tot_next = target_mixer(q_i_cat_next, batch_next_states)  # (B,1)\n",
    "\n",
    "                    # Odejmiemy alpha * sum_log_prob\n",
    "                    q_tot_next = q_tot_next - args.alpha * sum_log_prob_next\n",
    "\n",
    "                    # Bellman\n",
    "                    y = batch_rewards + (1 - batch_dones) * args.gamma * q_tot_next\n",
    "\n",
    "                # ---------- Oblicz Q^i current i Q_tot ----------\n",
    "                # critics1[i], critics2[i]\n",
    "                all_qi_1 = []\n",
    "                all_qi_2 = []\n",
    "                for i in range(n_agents):\n",
    "                    inp = torch.cat([batch_states, batch_acts], dim=1)\n",
    "                    q1_val = critics1[i](inp)\n",
    "                    q2_val = critics2[i](inp)\n",
    "                    all_qi_1.append(q1_val)\n",
    "                    all_qi_2.append(q2_val)\n",
    "\n",
    "                q1_cat = torch.cat(all_qi_1, dim=1)  # (B,N)\n",
    "                q2_cat = torch.cat(all_qi_2, dim=1)  # (B,N)\n",
    "                q_tot_1 = mixer(q1_cat, batch_states)  # (B,1)\n",
    "                q_tot_2 = mixer(q2_cat, batch_states)  # (B,1)\n",
    "                q_tot_current = torch.min(q_tot_1, q_tot_2)\n",
    "\n",
    "                critic_loss = F.mse_loss(q_tot_current, y)\n",
    "\n",
    "                # ---------- Update krytyków + mixera ----------\n",
    "                for opt in critic_opts:\n",
    "                    opt.zero_grad()\n",
    "                mixer_opt.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                for opt in critic_opts:\n",
    "                    opt.step()\n",
    "                mixer_opt.step()\n",
    "\n",
    "                # ---------- Update aktorów (SAC) ------------\n",
    "                # Podobnie jak w pseudo-kodzie: agent i\n",
    "                for i in range(n_agents):\n",
    "                    # \"Nową\" akcję daje actor i, reszta agentów => stara\n",
    "                    old_actions = batch_acts.clone()  # shape (B, N*act_dim_each)\n",
    "\n",
    "                    # Wyciągamy batch local_obs i\n",
    "                    agent_obs = []\n",
    "                    for b_idx in range(args.batch_size):\n",
    "                        agent_obs.append(obs_b[b_idx][i])\n",
    "                    agent_obs = torch.FloatTensor(np.array(agent_obs))\n",
    "\n",
    "                    new_action_i, new_logp_i = actors[i].sample(agent_obs)\n",
    "                    # Podmieniamy w old_actions\n",
    "                    # reshape -> (B, N, act_dim_each)\n",
    "                    old_actions_resh = old_actions.view(args.batch_size, n_agents, act_dim_each)\n",
    "                    old_actions_resh[:, i, :] = new_action_i\n",
    "                    new_joint_action = old_actions_resh.view(args.batch_size, -1)\n",
    "\n",
    "                    # Obliczamy Q^i z critics1[i], critics2[i]\n",
    "                    input_new = torch.cat([batch_states, new_joint_action], dim=1)\n",
    "                    q1_val_i = critics1[i](input_new)\n",
    "                    q2_val_i = critics2[i](input_new)\n",
    "                    q_val_i_min = torch.min(q1_val_i, q2_val_i)\n",
    "\n",
    "                    # Potrzebujemy Q^j dla j != i, bierzemy ze starych akcji?\n",
    "                    # Dla uproszczenia: liczymy on-the-fly\n",
    "                    # final wektor [Q^1, Q^2, ..., Q^N]\n",
    "                    q_current_list = []\n",
    "                    for j in range(n_agents):\n",
    "                        if j == i:\n",
    "                            q_current_list.append(q_val_i_min)  # (B,1)\n",
    "                        else:\n",
    "                            # stara akcja (B, N, act_dim_each)\n",
    "                            # w oryg. artykule QMIX jest robione w pętli,\n",
    "                            # by gradient nie przepływał przez aktor j\n",
    "                            # => ok, bierzemy \"no_grad\"?\n",
    "                            with torch.no_grad():\n",
    "                                inp_j = torch.cat([batch_states, batch_acts], dim=1)\n",
    "                                q1_j = critics1[j](inp_j)\n",
    "                                q2_j = critics2[j](inp_j)\n",
    "                                q_j_min = torch.min(q1_j, q2_j)\n",
    "                            q_current_list.append(q_j_min)\n",
    "                    # cat => (B, N)\n",
    "                    q_cat_actor = torch.cat(q_current_list, dim=1)\n",
    "                    q_tot_actor = mixer(q_cat_actor, batch_states)\n",
    "\n",
    "                    # entropia -> alpha * logp_i\n",
    "                    actor_loss = -(q_tot_actor - args.alpha * new_logp_i).mean()\n",
    "\n",
    "                    actor_opts[i].zero_grad()\n",
    "                    actor_loss.backward()\n",
    "                    actor_opts[i].step()\n",
    "                    \n",
    "                    writer.add_scalar(f\"loss/actor_loss_agent_{i}\", actor_loss.item(), global_step)\n",
    "\n",
    "                # ---------- Soft update targetów ------------\n",
    "                for i in range(n_agents):\n",
    "                    soft_update(critics1[i], target_critics1[i], args.tau)\n",
    "                    soft_update(critics2[i], target_critics2[i], args.tau)\n",
    "                soft_update(mixer, target_mixer, args.tau)\n",
    "\n",
    "                # ---------- logi do TB -----------\n",
    "                writer.add_scalar(\"loss/critic_loss\", critic_loss.item(), global_step)\n",
    "                writer.add_scalar(\"loss/qmix_loss\", critic_loss.item(), global_step)\n",
    "                writer.add_scalar(\"q_values/q_tot\", q_tot_current.mean().item(), global_step)\n",
    "                for i in range(n_agents):\n",
    "                    writer.add_scalar(f\"q_values/q_agent_{i}\", q1_cat[:, i].mean().item(), global_step)\n",
    "\n",
    "                writer.add_scalar(\"charts/SPS\", global_step / (time.time() - start_time), global_step)\n",
    "\n",
    "        # koniec epizodu\n",
    "        episode_rewards.append(ep_ret)\n",
    "        writer.add_scalar(\"charts/episodic_return\", ep_ret, ep)\n",
    "        writer.add_scalar(\"charts/average_return\", np.mean(episode_rewards[-100:]), ep)\n",
    "\n",
    "        # ewaluacja co X epizodów\n",
    "        if (ep+1) % args.eval_freq == 0:\n",
    "            evaluate_policy(ep+1)\n",
    "\n",
    "    env.close()\n",
    "    writer.close()\n",
    "\n",
    "    return episode_rewards\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 7. Uruchamianie\n",
    "# =====================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    args = get_args()\n",
    "    print(args)\n",
    "    rewards = run_sac_qmix(args)\n",
    "    print(\"Done. Last 10 episodes avg return:\", np.mean(rewards[-10:]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlzh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
