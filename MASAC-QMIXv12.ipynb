{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (2.4.1)\n",
      "Requirement already satisfied: numpy in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: supersuit in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (3.9.3)\n",
      "Requirement already satisfied: pettingzoo in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (1.24.3)\n",
      "Requirement already satisfied: pymunk in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (6.10.0)\n",
      "Requirement already satisfied: scipy in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (1.15.0)\n",
      "Requirement already satisfied: gymnasium in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (0.29.1)\n",
      "Requirement already satisfied: matplotlib in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: einops in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (0.8.0)\n",
      "Requirement already satisfied: tensorboard in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (2.18.0)\n",
      "Requirement already satisfied: wandb in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (0.19.4)\n",
      "Requirement already satisfied: imageio in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (2.35.0)\n",
      "Requirement already satisfied: cloudpickle in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from stable-baselines3) (3.0.0)\n",
      "Requirement already satisfied: pandas in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from stable-baselines3) (2.2.3)\n",
      "Requirement already satisfied: filelock in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: tinyscaler>=1.2.6 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from supersuit) (1.2.8)\n",
      "Requirement already satisfied: cffi>=1.17.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from pymunk) (1.17.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (1.69.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (5.29.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (72.2.0)\n",
      "Requirement already satisfied: six>1.9 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (4.2.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (6.0.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (2.10.5)\n",
      "Requirement already satisfied: pyyaml in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (2.20.0)\n",
      "Requirement already satisfied: setproctitle in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: pycparser in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from cffi>=1.17.1->pymunk) (2.22)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from pandas->stable-baselines3) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from pandas->stable-baselines3) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/michal/miniconda/envs/drlzh/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install stable-baselines3 numpy torch supersuit pettingzoo pymunk scipy gymnasium matplotlib einops tensorboard wandb imageio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple_spread_v3\n",
      "Resetting environment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1d/k2nxt21n3qn7tn1clzr__dkm0000gq/T/ipykernel_19536/1932352376.py:77: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  states = torch.tensor(states, dtype=torch.float32, device=self.device)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x14 and 20x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 974\u001b[0m\n\u001b[1;32m    972\u001b[0m experiment \u001b[38;5;241m=\u001b[39m Experiment(args)\n\u001b[1;32m    973\u001b[0m experiment\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m--> 974\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 772\u001b[0m, in \u001b[0;36mExperiment.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m<\u001b[39m args\u001b[38;5;241m.\u001b[39mn_steps:\n\u001b[1;32m    770\u001b[0m \u001b[38;5;66;03m#for self.e in range(self.e, args.n_episodes+1):\u001b[39;00m\n\u001b[1;32m    771\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39me \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m            \n\u001b[0;32m--> 772\u001b[0m     data, episode_reward, win_tag, step \u001b[38;5;241m=\u001b[39m  \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    773\u001b[0m     old_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m step                   \n",
      "Cell \u001b[0;32mIn[2], line 914\u001b[0m, in \u001b[0;36mRunner.run\u001b[0;34m(self, test_mode)\u001b[0m\n\u001b[1;32m    911\u001b[0m explore \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m test_mode\n\u001b[1;32m    913\u001b[0m \u001b[38;5;66;03m# Get actions from controller\u001b[39;00m\n\u001b[0;32m--> 914\u001b[0m actions_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontroller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavail_actions_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplore\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(actions_array) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39magents), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch between actions and agents.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;66;03m# Convert actions array to dict for PettingZoo environment\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 94\u001b[0m, in \u001b[0;36mController.get_actions\u001b[0;34m(self, states, avail_actions, explore)\u001b[0m\n\u001b[1;32m     91\u001b[0m     states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_actions], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 94\u001b[0m     ps, hs_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msys_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavail_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhiddens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhiddens \u001b[38;5;241m=\u001b[39m hs_next\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Action selection with validation\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 160\u001b[0m, in \u001b[0;36mActors.forward\u001b[0;34m(self, states, avail_actions, hiddens)\u001b[0m\n\u001b[1;32m    158\u001b[0m hiddens \u001b[38;5;241m=\u001b[39m hiddens\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,hiddens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    159\u001b[0m avail_actions \u001b[38;5;241m=\u001b[39m avail_actions\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,avail_actions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 160\u001b[0m ys, hs_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavail_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhiddens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m ys \u001b[38;5;241m=\u001b[39m ys\u001b[38;5;241m.\u001b[39mreshape(n_batch,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_agents,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    162\u001b[0m hs_next \u001b[38;5;241m=\u001b[39m hs_next\u001b[38;5;241m.\u001b[39mreshape(n_batch,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_agents,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 135\u001b[0m, in \u001b[0;36mActor.forward\u001b[0;34m(self, inputs, avail_actions, h_last)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, avail_actions, h_last):\n\u001b[0;32m--> 135\u001b[0m     h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m)        \n\u001b[1;32m    136\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(h, h_last)\n\u001b[1;32m    137\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(h)\n",
      "File \u001b[0;32m~/miniconda/envs/drlzh/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/drlzh/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/drlzh/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x14 and 20x128)"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import deque\n",
    "from typing import List, Dict, NamedTuple\n",
    "import gymnasium as gym\n",
    "from pettingzoo.mpe import simple_spread_v3\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from argparse import Namespace\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "\n",
    "\n",
    "class WritterUtil:\n",
    "    def __init__(self, writter: SummaryWriter, args):\n",
    "        self.writter = writter\n",
    "        self.log_every = args.log_every\n",
    "        self.scalars = {}\n",
    "        self.step = 0\n",
    "        self.log_step = 0\n",
    "\n",
    "    def set_step(self, step):\n",
    "        self.step += 1\n",
    "        self.log_step = step\n",
    "\n",
    "    def WriteScalar(self, tag, value):\n",
    "        \n",
    "        step = self.step\n",
    "        log_step = self.log_step\n",
    "        self.writter.add_scalar('raw/'+tag, value, log_step)\n",
    "\n",
    "        if tag not in self.scalars:\n",
    "            self.scalars[tag] = [0, step-1, torch.zeros(self.log_every)]\n",
    "        \n",
    "        n, last_step, buffer = self.scalars[tag]\n",
    "        \n",
    "        buffer[n] = value\n",
    "        n += 1\n",
    "        if step - last_step >= self.log_every:\n",
    "            mean = torch.mean(buffer[0:n])\n",
    "            self.writter.add_scalar('mean/'+tag, mean, log_step)\n",
    "            self.writter.add_histogram(tag, buffer[0:n], log_step)\n",
    "            \n",
    "            last_step = step\n",
    "            n=0\n",
    "\n",
    "\n",
    "        self.scalars[tag] = [n,last_step,buffer]\n",
    "\n",
    "class Controller:\n",
    "    def __init__(self, sys_agent, args):\n",
    "        self.n_agents = args.n_agents\n",
    "        self.n_actions = args.n_actions        \n",
    "        self.agent_id = args.agent_id\n",
    "        self.last_action = args.last_action\n",
    "        self.device = args.device    \n",
    "        self.sys_agent_src = sys_agent\n",
    "        self.sys_agent = type(sys_agent)(args)        \n",
    "        self.sys_agent.requires_grad_(False)        \n",
    "        self.episode = 0\n",
    "\n",
    "    def new_episode(self):\n",
    "        state_dict = self.sys_agent_src.state_dict()\n",
    "        self.sys_agent.load_state_dict(state_dict)        \n",
    "        self.hiddens = self.sys_agent.init_hiddens(1)\n",
    "        self.last_actions = torch.zeros(1, self.n_agents, self.n_actions)      \n",
    "        self.episode += 1\n",
    "\n",
    "    def get_actions(self, states, avail_actions, explore=False):\n",
    "        # Explicit tensor conversion with device and dtype\n",
    "        states = torch.tensor(states, dtype=torch.float32, device=self.device)\n",
    "        if states.dim() == 2:\n",
    "            states = states.unsqueeze(0)\n",
    "                \n",
    "        avail_actions = torch.tensor(avail_actions, dtype=torch.float32, device=self.device)\n",
    "        if avail_actions.dim() == 2:\n",
    "            avail_actions = avail_actions.unsqueeze(0)\n",
    "            \n",
    "        if self.agent_id:\n",
    "            agent_ids = torch.eye(self.n_agents, device=self.device)\n",
    "            agent_ids = agent_ids.unsqueeze(0).expand(states.size(0), -1, -1)\n",
    "            states = torch.cat([states, agent_ids], dim=-1)\n",
    "        \n",
    "        if self.last_action:            \n",
    "            states = torch.cat([states, self.last_actions], -1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ps, hs_next = self.sys_agent.forward(states, avail_actions, self.hiddens)\n",
    "        self.hiddens = hs_next\n",
    "        \n",
    "        # Action selection with validation\n",
    "        if explore:\n",
    "            while True:\n",
    "                actions = torch.multinomial(ps[0], 1).squeeze(-1)\n",
    "                selected_avails = avail_actions[0][torch.arange(self.n_agents), actions]\n",
    "                if not (selected_avails == 0).sum().item():\n",
    "                    break\n",
    "        else:\n",
    "            actions = torch.argmax(ps[0], -1)\n",
    "        \n",
    "        self.last_actions = self.one_hot(actions, self.n_actions).unsqueeze(0)\n",
    "        return actions.cpu().numpy()\n",
    "\n",
    "    def one_hot(self, tensor, n_classes):\n",
    "        return F.one_hot(tensor.to(dtype=torch.int64), n_classes).to(dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Actor, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.input_dim = args.input_dim        \n",
    "        self.n_actions = args.n_actions\n",
    "        self.n_agents = args.n_agents\n",
    "        self.rnn_hidden_dim = args.rnn_hidden_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(self.input_dim, self.rnn_hidden_dim)\n",
    "        self.rnn = nn.GRUCell(self.rnn_hidden_dim, self.rnn_hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.rnn_hidden_dim, self.n_actions)\n",
    "\n",
    "    def init_hidden(self, n_batch):\n",
    "        # make hidden states on same device as model\n",
    "        return self.fc1.weight.new_zeros(n_batch, self.rnn_hidden_dim)\n",
    "\n",
    "    def forward(self, inputs, avail_actions, h_last):\n",
    "        \n",
    "        h = F.relu(self.fc1(inputs))        \n",
    "        h = self.rnn(h, h_last)\n",
    "        y = self.fc2(h)\n",
    "        y[avail_actions == 0] = -float('inf')\n",
    "        y = torch.softmax(y,-1)\n",
    "\n",
    "        return y,h\n",
    "    \n",
    "class Actors(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Actors, self).__init__()\n",
    "        self.n_agents = args.n_agents\n",
    "        self.n_actions = args.n_actions\n",
    "        self.agent = Actor(args)\n",
    "\n",
    "    def init_hiddens(self, n_batch):\n",
    "        hiddens = self.agent.init_hidden(n_batch*self.n_agents)\n",
    "        hiddens = hiddens.reshape(n_batch,self.n_agents,-1)\n",
    "        return hiddens\n",
    "\n",
    "    def forward(self, states, avail_actions, hiddens):\n",
    "        n_batch = states.shape[0]\n",
    "        states = states.reshape(-1,states.shape[-1])\n",
    "        hiddens = hiddens.reshape(-1,hiddens.shape[-1])\n",
    "        avail_actions = avail_actions.reshape(-1,avail_actions.shape[-1])\n",
    "        ys, hs_next = self.agent.forward(states, avail_actions, hiddens)\n",
    "        ys = ys.reshape(n_batch,self.n_agents,-1)\n",
    "        hs_next = hs_next.reshape(n_batch,self.n_agents,-1)\n",
    "        \n",
    "        return ys, hs_next\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Critic, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.input_dim = args.input_dim\n",
    "        \n",
    "        self.n_actions = args.n_actions\n",
    "        self.n_agents = args.n_agents\n",
    "        self.rnn_hidden_dim = args.rnn_hidden_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(self.input_dim, self.rnn_hidden_dim)\n",
    "        self.rnn = nn.GRUCell(self.rnn_hidden_dim, self.rnn_hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.rnn_hidden_dim, self.n_actions)\n",
    "\n",
    "    def init_hidden(self, n_batch):\n",
    "        # make hidden states on same device as model\n",
    "        return self.fc1.weight.new_zeros(n_batch, self.rnn_hidden_dim)\n",
    "\n",
    "    def forward(self, inputs, avail_actions, h_last):\n",
    "        # x = torch.cat([inputs, action],1)\n",
    "        h = F.relu(self.fc1(inputs))\n",
    "        h = self.rnn(h, h_last)\n",
    "        y = self.fc2(h)\n",
    "        y[avail_actions == 0] = -1e38\n",
    "        return y, h\n",
    "\n",
    "class Critics(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Critics, self).__init__()\n",
    "        self.n_agents = args.n_agents\n",
    "        self.n_actions = args.n_actions\n",
    "        self.agent = Critic(args)\n",
    "\n",
    "\n",
    "\n",
    "    def init_hiddens(self, n_batch):\n",
    "        hiddens = self.agent.init_hidden(n_batch*self.n_agents)\n",
    "        hiddens = hiddens.reshape(n_batch,self.n_agents,-1)\n",
    "        return hiddens\n",
    "\n",
    "    def forward(self, states, avail_actions, hiddens):\n",
    "\n",
    "        n_batch = states.shape[0]\n",
    "        states = states.reshape(-1,states.shape[-1])\n",
    "        hiddens = hiddens.reshape(-1,hiddens.shape[-1])\n",
    "        avail_actions = avail_actions.reshape(-1,avail_actions.shape[-1])\n",
    "        ys, hs_next = self.agent.forward(states, avail_actions, hiddens)\n",
    "        ys = ys.reshape(n_batch,self.n_agents,-1)\n",
    "        hs_next = hs_next.reshape(n_batch,self.n_agents,-1)\n",
    "        \n",
    "        return ys, hs_next\n",
    "\n",
    "\n",
    "\n",
    "class EpisodeBuffer:\n",
    "    def __init__(self, scheme, args):\n",
    "        self.scheme = scheme.copy()\n",
    "        self.buffer_size = args.buffer_size\n",
    "        self.max_seq_length = args.episode_limit\n",
    "        # self.device = args.device\n",
    "        self.device = 'cpu'\n",
    "        self._setup_data()\n",
    "\n",
    "    def _setup_data(self):\n",
    "        self.index_st = 0\n",
    "        self.n_sample = 0\n",
    "\n",
    "        self.data = {}\n",
    "        for k, v in self.scheme.items():\n",
    "            shape = (self.buffer_size, self.max_seq_length) + v['shape']\n",
    "            self.data[k] = torch.zeros(shape, dtype=v['dtype'],device=self.device)\n",
    "\n",
    "    def sample(self, batch_size, top_n = 0):\n",
    "        if self.n_sample < batch_size:\n",
    "            return None\n",
    "\n",
    "        def trans_ids(ep_id):\n",
    "            ep_id += self.index_st\n",
    "            ep_id %= self.buffer_size\n",
    "            return ep_id\n",
    "\n",
    "        ep_ids = np.random.choice(self.n_sample - top_n, batch_size - top_n, replace=False)\n",
    "        ep_ids = list(ep_ids) + [self.n_sample-i-1 for i in range(top_n)]# [self.n_sample -1]\n",
    "        ep_ids = list(map(trans_ids, ep_ids))\n",
    "\n",
    "        ret = {}\n",
    "        for k, v in self.data.items():\n",
    "            ret[k] = v[ep_ids]\n",
    "\n",
    "        return ret\n",
    "    \n",
    "\n",
    "    def add_episode(self, data):\n",
    "\n",
    "        #len_ep = len\n",
    "        if self.n_sample < self.buffer_size:\n",
    "            self.n_sample += 1\n",
    "        else:\n",
    "            self.index_st += 1\n",
    "            self.index_st %= self.buffer_size\n",
    "\n",
    "        index_ep = (self.index_st + self.n_sample - 1) % self.buffer_size\n",
    "        \n",
    "        for k, v in data.items():\n",
    "\n",
    "            ep_len = len(v)\n",
    "            dtype = self.scheme[k]['dtype']\n",
    "            self.data[k][index_ep].zero_()\n",
    "            self.data[k][index_ep, 0:ep_len] = torch.as_tensor(v, dtype=dtype, device=self.device)\n",
    "\n",
    "    def clear(self):\n",
    "        self.index_st = 0\n",
    "        self.n_sample = 0\n",
    "        for item in self.data.values():\n",
    "            item.zero_()\n",
    "        \n",
    "\n",
    "    def state_dict(self):\n",
    "        buffer_state = {'index_st': self.index_st, 'n_sample': self.n_sample, 'data': self.data}\n",
    "        return buffer_state\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.index_st = state_dict['index_st']\n",
    "        self.n_sample = state_dict['n_sample']    \n",
    "        self.data = state_dict['data']\n",
    "\n",
    "class QMixNet(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(QMixNet, self).__init__()\n",
    "        self.args = args\n",
    "        # 因为生成的hyper_w1需要是一个矩阵，而pytorch神经网络只能输出一个向量，\n",
    "        # 所以就先输出长度为需要的 矩阵行*矩阵列 的向量，然后再转化成矩阵\n",
    "\n",
    "        # args.n_agents是使用hyper_w1作为参数的网络的输入维度，args.qmix_hidden_dim是网络隐藏层参数个数\n",
    "        # 从而经过hyper_w1得到(经验条数，args.n_agents * args.qmix_hidden_dim)的矩阵\n",
    "        \n",
    "        input_dim = args.state_dim\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hyper_w1 = nn.Linear(input_dim, args.n_agents * args.qmix_hidden_dim)\n",
    "        # 经过hyper_w2得到(经验条数, 1)的矩阵\n",
    "        self.hyper_w2 = nn.Linear(input_dim, args.qmix_hidden_dim * 1)\n",
    "\n",
    "        # hyper_w1得到的(经验条数，args.qmix_hidden_dim)矩阵需要同样维度的hyper_b1\n",
    "        self.hyper_b1 = nn.Linear(input_dim, args.qmix_hidden_dim)\n",
    "        # hyper_w2得到的(经验条数，1)的矩阵需要同样维度的hyper_b1\n",
    "        self.hyper_b2 =nn.Sequential(nn.Linear(input_dim, args.qmix_hidden_dim),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(args.qmix_hidden_dim, 1)\n",
    "                                     )\n",
    "\n",
    "    def forward(self, q_values, states):  # states的shape为(episode_num, max_episode_len， state_dim)\n",
    "        # 传入的q_values是三维的，shape为(episode_num, max_episode_len， n_agents)\n",
    "        episode_num = q_values.size(0)\n",
    "        q_values = q_values.view(-1, 1, self.args.n_agents)  # (episode_num * max_episode_len, 1, n_agents) = (1920,1,5)\n",
    "        states = states.reshape(-1, self.input_dim)  # (episode_num * max_episode_len, state_dim)\n",
    "\n",
    "        w1 = torch.abs(self.hyper_w1(states))  # (1920, 160)\n",
    "        b1 = self.hyper_b1(states)  # (1920, 32)\n",
    "\n",
    "        w1 = w1.view(-1, self.args.n_agents, self.args.qmix_hidden_dim)  # (1920, 5, 32)\n",
    "        b1 = b1.view(-1, 1, self.args.qmix_hidden_dim)  # (1920, 1, 32)\n",
    "\n",
    "        hidden = F.elu(torch.bmm(q_values, w1) + b1)  # (1920, 1, 32)\n",
    "\n",
    "        w2 = torch.abs(self.hyper_w2(states))  # (1920, 32)\n",
    "        b2 = self.hyper_b2(states)  # (1920, 1)\n",
    "\n",
    "        w2 = w2.view(-1, self.args.qmix_hidden_dim, 1)  # (1920, 32, 1)\n",
    "        b2 = b2.view(-1, 1, 1)  # (1920, 1， 1)\n",
    "\n",
    "        q_total = torch.bmm(hidden, w2) + b2  # (1920, 1, 1)\n",
    "        q_total = q_total.view(episode_num, -1, 1)  # (32, 60, 1)\n",
    "        return q_total\n",
    "\n",
    "class Learner:\n",
    "    def __init__(self, w_util, args):\n",
    "        \n",
    "        self.w_util = w_util\n",
    "        self.device = args.device\n",
    "        self.critic_attn = args.critic_attn\n",
    "\n",
    "        # if self.critic_attn:                        \n",
    "        #     from .attn_critics import Critics\n",
    "        # else:            \n",
    "        #     from .critics import Critics\n",
    "\n",
    "        self.sys_actor = Actors(args)\n",
    "        self.sys_critic1 = Critics(args)\n",
    "        self.sys_critic2 = Critics(args)\n",
    "        self.sys_critic1_tar = Critics(args)\n",
    "        self.sys_critic2_tar = Critics(args)\n",
    "        #self.sys_critic1.train()\n",
    "        #self.sys_critic2.train()\n",
    "    \n",
    "        self.mix_net1 = QMixNet(args)        \n",
    "        self.mix_net2 = QMixNet(args)        \n",
    "        self.mix_net1_tar = QMixNet(args)\n",
    "        self.mix_net2_tar = QMixNet(args)\n",
    "        \n",
    "        if self.device != 'cpu':\n",
    "            self.sys_actor.cuda(self.device)\n",
    "            self.sys_critic1.cuda(self.device)\n",
    "            self.sys_critic2.cuda(self.device)\n",
    "            self.sys_critic1_tar.cuda(self.device)\n",
    "            self.sys_critic2_tar.cuda(self.device)\n",
    "            self.mix_net1.cuda(self.device)\n",
    "            self.mix_net2.cuda(self.device)\n",
    "            self.mix_net1_tar.cuda(self.device)\n",
    "            self.mix_net2_tar.cuda(self.device)\n",
    "        \n",
    "        self.sys_critic1_tar.requires_grad_(False)\n",
    "        self.sys_critic2_tar.requires_grad_(False)\n",
    "        self.mix_net1_tar.requires_grad_(False)\n",
    "        self.mix_net2_tar.requires_grad_(False)\n",
    "\n",
    "\n",
    "\n",
    "        self._sync_target()\n",
    "\n",
    "        \n",
    "        self.n_agents = args.n_agents\n",
    "        self.n_actions = args.n_actions\n",
    "        self.gamma = args.gamma\n",
    "        self.entropy_tar = args.entropy_tar\n",
    "        self.lr = args.lr\n",
    "        self.lr_actor = args.lr_actor\n",
    "        self.lr_alpha = args.lr_alpha\n",
    "        self.l2 = args.l2\n",
    "        self.target_update = args.target_update\n",
    "        self.step = 0                \n",
    "        self.agent_id = args.agent_id\n",
    "        self.last_action = args.last_action  \n",
    "        self.log_alpha_st = args.log_alpha_st\n",
    "        self.shared_alpha = args.shared_alpha\n",
    "        self.maximum_entropy = args.maximum_entropy     \n",
    "        self.args = args\n",
    "\n",
    "        \n",
    "        if self.shared_alpha:\n",
    "            self.log_alpha = torch.tensor(args.log_alpha_st, dtype=torch.float32, requires_grad=True, device = self.device)\n",
    "        else:\n",
    "            self.log_alpha = torch.tensor([args.log_alpha_st]*self.n_agents, dtype=torch.float32, requires_grad=True, device = self.device)\n",
    "        \n",
    "        params_critic = list(self.sys_critic1.parameters()) + list(self.sys_critic2.parameters()) + list(self.mix_net1.parameters()) + list(self.mix_net2.parameters())\n",
    "        \n",
    "        self.params_critic = params_critic\n",
    "        self.params_actor = list(self.sys_actor.parameters())\n",
    "\n",
    "    \n",
    "        self.optim_actor = torch.optim.Adam(self.sys_actor.parameters(), lr = self.lr_actor, weight_decay=self.l2)\n",
    "        self.optim_critic = torch.optim.Adam(params_critic, lr = self.lr, weight_decay=self.l2)        \n",
    "        self.optim_alpha = torch.optim.Adam([self.log_alpha], lr=self.lr_alpha)\n",
    "\n",
    "    def train(self, data):\n",
    "\n",
    "        self.step += 1\n",
    "        w_util = self.w_util\n",
    "\n",
    "        state = data['state'].to(device=self.device, non_blocking=True)\n",
    "        obs = data['obs'].to(device=self.device, non_blocking=True)\n",
    "        actions = data['actions'].to(device=self.device, non_blocking=True)\n",
    "        reward = data['reward'].to(device=self.device, non_blocking=True)\n",
    "        valid = data['valid'].to(device=self.device, non_blocking=True)\n",
    "        avail_actions = data['avail_actions'].to(device=self.device, non_blocking=True)\n",
    "        actions_onehot = self.one_hot(actions,self.n_actions)\n",
    " \n",
    "        n_batch = obs.shape[0]\n",
    "        T = obs.shape[1]\n",
    "        alpha = torch.exp(self.log_alpha.detach())\n",
    "        valid_rate = torch.mean(valid.float())\n",
    "\n",
    "        if self.agent_id:\n",
    "            agent_ids = torch.eye(self.n_agents,device=obs.device)\n",
    "            agent_ids = agent_ids.reshape((1,)*(obs.ndim-2)+agent_ids.shape)            \n",
    "            agent_ids = agent_ids.expand(obs.shape[:-2]+(-1,-1))\n",
    "            obs = torch.cat([obs,agent_ids],-1)\n",
    "        if self.last_action:\n",
    "            last_actions = torch.zeros_like(actions_onehot)\n",
    "            last_actions[:,1:] = actions_onehot[:,:-1]\n",
    "            obs = torch.cat([obs,last_actions],-1)\n",
    "\n",
    "        hiddens1 = self.sys_critic1.init_hiddens(n_batch)\n",
    "        hiddens1_tar = self.sys_critic1_tar.init_hiddens(n_batch)\n",
    "        hiddens2 = self.sys_critic2.init_hiddens(n_batch)\n",
    "        hiddens2_tar = self.sys_critic2_tar.init_hiddens(n_batch)\n",
    "        hiddens_actor = self.sys_actor.init_hiddens(n_batch)\n",
    "        \n",
    "\n",
    "        Q1s = []\n",
    "        Q2s = []\n",
    "        Q1s_tar = []\n",
    "        Q2s_tar = []        \n",
    "        ps = []\n",
    "        \n",
    "        \n",
    "\n",
    "        for i in range(T):\n",
    "            \n",
    "            Q1, hiddens1 = self.sys_critic1.forward(obs[:,i], avail_actions[:,i], hiddens1)\n",
    "            Q2, hiddens2 = self.sys_critic2.forward(obs[:,i], avail_actions[:,i], hiddens2)\n",
    "            p, hiddens_actor = self.sys_actor.forward(obs[:,i], avail_actions[:,i], hiddens_actor)\n",
    "            \n",
    "            #with torch.no_grad():\n",
    "                \n",
    "            Q1_tar, hiddens1_tar = self.sys_critic1_tar.forward(obs[:,i], avail_actions[:,i], hiddens1_tar)\n",
    "            Q2_tar, hiddens2_tar = self.sys_critic2_tar.forward(obs[:,i], avail_actions[:,i], hiddens2_tar)\n",
    "            \n",
    "            Q1s.append(Q1)\n",
    "            Q2s.append(Q2)\n",
    "            Q1s_tar.append(Q1_tar)\n",
    "            Q2s_tar.append(Q2_tar)\n",
    "            ps.append(p)\n",
    "\n",
    "        Q1s = torch.stack(Q1s,1)\n",
    "        Q2s = torch.stack(Q2s,1)\n",
    "        Q1s_tar = torch.stack(Q1s_tar,1)\n",
    "        Q2s_tar = torch.stack(Q2s_tar,1)\n",
    "\n",
    "        ps = torch.stack(ps,1)\n",
    "        ps[valid == 0] = 0\n",
    "        \n",
    "        log_ps = torch.log(ps + 1e-38)\n",
    "        log_ps[valid == 0] = 0\n",
    "        #log_ps[avail_actions == 0] = 0\n",
    "        entropy = -torch.sum(ps*log_ps, -1)\n",
    "        \n",
    "\n",
    "        Q1s[valid == 0] = 0\n",
    "        Q2s[valid == 0] = 0\n",
    "        Q1s_tar[valid == 0] = 0\n",
    "        Q2s_tar[valid == 0] = 0\n",
    "\n",
    "        q1s = self.gather_end(Q1s,actions)\n",
    "        q2s = self.gather_end(Q2s,actions)\n",
    "\n",
    "        q1s_tot = self.mix_net1(q1s, state)\n",
    "        q2s_tot = self.mix_net2(q2s, state)\n",
    "        \n",
    "        q1s_tot[valid == 0] = 0\n",
    "        q2s_tot[valid == 0] = 0\n",
    "        \n",
    "        V1s = torch.sum(ps * Q1s.detach(), -1)\n",
    "        V2s = torch.sum(ps * Q2s.detach(), -1)\n",
    "        \n",
    "        V1s_tar = torch.sum(ps.detach() * Q1s_tar, -1)\n",
    "        V2s_tar = torch.sum(ps.detach() * Q2s_tar, -1)\n",
    "        \n",
    "        self.mix_net1.requires_grad_(False)\n",
    "        self.mix_net2.requires_grad_(False)\n",
    "        \n",
    "        V1s_tot = self.mix_net1.forward(V1s, state)\n",
    "        V2s_tot = self.mix_net2.forward(V2s, state)\n",
    "\n",
    "        self.mix_net1.requires_grad_(True)\n",
    "        self.mix_net2.requires_grad_(True)\n",
    "\n",
    "        V1s_tot_tar = self.mix_net1_tar.forward(V1s_tar, state)\n",
    "        V2s_tot_tar = self.mix_net2_tar.forward(V2s_tar, state)\n",
    "        \n",
    "        Vs_tot = torch.min(torch.stack([V1s_tot,V2s_tot],-1), -1)[0]\n",
    "        Vs_tot_tar = torch.min(torch.stack([V1s_tot_tar,V2s_tot_tar],-1), -1)[0]\n",
    "\n",
    "        Vs_tot[valid == 0] = 0\n",
    "        Vs_tot_tar[valid == 0] = 0\n",
    "\n",
    "        \n",
    "\n",
    "        alpha_entropy = torch.sum(alpha * entropy, -1, keepdim=True)     \n",
    "        Ves_tot = Vs_tot + alpha_entropy\n",
    "        Ves_tot_tar = Vs_tot_tar + alpha_entropy.detach()\n",
    "        \n",
    "        # train actor        \n",
    "        loss_actor = - torch.mean(Ves_tot)/valid_rate\n",
    "        self.optim_actor.zero_grad()\n",
    "        loss_actor.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.params_actor, self.args.grad_norm_clip)\n",
    "        self.optim_actor.step()\n",
    "               \n",
    "        \n",
    "                        \n",
    "        qs_star = torch.zeros_like(q1s_tot)\n",
    "        qs_star += reward.unsqueeze(-1)\n",
    "\n",
    "        if self.maximum_entropy:\n",
    "            sys_v_tar = Ves_tot_tar\n",
    "        else:\n",
    "            sys_v_tar = Vs_tot_tar\n",
    "\n",
    "        qs_star[:,:-1] += self.gamma * (sys_v_tar[:,1:])\n",
    "        \n",
    "        loss1 = F.mse_loss(q1s_tot,qs_star)/valid_rate\n",
    "        loss2 = F.mse_loss(q2s_tot,qs_star)/valid_rate        \n",
    "        loss = loss1+loss2\n",
    "        self.optim_critic.zero_grad()        \n",
    "        loss.backward()  \n",
    "        torch.nn.utils.clip_grad_norm_(self.params_critic, self.args.grad_norm_clip)      \n",
    "        self.optim_critic.step()        \n",
    "        self.update_target()\n",
    "        \n",
    "        loss_alpha = self.log_alpha*(entropy.detach()-self.entropy_tar)\n",
    "        loss_alpha[valid ==0] = 0\n",
    "        loss_alpha = torch.mean(loss_alpha)/valid_rate\n",
    "\n",
    "        self.optim_alpha.zero_grad()\n",
    "        loss_alpha.backward()\n",
    "        self.optim_alpha.step()\n",
    "        \n",
    "        m_loss = loss/2\n",
    "        m_alpha = alpha.mean()    \n",
    "        m_v = torch.mean(V1s.detach())/valid_rate        \n",
    "        m_v_total = torch.mean(Vs_tot.detach())/valid_rate\n",
    "        m_entropy = torch.mean(entropy.detach())/valid_rate        \n",
    "        m_max_p = torch.mean(torch.max(ps.detach(),-1)[0])/valid_rate\n",
    "        \n",
    "        w_util.WriteScalar('train/loss', m_loss.item())\n",
    "        w_util.WriteScalar('train/v', m_v.item())\n",
    "        w_util.WriteScalar('train/v_total', m_v_total.item())\n",
    "        w_util.WriteScalar('train/entropy', m_entropy.item())\n",
    "        w_util.WriteScalar('train/alpha', m_alpha.item())        \n",
    "        w_util.WriteScalar('train/max_p', m_max_p.item())\n",
    "        \n",
    "        return m_loss.item()\n",
    "\n",
    "    def state_dict(self):\n",
    "        state_dict = {}\n",
    "        state_dict['critic1'] = self.sys_critic1.state_dict()\n",
    "        state_dict['critic2'] = self.sys_critic2.state_dict()\n",
    "        state_dict['critic1_tar'] = self.sys_critic1_tar.state_dict()\n",
    "        state_dict['critic2_tar'] = self.sys_critic2_tar.state_dict()\n",
    "        state_dict['mix_net1'] = self.mix_net1.state_dict()\n",
    "        state_dict['mix_net2'] = self.mix_net2.state_dict()\n",
    "        state_dict['mix_net1_tar'] = self.mix_net1_tar.state_dict()\n",
    "        state_dict['mix_net2_tar'] = self.mix_net2_tar.state_dict()\n",
    "        state_dict['actor'] = self.sys_actor.state_dict()\n",
    "        state_dict['log_alpha'] = self.log_alpha.detach()\n",
    "        state_dict['optim_critic'] = self.optim_critic.state_dict()\n",
    "        state_dict['optim_actor'] = self.optim_actor.state_dict()\n",
    "        state_dict['optim_alpha'] = self.optim_alpha.state_dict()\n",
    "        return state_dict        \n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.sys_critic1.load_state_dict(state_dict['critic1'])\n",
    "        self.sys_critic2.load_state_dict(state_dict['critic2'])\n",
    "        self.sys_critic1_tar.load_state_dict(state_dict['critic1_tar'])\n",
    "        self.sys_critic2_tar.load_state_dict(state_dict['critic2_tar'])\n",
    "        self.mix_net1.load_state_dict(state_dict['mix_net1'])\n",
    "        self.mix_net2.load_state_dict(state_dict['mix_net2'])\n",
    "        self.mix_net1_tar.load_state_dict(state_dict['mix_net1_tar'])\n",
    "        self.mix_net2_tar.load_state_dict(state_dict['mix_net2_tar'])\n",
    "        self.sys_actor.load_state_dict(state_dict['actor'])\n",
    "        with torch.no_grad():\n",
    "            self.log_alpha.copy_(state_dict['log_alpha'])\n",
    "        self.optim_critic.load_state_dict(state_dict['optim_critic'])\n",
    "        self.optim_actor.load_state_dict(state_dict['optim_actor'])\n",
    "        self.optim_alpha.load_state_dict(state_dict['optim_alpha'])\n",
    "\n",
    "        \n",
    "    def gather_end(self, input, index):\n",
    "        index = torch.unsqueeze(index,-1).to(dtype=torch.int64)\n",
    "        return torch.gather(input, input.ndim -1, index).squeeze(-1)\n",
    "\n",
    "    def one_hot(self, tensor, n_classes):\n",
    "        return F.one_hot(tensor.to(dtype=torch.int64), n_classes).to(dtype=torch.float32)\n",
    "\n",
    "    def update_target(self):\n",
    "        if self.target_update >=1:\n",
    "            if self.step % self.target_update == 0:\n",
    "                self._sync_target()\n",
    "        else:\n",
    "            def soft_update(src,tar):\n",
    "                cur_state = src.state_dict()\n",
    "                tar_state = tar.state_dict()\n",
    "                for key in tar_state:                \n",
    "                    v_tar = tar_state[key]\n",
    "                    v_cur = cur_state[key]\n",
    "                    v_tar += self.target_update*(v_cur-v_tar).detach()\n",
    "            soft_update(self.sys_critic1,self.sys_critic1_tar)\n",
    "            soft_update(self.sys_critic2,self.sys_critic2_tar)\n",
    "            soft_update(self.mix_net1,self.mix_net1_tar)\n",
    "            soft_update(self.mix_net2,self.mix_net2_tar)\n",
    "    \n",
    "    def _sync_target(self):\n",
    "        self.sys_critic1_tar.load_state_dict(self.sys_critic1.state_dict())\n",
    "        self.sys_critic2_tar.load_state_dict(self.sys_critic2.state_dict())\n",
    "        self.mix_net1_tar.load_state_dict(self.mix_net1.state_dict())\n",
    "        self.mix_net2_tar.load_state_dict(self.mix_net2.state_dict())\n",
    " \n",
    "    def _valid_mask(self, input, valid):\n",
    "        input *= valid.view(list(valid.shape) + [1] * (input.ndim - valid.ndim))\n",
    "        return input\n",
    "        \n",
    "class Experiment:\n",
    "    def __init__(self, args):        \n",
    "        self.args = args\n",
    "\n",
    "\n",
    "    def save(self):\n",
    "\n",
    "        run_state = {}                \n",
    "        run_state['episode'] = self.e\n",
    "        run_state['step'] = self.step\n",
    "        run_state['best_win_rate'] = self.best_win_rate\n",
    "        run_state['learner'] = self.learner.state_dict()\n",
    "        run_state['buffer'] = self.buffer.state_dict()\n",
    "        torch.save(run_state, self.path_checkpt)        \n",
    "\n",
    "    def load(self):\n",
    "        run_state = torch.load(self.path_checkpt)\n",
    "        self.e = run_state['episode']\n",
    "        self.step = run_state['step']\n",
    "        self.best_win_rate = run_state['best_win_rate']\n",
    "        self.learner.load_state_dict(run_state['learner'])                \n",
    "        self.buffer.load_state_dict(run_state['buffer'])\n",
    "        self.result = np.load(self.path_result)\n",
    "\n",
    "    def start(self):\n",
    "        args = self.args\n",
    "\n",
    "        path_checkpt = 'checkpoints'\n",
    "        path_result = 'results'\n",
    "        path_model = 'models'\n",
    "        if not os.path.exists(path_checkpt):\n",
    "            os.mkdir(path_checkpt)\n",
    "        if not os.path.exists(path_result):\n",
    "            os.mkdir(path_result)\n",
    "        if not os.path.exists(path_model):\n",
    "            os.mkdir(path_model)\n",
    "        path_checkpt = os.path.join(path_checkpt, args.run_name + '.tar')\n",
    "        path_result = os.path.join(path_result, args.run_name + '.npy')\n",
    "        path_model = os.path.join(path_model, args.run_name + '.tar')\n",
    "            \n",
    "        # env = StarCraft2Env(map_name=args.map_name, window_size_x=640, window_size_y=480)\n",
    "        env = simple_spread_v3.parallel_env(N=args.N, local_ratio=args.local_ratio, max_cycles=args.max_cycles, continuous_actions=args.continuous_actions)\n",
    "        print(env)\n",
    "        env.reset()\n",
    "\n",
    "        env_info = {\n",
    "            \"n_agents\": args.N,\n",
    "            \"n_actions\": 5,  # no_action, move_left, move_right, move_down, move_up\n",
    "            \"obs_shape\": 18,\n",
    "            \"state_shape\": 54,\n",
    "            \"episode_limit\": args.max_cycles\n",
    "        }\n",
    "        # env_info = env.get_env_info()\n",
    "\n",
    "        args.n_agents = env_info[\"n_agents\"]\n",
    "        args.n_actions = env_info[\"n_actions\"]\n",
    "        args.obs_dim = env_info['obs_shape']\n",
    "        args.input_dim = args.obs_dim\n",
    "        if args.agent_id:\n",
    "            args.input_dim += args.n_agents\n",
    "        if args.last_action:        \n",
    "            args.input_dim += args.n_actions        \n",
    "        args.state_dim = env_info['state_shape']\n",
    "        args.episode_limit = env_info['episode_limit']\n",
    "        \n",
    "        writter = SummaryWriter('runs/'+ args.run_name + '/' + datetime.datetime.now().strftime('%Y-%m-%d,%H%M%S'))\n",
    "        w_util = WritterUtil(writter,args)\n",
    "        learner = Learner(w_util,args)               \n",
    "        ctrler = Controller(learner.sys_actor,args)\n",
    "        runner = Runner(env,ctrler,args)\n",
    "                \n",
    "        scheme = {}\n",
    "        n_agents = args.n_agents\n",
    "        scheme['obs'] = {'shape':(n_agents, args.obs_dim), 'dtype': torch.float32}\n",
    "        scheme['valid'] = {'shape':(), 'dtype': torch.int32}\n",
    "        scheme['actions'] = {'shape':(n_agents,), 'dtype': torch.int32}\n",
    "        scheme['avail_actions'] = {'shape':(n_agents, args.n_actions), 'dtype': torch.int32}\n",
    "        scheme['reward'] = {'shape':(), 'dtype': torch.float32}\n",
    "        scheme['state'] = {'shape':(args.state_dim,), 'dtype': torch.float32}\n",
    "\n",
    "        buffer = EpisodeBuffer(scheme, args)\n",
    "        result = np.zeros((3, args.n_steps // args.test_every_step))\n",
    "\n",
    "        self.e = 0\n",
    "        self.step = 0\n",
    "        self.best_win_rate = 0                   \n",
    "        self.path_checkpt = path_checkpt\n",
    "        self.path_result = path_result\n",
    "        self.path_model = path_model      \n",
    "        self.env = env        \n",
    "        self.writter = writter\n",
    "        self.w_util = w_util\n",
    "        self.ctrler = ctrler\n",
    "        self.runner = runner\n",
    "        self.learner = learner\n",
    "        self.buffer = buffer\n",
    "        self.result = result\n",
    "\n",
    "        if args.continue_run:\n",
    "            self.load()\n",
    "                    \n",
    "        \n",
    "    def run(self):\n",
    "        \n",
    "        args = self.args\n",
    "        buffer = self.buffer        \n",
    "        runner = self.runner\n",
    "        learner = self.learner\n",
    "        w_util = self.w_util\n",
    "        \n",
    "        while self.step < args.n_steps:\n",
    "        #for self.e in range(self.e, args.n_episodes+1):\n",
    "            self.e += 1            \n",
    "            data, episode_reward, win_tag, step =  runner.run()\n",
    "            old_step = self.step\n",
    "            self.step += step                   \n",
    "            w_util.set_step(self.step)            \n",
    "            w_util.WriteScalar('train/reward', episode_reward)\n",
    "            print(\"Episode {}, step {}, win = {}, reward = {}\".format(self.e, self.step, win_tag, episode_reward))\n",
    "            buffer.add_episode(data)\n",
    "            data = buffer.sample(args.n_batch, args.top_n)\n",
    "            if data:\n",
    "                loss = learner.train(data)                        \n",
    "                        \n",
    "            if self.step // args.test_every_step != old_step // args.test_every_step:\n",
    "                self.test_model()            \n",
    "\n",
    "            if args.save_every and self.e % args.save_every == 0:                \n",
    "                self.save()\n",
    "\n",
    "        self.env.close()\n",
    "\n",
    "    def test_model(self):\n",
    "        args = self.args\n",
    "        w_util = self.w_util\n",
    "        runner = self.runner\n",
    "        result = self.result\n",
    "\n",
    "        win_count = 0\n",
    "        reward_avg = 0\n",
    "        for i in range(args.test_count):\n",
    "            _, episode_reward, win_tag, _ =  runner.run(test_mode=True)\n",
    "            if win_tag:\n",
    "                win_count += 1\n",
    "            reward_avg += episode_reward                \n",
    "        win_rate = win_count/args.test_count        \n",
    "        reward_avg /= args.test_count\n",
    "        w_util.WriteScalar('test/reward', reward_avg)\n",
    "        w_util.WriteScalar('test/win_rate', win_rate)\n",
    "        result[:, self.step // args.test_every_step - 1] = [self.e, self.step, win_rate]\n",
    "        np.save(self.path_result, self.result)\n",
    "        if win_rate >= self.best_win_rate:\n",
    "            self.best_win_rate = win_rate\n",
    "            torch.save(self.learner.sys_actor.state_dict(), self.path_model)            \n",
    "        print('Test reward = {}, win_rate = {}'.format(reward_avg, win_rate))\n",
    "        \n",
    "class Runner:\n",
    "    def __init__(self, env, controller, args):\n",
    "        \"\"\"\n",
    "        Initialize the Runner.\n",
    "        Args:\n",
    "            env: The PettingZoo environment.\n",
    "            controller: The controller that decides agent actions.\n",
    "            args: Additional arguments, including max_cycles and number of agents (N).\n",
    "        \"\"\"\n",
    "        self.controller = controller\n",
    "        self.env = env\n",
    "        self.args = args\n",
    "\n",
    "    def run(self, test_mode=False):\n",
    "        \"\"\"\n",
    "        Runs a single episode in a parallel PettingZoo environment using the provided controller.\n",
    "        In the Parallel API, all agents receive observations and take actions simultaneously, as outlined in:\n",
    "            https://pettingzoo.farama.org/api/parallel/\n",
    "        This function:\n",
    "            1. Resets the environment and the controller for a new episode.\n",
    "            2. Iteratively collects observations in a parallel fashion for all agents.\n",
    "            3. Obtains actions from the controller and steps the environment.\n",
    "            4. Logs transition data (observations, actions, rewards, etc.) during each time step.\n",
    "            5. Terminates when the maximum number of cycles is reached.\n",
    "            6. Returns episode data, the total reward, whether a win condition was met, and the number of steps taken.\n",
    "        Args:\n",
    "            test_mode (bool, optional): If True, disables exploration to evaluate a learned policy. Defaults to False.\n",
    "        Returns:\n",
    "            tuple:\n",
    "                - data (dict): Dictionary containing keys 'state', 'obs', 'valid', 'actions', 'avail_actions', and 'reward'.\n",
    "                - episode_reward (float): Cumulative reward over the episode.\n",
    "                - win_tag (bool): Indicator of whether a win condition was satisfied (criteria can be environment-specific).\n",
    "                - steps (int): Number of steps taken in the episode.\n",
    "        Notes on Prints and Asserts (Parallel API):\n",
    "            - Print statements can be added throughout to debug the flow of observations and actions in a multi-agent setting.\n",
    "            - Asserts can validate that the environment adheres to the Parallel API, such as checking that the step() method\n",
    "              returns dictionaries keyed by agent for observations, rewards, terminations, truncations, and info.\n",
    "            - Additional prints and asserts can be useful when converting or wrapping an AEC environment into a Parallel environment\n",
    "              to ensure consistent multi-agent behavior across all agents.\n",
    "        \"\"\"\n",
    "class Runner:\n",
    "    def __init__(self, env, controller, args):\n",
    "        \"\"\"\n",
    "        Initialize the Runner.\n",
    "        Args:\n",
    "            env: The PettingZoo environment.\n",
    "            controller: The controller that decides agent actions.\n",
    "            args: Additional arguments, including max_cycles and number of agents (N).\n",
    "        \"\"\"\n",
    "        self.controller = controller\n",
    "        self.env = env\n",
    "        self.args = args\n",
    "\n",
    "    def run(self, test_mode=False):\n",
    "        \"\"\"\n",
    "        Run a single episode in the PettingZoo environment.\n",
    "\n",
    "        Args:\n",
    "            test_mode (bool): If True, disables exploration. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (data, episode_reward, win_tag, steps)\n",
    "        \"\"\"\n",
    "        print(\"Resetting environment...\")\n",
    "        observations, infos = self.env.reset()\n",
    "\n",
    "        self.controller.new_episode()\n",
    "        data = {\n",
    "            'state': [],\n",
    "            'obs': [],\n",
    "            'valid': [],\n",
    "            'actions': [],\n",
    "            'avail_actions': [],\n",
    "            'reward': []\n",
    "        }\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        while steps < self.args.max_cycles:\n",
    "            steps += 1\n",
    "            \n",
    "            # Ensure environment state is valid\n",
    "            assert hasattr(self.env, 'state'), \"Environment does not have a 'state()' method.\"\n",
    "            state = self.env.state()\n",
    "\n",
    "            # Validate observations dictionary\n",
    "            assert isinstance(observations, dict), \"Observations must be a dictionary.\"\n",
    "            assert len(observations) == len(self.env.agents), \"Mismatch between agents and observations.\"\n",
    "\n",
    "            # Convert observations dict to list in consistent order\n",
    "            obs_list = [observations[agent_id] for agent_id in self.env.agents]\n",
    "\n",
    "            # Validate available actions\n",
    "            avail_actions_list = [[1] * 5 for _ in range(self.args.N)]  # Example: 5 actions per agent\n",
    "            assert len(avail_actions_list) == self.args.N, \"Available actions list size mismatch.\"\n",
    "\n",
    "            explore = not test_mode\n",
    "\n",
    "            # Get actions from controller\n",
    "            actions_array = self.controller.get_actions(obs_list, avail_actions_list, explore)\n",
    "            assert len(actions_array) == len(self.env.agents), \"Mismatch between actions and agents.\"\n",
    "\n",
    "            # Convert actions array to dict for PettingZoo environment\n",
    "            actions_dict = {agent_id: action for agent_id, action in zip(self.env.agents, actions_array)}\n",
    "\n",
    "            # Step the environment\n",
    "            try:\n",
    "                observations, rewards, terminations, truncations, infos = self.env.step(actions_dict)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during environment step: {e}\")\n",
    "                break\n",
    "\n",
    "            # Validate rewards dictionary\n",
    "            assert isinstance(rewards, dict), \"Rewards must be a dictionary.\"\n",
    "\n",
    "            # Calculate total reward for this step\n",
    "            step_reward = sum(rewards.values())\n",
    "            episode_reward += step_reward\n",
    "\n",
    "            # Store transition data\n",
    "            data['state'].append(state)\n",
    "            data['obs'].append(obs_list)\n",
    "            data['valid'].append(1)  # Assume all steps are valid\n",
    "            data['actions'].append(actions_array)\n",
    "            data['avail_actions'].append(avail_actions_list)\n",
    "            data['reward'].append(step_reward)\n",
    "\n",
    "            # Check if all agents are done\n",
    "            if all(terminations.values()) or all(truncations.values()):\n",
    "                break\n",
    "\n",
    "        # Define win condition placeholder\n",
    "        win_tag = episode_reward > 0  # Modify based on environment-specific criteria\n",
    "\n",
    "        return data, episode_reward, win_tag, steps\n",
    "\n",
    "class Args:\n",
    "    pass\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    config_path='config.json'\n",
    "        \n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    args = Args()\n",
    "    args.__dict__.update(config)\n",
    "\n",
    "    args.continue_run = False\n",
    "    argv = sys.argv\n",
    "    if len(argv)>1:\n",
    "        if argv[1] == '-c':  ##continue\n",
    "            args.continue_run = True\n",
    "\n",
    "    experiment = Experiment(args)\n",
    "    experiment.start()\n",
    "    experiment.run()\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlzh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
